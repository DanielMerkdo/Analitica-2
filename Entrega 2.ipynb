{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1vV1eVtv-ClzLusbtzpnBDuTwVNr18TrP","timestamp":1726610006955}],"collapsed_sections":["ZDkre2ely74a","vtJeDeR3R0hk","GtDZg2aOS0FV","xV-JW-G9TSMT","t0qJ-8dfyYb2","Qy1Dp_ZaUd4u","wR9hIeYLUgnl","MrqVHJkoUq3k","6fi8gzxmY3Hv","P4uizgw8KQCa","AiJ9uqv1iGw2","mL73ct9YW2H4","Y-LNT1Da3Yse","76U9nji8eYbK","MdekyjQMe4ya","J2K7a4QDeqTa","kXQ9ToStfUe-","T9vx6BqB3Ysf","Z83c_H_HIoc0","QyefZypnaIRE","hQVMNxFpIMQG","3nrU6U745OHD","NKwT_Cark82i","TWBDV9BGlPdT","vtQqi8wFTE6L","mpEnKn0uTE6M"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Integrantes:\n","\n","*   Daniel Perea Mercado\n","*   David Diaz Rodriguez\n","*   Nicolas Niño Valderrama\n","*   Valentina Jimenez Torres"],"metadata":{"id":"ilM7iWdTqATc"}},{"cell_type":"markdown","source":["# **Preparación del notebook**"],"metadata":{"id":"aiO0uOi8Q1Cc"}},{"cell_type":"markdown","source":["___\n","## Cargue de librerias"],"metadata":{"id":"2dnJ5LR42sI0"}},{"cell_type":"code","source":["!pip install unidecode"],"metadata":{"id":"fqbvaGtF2r2u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Eliminar Warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Data\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","import unidecode\n","from scipy.stats import chi2_contingency\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Estadística\n","import math\n","import scipy.stats as stats\n","from math import pi\n","from scipy.stats import zscore\n","\n","# Visualización\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","import plotly.graph_objects as go\n","import plotly.subplots as sp\n","import seaborn as sns\n","from matplotlib.patches import Patch\n","from IPython.display import Image\n","\n","# Modelling\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","import statsmodels.api as sm\n","from sklearn.metrics import mean_squared_error,  mean_absolute_error, mean_absolute_percentage_error, r2_score, classification_report, accuracy_score,confusion_matrix, ConfusionMatrixDisplay, f1_score\n","from sklearn import metrics, tree\n","import graphviz\n","import math\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.utils import shuffle\n","from sklearn.datasets import load_iris\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n","from sklearn.datasets import make_classification, make_regression\n","from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, RepeatedKFold\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.datasets import load_iris\n","from xgboost import XGBClassifier, XGBRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.svm import SVR"],"metadata":{"id":"wv71k9U82n_I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["___\n","## Instalación del entorno de GitHub"],"metadata":{"id":"vn3w1DZ-y0XZ"}},{"cell_type":"code","source":["# Instalar git en Colab si no está instalado\n","!apt-get install git"],"metadata":{"id":"zIVEJ4fTvNZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clonar el repositorio de GitHub\n","!git clone https://github.com/niconinov/Proyecto-Aula.git"],"metadata":{"id":"MKl6tEfJvOJ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["___\n","## Descarga de las bases de datos en el notebook"],"metadata":{"id":"ZDkre2ely74a"}},{"cell_type":"code","source":["# Nombre de las bases de datos en GitHub\n","names =['algebra_vectorial', 'calculo_diferencial']"],"metadata":{"id":"3P41pkO5zIJj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lectura de los csv en GitHub\n","dfs = {}\n","\n","for i in names:\n","  dfs[i] = pd.read_csv(f'/content/Proyecto-Aula/{i}.csv')"],"metadata":{"id":"cBFhs0NAyySg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creación de las bases de datos\n","df_alg = pd.DataFrame(dfs['algebra_vectorial'])\n","df_cal = pd.DataFrame(dfs['calculo_diferencial'].iloc[:, 1:])"],"metadata":{"id":"aCcsB4f3vYFq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg = pd.read_csv('/content/Proyecto-Aula/algebra_vectorial.csv', sep=';')\n","df_alg = df_alg.drop(df_alg.columns[0], axis=1)\n","df_alg.head()"],"metadata":{"id":"NMt-yS0TrOj6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**PRIMERA ENTREGA**"],"metadata":{"id":"xkcrsTpYd96U"}},{"cell_type":"markdown","source":["___\n","## **Tratamiento de las bases de datos**"],"metadata":{"id":"YgjrkMiDjtwy"}},{"cell_type":"markdown","source":["### CÁLCULO DIFERENCIAL"],"metadata":{"id":"j6Z1CoUoQi65"}},{"cell_type":"markdown","source":["#### Exploración"],"metadata":{"id":"vtJeDeR3R0hk"}},{"cell_type":"code","source":["df_cal.shape"],"metadata":{"id":"AOYDRXxGjxjq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Respectivas variables ordinales en categorías ordinales\n","\n","categorical_columns = ['madre_edu', 'padre_edu', 't_examen', 't_estudio', 'relacion_fam', 'tiempo_libre',\n","                        'salir_amigos', 'cons_alcohol_sem', 'cons_alcohol_finde', 'salud']\n","\n","df_cal[categorical_columns] = df_cal[categorical_columns].astype('object')"],"metadata":{"id":"aWsTEhti3oZ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_cal.info()"],"metadata":{"id":"EME0Rhu3j9xX","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_cal.head()"],"metadata":{"id":"jza-I0oRlV_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Se guarda esta copia para cambiar el nombre de las columnas en el dataframe de álgebra\n","df_cal_og = df_cal.copy()"],"metadata":{"id":"5x_K0SaPXSA2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Datos duplicados"],"metadata":{"id":"GtDZg2aOS0FV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6FnEjIS-CgA"},"outputs":[],"source":["df_cal.duplicated().sum()"]},{"cell_type":"code","source":["duplicadoscal = df_cal[df_cal.duplicated(keep=False)]\n","print(duplicadoscal)"],"metadata":{"id":"_6ZgS8nwcsvL","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Eliminación de los datos duplicados\n","df_cal = df_cal.drop_duplicates()"],"metadata":{"id":"Lgwx6j_2gBSy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">En el DataFrame referente a los estudiantes de Calculo, se identificaron dos filas duplicadas, ubicadas en las posiciones 395 y 396. Debido a la naturaleza de los datos concideramos que no es posible que existan dos estudiantes con exactamente las misma caracteristicas, por lo cual tomamos la desición de eliminar estas filas duplicadas antes de continuar con el tratamiento y análisis de la base de datos."],"metadata":{"id":"dr6c2JH76yKA"}},{"cell_type":"markdown","source":["####Valores nulos"],"metadata":{"id":"xV-JW-G9TSMT"}},{"cell_type":"code","source":["nuloscal = df_cal.isnull().sum()\n","for columna, cantidad in nuloscal.items():\n","    print(f\"La columna '{columna}' tiene {cantidad} observaciones nulas.\")"],"metadata":{"collapsed":true,"id":"PRLYomlEAAJ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Se identifica y reporta la cantidad de valores nulos en cada columna del DataFrame, proporcionando una visión general de la cantidad de datos faltantes por columna, lo cual es útil para la limpieza y el análisis de datos. La salida del código ayudará a identificar las columnas que podrían necesitar atención adicional para manejar los valores nulos, ya sea mediante imputación, eliminación o alguna otra estrategia de tratamiento de datos faltantes. Para este caso, se puede evidenciar que en el DataFrame no se encuentran datos nulos."],"metadata":{"id":"XjWri9WS6z-Z"}},{"cell_type":"markdown","source":["#### Outliers"],"metadata":{"id":"Kgxm38Gjhxa8"}},{"cell_type":"markdown","source":["Revisamos el método gráfico para las variables con naturaleza numérica, las variables son: 'edad', 'faltas', 'ausencias', 'nota_01', 'nota_02', 'nota_03'."],"metadata":{"id":"2C2HSPspHxLv"}},{"cell_type":"code","source":["columnas_especificas = ['edad', 'faltas', 'ausencias', 'nota_01', 'nota_02', 'nota_03']\n","\n","# Configuramos el número de columnas y filas\n","n_cols = 3\n","n_rows = math.ceil(len(columnas_especificas) / n_cols)\n","\n","# Creamos la figura y los ejes\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n","\n","# Generamos los boxplots\n","for i, col in enumerate(columnas_especificas):\n","    row = i // n_cols\n","    col_idx = i % n_cols\n","    sns.boxplot(data=df_cal, x=col, ax=axes[row, col_idx])\n","    axes[row, col_idx].set_xlabel(col.capitalize())\n","    axes[row, col_idx].set_title(f'Boxplot de {col.capitalize()}')\n","\n","# Eliminamos los ejes vacíos\n","for j in range(i + 1, n_rows * n_cols):\n","    row = j // n_cols\n","    col_idx = j % n_cols\n","    fig.delaxes(axes[row, col_idx])\n","\n","# Ajustamos el layout para que no se solapen las etiquetas\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"F4BUwm1ZEL3Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Revisamos el z-core para aquellas variables con puntos fuera de la caja"],"metadata":{"id":"H6vDclMFMwan"}},{"cell_type":"code","source":["#Obtenemos el conteo de los valores únicos de cada variable para analizar los outliers\n","columnas = ['edad', 'faltas', 'ausencias', 'nota_02']\n","\n","for col in columnas:\n","    conteo_valores = df_cal[col].value_counts().sort_index()\n","    print(f\"Variable:\")\n","    print(conteo_valores)\n","    print(\"---\")"],"metadata":{"id":"L8q7xHiiDio1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col in columnas:\n","\n","    # Calcular z-score para cada columna y obtener su valor absoluto\n","    z_scores = zscore(df_cal[col])\n","    abs_z_scores = np.abs(z_scores)\n","\n","    # Seleccionar los outliers usando un límite de 3\n","    outliers_zscore = df_cal[abs_z_scores > 3]\n","\n","    # Contar el número de valores atípicos\n","    num_outliers = outliers_zscore[col].count()\n","\n","    # Imprimir el nombre de la columna, el valor mínimo del outlier y el número de outliers\n","    print(f\"Variable: {col}\")\n","    print(f\"Número de valores atípicos: {num_outliers}\")\n","    print(f\"Outlier mínimo (z-score): {outliers_zscore[col].min()}\")\n","    print(\"---\")"],"metadata":{"id":"aVINQVHwBAiD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# En el caso de la variable edad, considerando el tamaño relativamente pequeño del dataframe\n","# se decide imputar los 2 outliers con la mediana\n","z_scores = zscore(df_cal['edad'])\n","abs_z_scores = np.abs(z_scores)\n","\n","outliers_zscore = df_cal[abs_z_scores > 3]\n","\n","mediana = df_cal['edad'].median()\n","df_cal.loc[outliers_zscore.index, 'edad'] = mediana\n","\n","print('La mediana de la variable edad es', mediana)\n","print(\"---\")\n","print(df_cal['edad'].value_counts())"],"metadata":{"id":"iRL7bmnyrI8l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> En el caso de la variable faltas, no se consideran valores atípicos al resultado del código z-score pues entre el conteo para 2 faltas y el conteo para 3 faltas hay un sólo registro de diferencia. En este caso, se prefiere hacer una transformación a booleana creando las nuevas categorías 'No falta a clase' con los registros de cero faltas y otra categoría 'Falta a clase' con los registros de 1, 2 y 3 faltas."],"metadata":{"id":"3JKAUQ-T5xNl"}},{"cell_type":"code","source":["df_cal['faltas'].value_counts()"],"metadata":{"id":"kdgAG0xjnWwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aplicamos la recategorización para la variable faltas\n","def Transf(faltas):\n","    if faltas == 0:\n","        return 'No falta a clase'\n","    else:\n","        return 'Falta a clase'\n","\n","# Aplicar la función a la columna 'ausencias'\n","df_cal['faltas'] = df_cal['faltas'].apply(Transf)\n","df_cal['faltas'].value_counts()"],"metadata":{"id":"y0cHwwVgpkX2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aplicamos la recategorización para la variable ausencias\n","def Transf(ausencias):\n","    if ausencias == 0:\n","        return 'No falta a la universidad'\n","    else:\n","        return 'Falta a la universidad'\n","\n","# Aplicar la función a la columna 'ausencias'\n","df_cal['ausencias'] = df_cal['ausencias'].apply(Transf)\n","df_cal['ausencias'].value_counts()"],"metadata":{"id":"bxCWMY53mRgr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# En el caso de la variable ausencias, considerando el tamaño relativamente pequeño del dataframe\n","# se decide imputar los 6 outliers con la mediana\n","# z_scores = zscore(df_cal['ausencias'])\n","# abs_z_scores = np.abs(z_scores)\n","\n","# outliers_zscore = df_cal[abs_z_scores > 3]\n","\n","# mediana = df_cal['ausencias'].median()\n","# df_cal.loc[outliers_zscore.index, 'ausencias'] = mediana\n","\n","# print('La mediana para imputar en la variable ausencias es', mediana)\n","# print(\"---\")\n","# print(df_cal['ausencias'].value_counts().sort_index())"],"metadata":{"id":"1tUricQW138V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Finalmente para nota 02, el punto atípico del boxplot es la nota 0 que cuenta con 13 registros. Consideramos que en este caso y con apoyo del z-score que no muestra atípicos para la variable, no es necesario hacer tratamiento, pues brinda información relevante."],"metadata":{"id":"C4u2m-8jrM2g"}},{"cell_type":"markdown","source":["#### Análisis univariado"],"metadata":{"id":"xVVDnmWYvGwR"}},{"cell_type":"markdown","source":["##### Variable objetivo"],"metadata":{"id":"z38b1Oachm-i"}},{"cell_type":"code","source":["df_cal['nota_03'].describe()"],"metadata":{"id":"nOdZQ8kOvfOL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En la variable nota_03, la calificación promedio fue de 10-11, lo que sugiere que la mayoría de los estudiantes obtuvo notas cercanas a este valor. Sin embargo, hubo una gran variación en los resultados, la nota más baja fue 0, mientras que la más alta fue 20. Una cuarta parte de los estudiantes obtuvo menos de 8 puntos, mientras que la mitad logró una nota igual o menor a 11. Los estudiantes con mejores resultados, es decir, aquellos en el 25% superior, alcanzaron calificaciones de 14 o más."],"metadata":{"id":"-53p0J644xRd"}},{"cell_type":"code","source":["## Distribución de la variable\n","sns.set_style('darkgrid')\n","\n","plt.figure(figsize=(5, 5))\n","sns.histplot(df_cal['nota_03'], bins=len(df_cal['nota_03'].unique()), kde=True, color='blue', edgecolor='black')\n","\n","plt.title('Distribución de la variable objetivo nota_03', fontsize=14)\n","plt.xlabel('Nota 03', fontsize=12)\n","plt.ylabel('Frecuencia', fontsize=12)\n","\n","plt.show()"],"metadata":{"id":"mP7LuPAHso4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#retorno valores shapiro\n","statistic, p_value = stats.shapiro(df_cal['nota_03'])\n","#Imprimir estadistica de prueba\n","print(\"estadistica de prueba:\", statistic)\n","#Imprimir p_value\n","print(\"p-value:\", p_value)"],"metadata":{"id":"7XTBKtHmh2ap"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Los resultados de la prueba de Shapiro para la variable nota_03 indican una estadística de prueba de 0.9287 y un p-valor extremadamente bajo (8.8359e-13). Dado que el p-valor es mucho menor que un nivel de significancia comúnmente utilizado (0.05), rechazamos la hipótesis nula de normalidad. Por lo tanto, podemos concluir que la variable nota_03 no sigue una distribución normal."],"metadata":{"id":"_NEO4VXw6oRw"}},{"cell_type":"markdown","source":["##### Análisis variables numéricas"],"metadata":{"id":"mBWgPhsHWkk7"}},{"cell_type":"markdown","source":["###### Resumen estadístico"],"metadata":{"id":"KSGQsrbuv1Th"}},{"cell_type":"code","source":["# Resumen estadístico para las variables numéricas\n","columnas_especificas = ['edad', 'nota_01', 'nota_02']\n","df_cal[columnas_especificas].describe()"],"metadata":{"id":"7NTQXAAtv5No"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El análisis estadístico de las variables muestra que la edad promedio de los estudiantes es de 16 a 17 años, con poca variabilidad, y que la media de ausencias es 5-6, aunque con una alta dispersión, llegando hasta 28 ausencias en algunos casos. Las calificaciones promedio de nota_01 y nota_02 son 10-11, respectivamente, con una distribución relativamente simétrica en ambas, aunque nota_02 muestra una mayor dispersión y un valor mínimo de 0."],"metadata":{"id":"ww9XTdOqxaRs"}},{"cell_type":"markdown","source":["###### Distribuciones"],"metadata":{"id":"DgEm98R520n5"}},{"cell_type":"code","source":["## Distribución de la variable\n","sns.set_style('darkgrid')\n","\n","n_col = 2  # Número de columnas\n","n_row = (len(columnas_especificas) + 1) // 2  # Número de filas\n","\n","fig, axes = plt.subplots(n_row, n_col, figsize=(12, 6))\n","\n","# Aplanar la matriz de ejes para iterar fácilmente\n","axes = axes.flatten()\n","\n","# Crear histogramas para cada variable\n","for i, columna in enumerate(columnas_especificas):\n","    sns.histplot(df_cal[columna], bins=len(df_cal[columna].unique()), kde=True, color='blue', edgecolor='black', ax=axes[i])\n","    axes[i].set_title(f'Histograma de {columna}', fontsize=14)\n","    axes[i].set_xlabel(columna, fontsize=12)\n","    axes[i].set_ylabel('Frecuencia', fontsize=12)\n","\n","# Eliminar ejes vacíos si hay menos subgráficos que columnas\n","for j in range(len(columnas_especificas), len(axes)):\n","    fig.delaxes(axes[j])\n","\n","# Ajustar el espacio entre las filas y columnas\n","plt.subplots_adjust(hspace=0.8, wspace=0.3)  # Aumenta el hspace para más espacio entre filas\n","\n","plt.show()"],"metadata":{"id":"zMArABKl4vRS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Pruebas de normalidad"],"metadata":{"id":"t0qJ-8dfyYb2"}},{"cell_type":"code","source":["for columna in columnas_especificas:\n","    statistic, p_value = stats.shapiro(df_cal[columna])\n","\n","    print(f\"\\nVariable: {columna}\")\n","    print(\"Estadística de prueba:\", statistic)\n","    print(\"p-value:\", p_value)\n","\n","    if p_value > 0.05:\n","        print(f\"La variable {columna} sigue una distribución normal (El p-value es mayor a 0.05).\")\n","    else:\n","        print(f\"La variable {columna} no sigue una distribución normal (El p-value es menor a 0.05).\")\n","\n","    print(\"\\n-----------------------------------------\")"],"metadata":{"id":"kie67NBW4cEt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["No sólo la variable respuesta NO sigue una distribución normal, sino que además de esto, ninguna de las variables númericas lo sigue."],"metadata":{"id":"-pMGjFjE7F09"}},{"cell_type":"markdown","source":["##### Análisis variables categóricas"],"metadata":{"id":"AdSqzY4KyE1j"}},{"cell_type":"markdown","source":["###### **Variables categóricas con más de dos categorías**"],"metadata":{"id":"wL2xortO0sir"}},{"cell_type":"markdown","source":["####### Exploración"],"metadata":{"id":"URrWBkrgGZMI"}},{"cell_type":"markdown","source":["Para las variables categóricas consideramos que las categorías con menos del 5% se podrían considerar ruido, dependiendo del contexto."],"metadata":{"id":"VZFiK2WaO4Hw"}},{"cell_type":"code","source":["# Lista de columnas para graficar\n","columnas = [col for col in df_cal.select_dtypes(include=['object']).columns\n","                                   if len(df_cal[col].unique()) > 2]\n","\n","# Número de gráficos\n","n = len(columnas)\n","n_cols = 2\n","n_rows = (n + 1) // n_cols  # Calcula el número de filas necesarias\n","\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))\n","axes = axes.flatten()  # Aplanar la matriz de ejes\n","\n","for i, columna in enumerate(columnas):\n","    sizes = df_cal[columna].value_counts()\n","    labels = sizes.index\n","    counts = sizes.values\n","\n","    # Crear el gráfico de dona sin etiquetas\n","    wedges, texts, autotexts = axes[i].pie(sizes, autopct='%1.1f%%', startangle=140, pctdistance=0.85)\n","    centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n","    axes[i].add_artist(centre_circle)\n","\n","    # Crear leyenda con conteos\n","    legend_labels = [f'{label} (Conteo: {count})' for label, count in zip(labels, counts)]\n","    axes[i].legend(wedges, legend_labels, title=\"Categorías\", loc=\"upper left\", bbox_to_anchor=(1, 1), fontsize='small')\n","\n","    axes[i].set_title(f'Diagrama de Dona - {columna}')\n","\n","# Eliminar ejes vacíos si hay menos gráficos que subplots\n","for ax in axes[n:]:\n","    ax.remove()\n","\n","plt.tight_layout(rect=[0, 0, 0.75, 1])  # Ajustar el espacio para la leyenda\n","plt.show()"],"metadata":{"id":"kfBKZ2gOFi9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(columnas)"],"metadata":{"id":"Q4gaUbc84CKq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####### madre_edu y padre_edu"],"metadata":{"id":"sjiJG0EZMQRx"}},{"cell_type":"markdown","source":["En el caso de estas variables, aunque la categoría 0 (ninguna educación) hace ruido al representar menos del 1% de los datos, consideramos que no es correto agruparla con ningún otra categoría, pues el contexto de no tener ningún tipo de educación es muy diferente a tener un mínimo grado pues esto puede influir en la habilidad de leer, escribir y tener conocimiento básico de matemáticas."],"metadata":{"id":"Mf4RnxcIMcgw"}},{"cell_type":"markdown","source":["####### t_examen"],"metadata":{"id":"oOIFHc_E0n_e"}},{"cell_type":"markdown","source":["En el caso de esta categoría, al tener una connotación importante tener mas de una hora con las demás opciones, no se considera viable reagrupar."],"metadata":{"id":"zJKffIK40n_f"}},{"cell_type":"markdown","source":["####### madre_trab"],"metadata":{"id":"Gd5CLZweUp9U"}},{"cell_type":"markdown","source":["En el caso de esta variable, al no haber ningún tipo de relación entre los trabajos, tampoco es correcto pensar en reagrupaciones."],"metadata":{"id":"j8coPA7oUtka"}},{"cell_type":"markdown","source":["####### relacion_fam, tiempo_libre, salir_amigos, cons_alcohol_sem, cons_alcohol_finde, salud\n","\n","\n","\n","\n"],"metadata":{"id":"d795jwVKRRYy"}},{"cell_type":"markdown","source":["En el caso de estas variables, calificadas desde 1 - muy bajo hasta 5 - muy alto, no es correcto reagrupar las categorías para darle tratamiento, ya que cada una representa una percepción y comportamiento en diferentes aspectos, los valores en cada variable tienen un significado específico dentro de su contexto, lo cual impide que se puedan tratar de manera uniforme."],"metadata":{"id":"udS2MQ5ER0_T"}},{"cell_type":"markdown","source":["###### **Variables boolenas**"],"metadata":{"id":"th1HH2LY03m-"}},{"cell_type":"markdown","source":[">Para las variables booleanas hemos definido un umbral de 80-20. Donde las varibles que se distribuyan mayor de 80 y menor de 20, no serán significativas y se eliminarán del modelo."],"metadata":{"id":"x937tGO7i3S2"}},{"cell_type":"code","source":["# Lista de columnas para graficar\n","columnas = [col for col in df_cal.select_dtypes(include=['object']).columns\n","            if len(df_cal[col].unique()) <= 2]\n","\n","columnas_mayor_80 = []\n","\n","# Evaluar cada columna y crear gráficos\n","n = len(columnas)\n","n_cols = 2\n","n_rows = (n + 1) // n_cols  # Calcula el número de filas necesarias\n","\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))\n","axes = axes.flatten()  # Aplanar la matriz de ejes\n","\n","for i, columna in enumerate(columnas):\n","    sizes = df_cal[columna].value_counts(normalize=True) * 100\n","    if any(sizes > 80):\n","        columnas_mayor_80.append(columna)\n","\n","    # Crear gráfico de dona para cada columna\n","    sizes = df_cal[columna].value_counts()\n","    labels = sizes.index\n","    counts = sizes.values\n","\n","    # Crear el gráfico de dona\n","    wedges, texts, autotexts = axes[i].pie(sizes, autopct='%1.1f%%', startangle=140, pctdistance=0.85)\n","    centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n","    axes[i].add_artist(centre_circle)\n","\n","    # Crear leyenda con conteos\n","    legend_labels = [f'{label} (Conteo: {count})' for label, count in zip(labels, counts)]\n","    axes[i].legend(wedges, legend_labels, title=\"Categorías\", loc=\"upper left\", bbox_to_anchor=(1, 1), fontsize='small')\n","\n","    axes[i].set_title(f'Diagrama de dona - {columna}')\n","\n","# Eliminar ejes vacíos si hay menos gráficos que subplots\n","for ax in axes[n:]:\n","    ax.remove()\n","\n","plt.tight_layout(rect=[0, 0, 0.75, 1])  # Ajustar el espacio para la leyenda\n","plt.show()"],"metadata":{"id":"lWqSd6pYf8aM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Columnas con categorías mayores al 80%:\")\n","print(columnas_mayor_80)"],"metadata":{"id":"2qMNPxRJ1m9E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_cal = df_cal.drop(columns = columnas_mayor_80, axis=1)"],"metadata":{"id":"fYu88ILfjDXQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Analisis Bivariado"],"metadata":{"id":"F45ynCoxwu5p"}},{"cell_type":"markdown","source":["##### Variables numéricas"],"metadata":{"id":"r39D8xmLnM2Z"}},{"cell_type":"markdown","source":["###### Correlación y multicolinealidad"],"metadata":{"id":"uQ9gNQyefZBO"}},{"cell_type":"markdown","source":[">Se revisa la correlación de las variables númericas con la variable a predecir"],"metadata":{"id":"0cAxlF4IxzWM"}},{"cell_type":"code","source":["corr_matrix_num = df_cal.select_dtypes(include=np.number).corr()\n","print(\"Correlaciones de las variables numericas con 'nota_03':\")\n","print(corr_matrix_num[\"nota_03\"].sort_values(ascending=False))"],"metadata":{"id":"aVVvydmTupst"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold = 0.8 #este valor se utiliza para correlacion del 80 en adelante\n","filter_= np.abs(corr_matrix_num[\"nota_03\"])> threshold\n","corr_features = corr_matrix_num.columns[filter_].tolist()\n","plt.figure(figsize=(12,8))\n","sns.heatmap(df_cal[corr_features].corr(),annot=True, cmap = 'Blues', fmt=\".2f\")\n","plt.title(\"Correlacion entre variables numericas (Threshold: {})\".format(threshold))\n","plt.show()"],"metadata":{"id":"dt7BFT_hexB_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Variables numéricas\n","sns.pairplot(df_cal[corr_features], diag_kind=\"kde\")"],"metadata":{"id":"9h-CBQwK4XHw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Por la alta correlación de nota_01 y nota_02 con la variable respuesta, se realiza la prueba de correlación entre ellas para revisar multicolinealidad."],"metadata":{"id":"GJzLP0EJnXcg"}},{"cell_type":"code","source":["correlacion_nota_01_02 = df_cal[\"nota_01\"].corr(df_cal[\"nota_02\"])\n","print(\"La correlación entre nota_01 y nota_02 es:\", correlacion_nota_01_02)"],"metadata":{"id":"imDKBwYpTbP8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Al encontrar multicolinealidad entre ellas, se decide eliminar nota_01, pues es la que menor correlación tiene con la variable respuesta.."],"metadata":{"id":"Ph7HdvKjlisF"}},{"cell_type":"code","source":["df_cal = df_cal.drop(['nota_01'], axis=1)"],"metadata":{"id":"zvrezSy5pBq0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Variables categóricas con más de dos categorías"],"metadata":{"id":"d2bf2fQwnTag"}},{"cell_type":"markdown","source":["###### Correlaciones con matriz de Cramér's V"],"metadata":{"id":"f9FWO0YJyTIp"}},{"cell_type":"markdown","source":[">Se revisa la correlación de estas variables categóricas entre sí.\n","Cramér's V usa la tabla de contingencia para calcular el estadístico Chi-cuadrado, que luego se convierte en una medida de la fuerza de la asociación entre las variables categóricas. El estadístico de pruba V va de 0 (sin asociación) a 1 (asociación perfecta)."],"metadata":{"id":"fjwDgY2O4ZJL"}},{"cell_type":"code","source":["def cramers_v(x, y):\n","    # Crear una tabla de contingencia\n","    contingency_table = pd.crosstab(x, y)\n","    # Calcular el estadístico Chi-Cuadrado\n","    chi2_stat, _, _, _ = chi2_contingency(contingency_table)\n","    # Obtener el número de observaciones\n","    n = contingency_table.sum().sum()\n","    # Número de filas y columnas\n","    k = min(contingency_table.shape) - 1\n","    # Calcular Cramer's V\n","    return np.sqrt(chi2_stat / (n * k))"],"metadata":{"id":"6sFBRzHgsPIP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lista de variables categóricas\n","categorical_vars = [col for col in df_cal.select_dtypes(include=['object']).columns\n","                                   if len(df_cal[col].unique()) > 2]\n","\n","# Crear un DataFrame para almacenar los resultados\n","cramers_v_results = pd.DataFrame(index=categorical_vars, columns=categorical_vars)\n","\n","# Calcular Cramér's V para cada par de variables categóricas\n","for var1 in categorical_vars:\n","    for var2 in categorical_vars:\n","        if var1 != var2:\n","            cramers_v_value = cramers_v(df_cal[var1], df_cal[var2])\n","            cramers_v_results.loc[var1, var2] = cramers_v_value\n","        else:\n","            # Para la diagonal (mismo par de variables), podemos poner NaN o 0\n","            cramers_v_results.loc[var1, var2] = np.nan\n","\n","print(\"Matriz de Cramér's V entre variables categóricas:\")\n","print(cramers_v_results)"],"metadata":{"id":"1Ry6tlk5KYfl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">En el caso de las variables categóricas ordinales, no encontramos problemas de colinealidad o multicolinealidad, considerando correlacines entre ellas con un nivel de significancia por encima del 80%."],"metadata":{"id":"ig_ZGlTVqRTt"}},{"cell_type":"markdown","source":["##### Variables categóricas booleanas"],"metadata":{"id":"cpNZiAf-1R36"}},{"cell_type":"markdown","source":["###### Correlaciones con matriz de Cramér's V"],"metadata":{"id":"xIYOQjFG1R4Y"}},{"cell_type":"markdown","source":[">Se revisa la correlación de estas variables categóricas entre sí.\n","Cramér's V usa la tabla de contingencia para calcular el estadístico Chi-cuadrado, que luego se convierte en una medida de la fuerza de la asociación entre las variables categóricas. El estadístico de pruba V va de 0 (sin asociación) a 1 (asociación perfecta)."],"metadata":{"id":"p_E9PZzF1R4Y"}},{"cell_type":"code","source":["# Lista de variables categóricas\n","categorical_vars = [col for col in df_cal.select_dtypes(include=['object']).columns\n","                                   if len(df_cal[col].unique() == 2)]\n","\n","# Crear un DataFrame para almacenar los resultados\n","cramers_v_results = pd.DataFrame(index=categorical_vars, columns=categorical_vars)\n","\n","# Calcular Cramér's V para cada par de variables categóricas\n","for var1 in categorical_vars:\n","    for var2 in categorical_vars:\n","        if var1 != var2:\n","            cramers_v_value = cramers_v(df_cal[var1], df_cal[var2])\n","            cramers_v_results.loc[var1, var2] = cramers_v_value\n","        else:\n","            # Para la diagonal (mismo par de variables), podemos poner NaN o 0\n","            cramers_v_results.loc[var1, var2] = np.nan\n","\n","print(\"Matriz de Cramér's V entre variables categóricas:\")\n","print(cramers_v_results)"],"metadata":{"id":"3ourUusH1R4Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">En el caso de las variables categóricas booleanas, no encontramos problemas de colinealidad o multicolinealidad, considerando correlacines entre ellas con un nivel de significancia por encima del 80%."],"metadata":{"id":"Zs02fIhy1R4a"}},{"cell_type":"markdown","source":["#### Transformaciones"],"metadata":{"id":"PmehLZ7xaAAs"}},{"cell_type":"markdown","source":[">Revisamos los valores únicos dentro de cada variable de manera gráfica"],"metadata":{"id":"MWzvjVRVM8Hb"}},{"cell_type":"code","source":["# Se valida que variables no pueden ser booleanas\n","columnas_categoricas_no_dummies = [col for col in df_cal.select_dtypes(include=['object']).columns\n","                                   if len(df_cal[col].unique()) > 2]\n","\n","print(\"Columnas con más de dos valores únicos:\")\n","print(columnas_categoricas_no_dummies)"],"metadata":{"id":"jrSqMfkKepNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Conversión de variables categóricas con dos valores únicos en variables dummies\n","df_cal = pd.get_dummies(df_cal,\n","                        columns=[col for col in df_cal.select_dtypes(include=['object']).columns\n","                                 if col not in columnas_categoricas_no_dummies],\n","                        drop_first=True)\n","\n","df_cal.info()"],"metadata":{"id":"_bJJnLJQkzaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Para las variables categóricas con más de dos valores únicos se trabaja el método One-Hot para\n","# transformarlas en un conjunto de columnas binarias que indiquen la presencia o ausencia de la categoría\n","\n","columnas_originales = df_cal.columns.tolist()\n","df_cal = pd.get_dummies(df_cal, columns=columnas_categoricas_no_dummies)\n","columnas_resultantes = df_cal.columns.tolist()\n","\n","# Obtener las nuevas columnas generadas por pd.get_dummies() para mas adelante\n","nuevas_columnas = list(set(columnas_resultantes) - set(columnas_originales))\n","print(nuevas_columnas)"],"metadata":{"id":"HLso0b3Z7JBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Se normalizan las columnas númericas dividiendo cada valor por el máximo valor en la columna respectiva,\n","# de forma que los valores queden en un rango entre 0 y 1, exceptuando la variable a precedir 'nota_03'.\n","numeric_columns = []\n","\n","for col in df_cal.select_dtypes(include=np.number).columns:\n","    numeric_columns.append(col)\n","    if col != 'nota_03':\n","        df_cal[col] = df_cal[col] / df_cal[col].max()\n","\n","df_cal[numeric_columns].head()"],"metadata":{"id":"5N2Z8EntoJwF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Se normaliza la variable a predecir (opcional)\n","# df_cal['nota_03'] = df_cal['nota_03']/df_cal['nota_03'].max()\n","# df_cal['nota_03'].head()"],"metadata":{"id":"XTgiweF_ZV5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_cal.head()"],"metadata":{"id":"BCPvivRxAoJy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_cal.info()"],"metadata":{"id":"EHHXiJz2jyLF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Análisis univariado VARIABLES NUEVAS"],"metadata":{"id":"45uzoPVbv9T1"}},{"cell_type":"markdown","source":["##### **Variables boolenas**"],"metadata":{"id":"QdWen9Bz5gru"}},{"cell_type":"markdown","source":[">Para las variables booleanas hemos definido un umbral de 80-20. Donde las varibles que se distribuyan mayor de 80 y menor de 20, no serán significativas y se eliminarán del modelo."],"metadata":{"id":"P9xXtMOK5gr2"}},{"cell_type":"code","source":["# Lista de columnas para graficar\n","nuevas_columnas.sort()\n","columnas = nuevas_columnas\n","\n","columnas_mayor_80 = []\n","\n","# Evaluar cada columna y crear gráficos\n","n = len(columnas)\n","n_cols = 2\n","n_rows = (n + 1) // n_cols  # Calcula el número de filas necesarias\n","\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))\n","axes = axes.flatten()  # Aplanar la matriz de ejes\n","\n","for i, columna in enumerate(columnas):\n","    sizes = df_cal[columna].value_counts(normalize=True) * 100\n","    if any(sizes > 80):\n","        columnas_mayor_80.append(columna)\n","\n","    # Crear gráfico de dona para cada columna\n","    sizes = df_cal[columna].value_counts()\n","    labels = sizes.index\n","    counts = sizes.values\n","\n","    # Crear el gráfico de dona\n","    wedges, texts, autotexts = axes[i].pie(sizes, autopct='%1.1f%%', startangle=140, pctdistance=0.85)\n","    centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n","    axes[i].add_artist(centre_circle)\n","\n","    # Crear leyenda con conteos\n","    legend_labels = [f'{label} (Conteo: {count})' for label, count in zip(labels, counts)]\n","    axes[i].legend(wedges, legend_labels, title=\"Categorías\", loc=\"upper left\", bbox_to_anchor=(1, 1), fontsize='small')\n","\n","    axes[i].set_title(f'Diagrama de dona - {columna}')\n","\n","# Eliminar ejes vacíos si hay menos gráficos que subplots\n","for ax in axes[n:]:\n","    ax.remove()\n","\n","plt.tight_layout(rect=[0, 0, 0.75, 1])  # Ajustar el espacio para la leyenda\n","plt.show()"],"metadata":{"id":"NfbE_HAh5gr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Columnas con categorías mayores al 80%:\")\n","columnas_mayor_80.sort()\n","print(columnas_mayor_80)"],"metadata":{"id":"d-i4H2fu5gr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_cal = df_cal.drop(columns = ['razon_otro', 't_examen_3', 't_examen_4'], axis=1)"],"metadata":{"id":"BHlRbmqJ5gr2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Ultimas transformaciónes**"],"metadata":{"id":"VNQ6-xc-c9Hw"}},{"cell_type":"markdown","source":[">Al analizar las nuevas columnas resultantes de la dummización de las variables categóricas con más de dos valores únicos, identificamos que algunas categorías presentaban una baja representación, lo que podría influir negativamente en la capacidad predictiva del modelo. Para mitigar este efecto, decidimos agrupar valores con características similares dentro de estas variables categóricas. Este agrupamiento nos permitió mejorar la proporción de los datos en cada categoría, generando clases más equilibradas y con mayor representatividad. Además, renombramos estas nuevas variables agrupadas."],"metadata":{"id":"k3sDszYbU_ZI"}},{"cell_type":"code","source":["df_cal1 = df_cal.copy()"],"metadata":{"id":"ttaxW7aRxGVs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combinamos las columnas 4 y 5 de la variable cons_alcohol_finde_4\n","df_cal1['cons_alcohol_finde_4'] = df_cal1['cons_alcohol_finde_4'] | ~df_cal1['cons_alcohol_finde_4'] & df_cal1['cons_alcohol_finde_5']\n","df_cal1 = df_cal1.drop('cons_alcohol_finde_5', axis = 1)"],"metadata":{"id":"_xMc0EYQdPSz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combinamos las columnas de la variable cons_alcohol_sem dejando solo 1 categoría cons_alcohol_sem_reg que corresponde a\n","# los niveles 3,4 y 5 de la variable original y el false sería un consumo esporádico con los niveles 1 y 2\n","df_cal1['cons_alcohol_sem_reg'] = (df_cal1['cons_alcohol_sem_3'] | ~df_cal1['cons_alcohol_sem_3'] & df_cal1['cons_alcohol_sem_4']) | ~df_cal1['cons_alcohol_sem_3'] & ~df_cal1['cons_alcohol_sem_4'] & df_cal1['cons_alcohol_sem_5']\n","df_cal1 = df_cal1.drop(columns = ['cons_alcohol_sem_3', 'cons_alcohol_sem_4', 'cons_alcohol_sem_5'], axis = 1)"],"metadata":{"id":"XZCb3fXIitzV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combinamos las columnas 0 y 1 de la variable padre_edu\n","df_cal1['padre_edu_1'] = df_cal1['padre_edu_1'] | ~df_cal1['padre_edu_1'] & df_cal1['padre_edu_0']\n","df_cal1 = df_cal1.drop('padre_edu_0', axis = 1)"],"metadata":{"id":"F6KzK1XDfEAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combinamos las columnas 0 y 1 de la variable madre_edu\n","df_cal1['madre_edu_1'] = df_cal1['madre_edu_1'] | ~df_cal1['madre_edu_1'] & df_cal1['madre_edu_0']\n","df_cal1 = df_cal1.drop('madre_edu_0', axis = 1)"],"metadata":{"id":"5IARy-xTfz_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Incluimos la opción salud en otros para la variable madre_trab\n","df_cal1['madre_trab_otro'] = df_cal1['madre_trab_otro'] | ~df_cal1['madre_trab_otro'] & df_cal1['madre_trab_salud']\n","df_cal1 = df_cal1.drop('madre_trab_salud', axis = 1)"],"metadata":{"id":"3WK0-cH5gAz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Incluimos la opción en casa en otros para la variable padre_trab\n","df_cal1['padre_trab_otro'] = df_cal1['padre_trab_otro'] | ~df_cal1['padre_trab_otro'] & df_cal1['padre_trab_en_casa']\n","df_cal1 = df_cal1.drop('padre_trab_en_casa', axis = 1)"],"metadata":{"id":"V_-Zn9YoXU1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Incluimos la opción salud y profesor en servicios para la variable padre_trab, entendiendo servicios como trabajos\n","# del sector publico\n","df_cal1['padre_trab_servicios'] = (df_cal1['padre_trab_servicios'] | ~df_cal1['padre_trab_servicios'] & df_cal1['padre_trab_profesor']) | ~df_cal1['padre_trab_servicios'] & ~df_cal1['padre_trab_profesor'] & df_cal1['padre_trab_salud']\n","df_cal1 = df_cal1.drop(columns = ['padre_trab_profesor', 'padre_trab_salud'], axis = 1)"],"metadata":{"id":"JUrqowpuY4dD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Reducimos las opciones de relacion_fam de 1,2,3,4 y 5 a regular (1,2 y 3), buena (4) y excelente (5)\n","df_cal1['relacion_fam_reg'] = (df_cal1['relacion_fam_1'] | ~df_cal1['relacion_fam_1'] & df_cal1['relacion_fam_2']) | ~df_cal1['relacion_fam_1'] & ~df_cal1['relacion_fam_2'] & df_cal1['relacion_fam_3']\n","df_cal1 = df_cal1.drop(columns = ['relacion_fam_1', 'relacion_fam_2', 'relacion_fam_3'], axis = 1)"],"metadata":{"id":"mCTrR0O5k4dW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Renombrar la variable\n","df_cal1 = df_cal1.rename(columns={'relacion_fam_4': 'relacion_fam_buena'})"],"metadata":{"id":"D8NcSTxwk4dP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Renombrar la variable\n","df_cal1 = df_cal1.rename(columns={'relacion_fam_5': 'relacion_fam_exc'})"],"metadata":{"id":"hNYksruWmYtb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Reducimos las opciones de salir_amigos de 1,2,3,4 y 5 a bajo (1 y 2), medio (3) y alto (4 y 5)\n","df_cal1['salir_amigos_bajo'] = df_cal1['salir_amigos_2'] | ~df_cal1['salir_amigos_2'] & df_cal1['salir_amigos_1']\n","df_cal1 = df_cal1.drop(columns = ['salir_amigos_1', 'salir_amigos_2'], axis = 1)"],"metadata":{"id":"iuG_z59Imm4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Renombrar la variable\n","df_cal1 = df_cal1.rename(columns={'salir_amigos_3': 'salir_amigos_medio'})"],"metadata":{"id":"4C14eXJOmegi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combinamos las columnas con el operador | y ~\n","df_cal1['salir_amigos_alto'] = df_cal1['salir_amigos_4'] | ~df_cal1['salir_amigos_4'] & df_cal1['salir_amigos_5']\n","df_cal1 = df_cal1.drop(columns = ['salir_amigos_4', 'salir_amigos_5'], axis = 1)"],"metadata":{"id":"nGPuqp3km6v4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Reducimos las opciones de salud de 1,2,3,4 y 5 a mala (1 y 2), regular (3) y alta (4 y 5)\n","df_cal1['salud_mala'] = df_cal1['salud_1'] | ~df_cal1['salud_1'] & df_cal1['salud_2']\n","df_cal1 = df_cal1.drop(columns = ['salud_1', 'salud_2'], axis = 1)"],"metadata":{"id":"RQziRW0WnvNW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Renombrar la variable\n","df_cal1 = df_cal1.rename(columns={'salud_3': 'salud_regular'})"],"metadata":{"id":"apY07qTmnvNY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combinamos las columnas con el operador | y ~\n","df_cal1['salud_alta'] = df_cal1['salud_4'] | ~df_cal1['salud_4'] & df_cal1['salud_5']\n","df_cal1 = df_cal1.drop(columns = ['salud_4', 'salud_5'], axis = 1)"],"metadata":{"id":"N0iBnfUNnvNY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combinamos las columnas 3 y 4 de la variable t_estudio\n","df_cal1['t_estudio_3'] = df_cal1['t_estudio_3'] | ~df_cal1['t_estudio_3'] & df_cal1['t_estudio_4']\n","df_cal1 = df_cal1.drop('t_estudio_4', axis = 1)"],"metadata":{"id":"97lAYsvboQVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Reducimos las opciones de salud de 1,2,3,4 y 5 a vajo (1 y 2), medio (3) y alto (4 y 5)\n","df_cal1['tiempo_libre_bajo'] = df_cal1['tiempo_libre_1'] | ~df_cal1['tiempo_libre_1'] & df_cal1['tiempo_libre_2']\n","df_cal1 = df_cal1.drop(columns = ['tiempo_libre_1', 'tiempo_libre_2'], axis = 1)"],"metadata":{"id":"_vTPVUAyob6c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Renombrar la variable\n","df_cal1 = df_cal1.rename(columns={'tiempo_libre_3': 'tiempo_libre_medio'})"],"metadata":{"id":"uG41cRQFob6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combinamos las columnas con el operador | y ~\n","df_cal1['tiempo_libre_alto'] = df_cal1['tiempo_libre_4'] | ~df_cal1['tiempo_libre_4'] & df_cal1['tiempo_libre_5']\n","df_cal1 = df_cal1.drop(columns = ['tiempo_libre_4', 'tiempo_libre_5'], axis = 1)"],"metadata":{"id":"Gixh331gob6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["columnas_resultantes = df_cal1.columns.tolist()\n","\n","# Obtener las nuevas columnas generadas por pd.get_dummies() para mas adelante\n","nuevas_columnas = list(set(columnas_resultantes) - set(columnas_originales))\n","\n","# Lista de columnas para graficar\n","nuevas_columnas.sort()\n","columnas = nuevas_columnas\n","\n","columnas_mayor_80 = []\n","\n","# Evaluar cada columna y crear gráficos\n","n = len(columnas)\n","n_cols = 2\n","n_rows = (n + 1) // n_cols  # Calcula el número de filas necesarias\n","\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))\n","axes = axes.flatten()  # Aplanar la matriz de ejes\n","\n","for i, columna in enumerate(columnas):\n","    sizes = df_cal1[columna].value_counts(normalize=True) * 100\n","    if any(sizes > 80):\n","        columnas_mayor_80.append(columna)\n","\n","    # Crear gráfico de dona para cada columna\n","    sizes = df_cal1[columna].value_counts()\n","    labels = sizes.index\n","    counts = sizes.values\n","\n","    # Crear el gráfico de dona\n","    wedges, texts, autotexts = axes[i].pie(sizes, autopct='%1.1f%%', startangle=140, pctdistance=0.85)\n","    centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n","    axes[i].add_artist(centre_circle)\n","\n","    # Crear leyenda con conteos\n","    legend_labels = [f'{label} (Conteo: {count})' for label, count in zip(labels, counts)]\n","    axes[i].legend(wedges, legend_labels, title=\"Categorías\", loc=\"upper left\", bbox_to_anchor=(1, 1), fontsize='small')\n","\n","    axes[i].set_title(f'Diagrama de dona - {columna}')\n","\n","# Eliminar ejes vacíos si hay menos gráficos que subplots\n","for ax in axes[n:]:\n","    ax.remove()\n","\n","plt.tight_layout(rect=[0, 0, 0.75, 1])  # Ajustar el espacio para la leyenda\n","plt.show()"],"metadata":{"id":"IT_u22NghqCN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Para las variables booleanas hemos definido un umbral de 80-20. Donde las varibles que se distribuyan mayor de 80 y menor de 20, no serán significativas y se eliminarán del modelo."],"metadata":{"id":"9BHzDRB-p9tC"}},{"cell_type":"code","source":["print(\"Columnas con categorías mayores al 80%:\")\n","columnas_mayor_80.sort()\n","print(columnas_mayor_80)"],"metadata":{"id":"ksrM0ehGpehK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_cal1 = df_cal1.drop(columns = columnas_mayor_80, axis=1)"],"metadata":{"id":"RphLcUSShaLJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_cal1.shape"],"metadata":{"id":"l1Q_1aVi_v_x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> Al final de la exploración y el preprocesado de la base de datos, el data frame queda con 51 columnas y 395 filas"],"metadata":{"id":"buA7yo-M_1OT"}},{"cell_type":"markdown","source":["#### Modelo de regresión"],"metadata":{"id":"jig8vPNk6hQH"}},{"cell_type":"code","source":["# Definir las columnas para el DataFrame\n","columns = ['DF', 'Modelo', 'Accuracy', 'MSE', 'RMSE', 'MAE', 'MAPE', 'R2', 'R2_adjusted']\n","\n","# Inicializar un DataFrame vacío\n","df_resultados = pd.DataFrame(columns=columns)"],"metadata":{"id":"HwJhjyq08ZtQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Regresión lineal\n"],"metadata":{"id":"pIW0QwQW6rB2"}},{"cell_type":"code","source":["# definir gráfica\n","fig = px.histogram(df_cal1, x = 'nota_03', nbins = 50, width = 650, height = 500, title = '<b>Histograma nota 3<b>')\n","\n","# agregar detalles\n","fig.update_layout(\n","    xaxis_title = '<b>Nota<b>',\n","    yaxis_title = '<b>Frecuencia<b>',\n","    template = 'simple_white',\n","    title_x = 0.5,\n","    barmode='overlay')\n","fig.update_traces(opacity=0.75)\n","\n","fig.show()"],"metadata":{"id":"GUZiqfV86rB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_cal = df_cal1.drop([\"nota_03\"], axis=1)\n","y_cal = df_cal1[\"nota_03\"]\n","# Tranformación logarítmica\n","y_log = np.log(y_cal + 1)\n","# Separar los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X_cal, y_log, test_size=0.3, random_state=59)\n","# Inicializar y entrenar el modelo GBM\n","lr_reg = LinearRegression()\n","lr_reg.fit(X_train, y_train) # .fit: entrenar el modelo\n","# Realizar predicciones\n","lr_pred = lr_reg.predict(X_test)\n","DF = 'df_cal, orig'\n","Modelo = 'Test_original'\n","# Evaluación del modelo (por ejemplo, precisión)\n","accuracy = lr_reg.score(X_test, y_test)\n","print(\"Accuracy:\", accuracy)\n","# Metricas\n","MSE = mean_squared_error(y_test, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, lr_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test )-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"PLuuy7Ga6rB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(df_resultados)"],"metadata":{"id":"73Zcz1L_8lBU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_pred = lr_reg.predict(X_train)"],"metadata":{"id":"iIp_qqqG6rB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, orig'\n","Modelo = 'Train_original'\n","# Metricas\n","accuracy = lr_reg.score(X_train, y_train)\n","print(\"Accuracy: %.2f\" % accuracy)\n","MSE = mean_squared_error(y_train, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, lr_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"fdlmUu666rB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(df_resultados)"],"metadata":{"id":"eYWGCpb5-YpQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> Luego de calcular las métricas del moldelo, le realizamos una transformación inversa a las predicciones y a los valores reales para una mejor visualización."],"metadata":{"id":"NHDCz39KzPrs"}},{"cell_type":"code","source":["# Aplicar la transformación logarítmica inversa\n","y_test_inv = np.exp(y_test) - 1\n","lr_pred_inv = np.exp(lr_pred) - 1"],"metadata":{"id":"22UBYRigzlw2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test_inv.head()"],"metadata":{"id":"fu-WTaQqznHN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(lr_pred_inv)"],"metadata":{"id":"o9p3433h1JOv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ÁLGEBRA VECTORIAL"],"metadata":{"id":"NAKaxFlASC8n"}},{"cell_type":"markdown","source":["#### Exploración"],"metadata":{"id":"Qy1Dp_ZaUd4u"}},{"cell_type":"code","source":["df_alg.shape"],"metadata":{"id":"9jCrKIhTlWKj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg.info()"],"metadata":{"id":"ycivSdG_lWKl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg.head()"],"metadata":{"id":"n3BjhE6inAD2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["___\n","Al revisar la dimesión del data frame de Algebra Vectorial notamos que toda la información esta condensada en una única columna que esta compuesta por las demás variables separadas por \";\" por lo que debemos dividirlas para trabajar con ellas."],"metadata":{"id":"tJVqJBghqCOR"}},{"cell_type":"code","source":["# Separación de las columnas\n","#df_alg1 = df_alg.iloc[:, 0].str.split(';', expand=True)\n","# Eliminar la primera columna que actúa como índice\n","#df_alg1 = df_alg1.drop(df_alg1.columns[0], axis=1)\n","#df_alg1.shape"],"metadata":{"id":"_yNeS4RQnOKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg1 = df_alg"],"metadata":{"id":"yOA4YtfuNPhO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg1.head()"],"metadata":{"id":"iVGF05ranbZl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Además, cambiamos los nombres de las columnas para mejor entendimiento."],"metadata":{"id":"t5YxnstginPh"}},{"cell_type":"code","source":["# Extraer los nombres de las columnas de df_cal_og\n","col_name = df_cal_og.columns.tolist()\n","\n","# Reemplazar los nombres de las columnas en df_agl1 con los extraídos de df_cal_og\n","#df_alg1.columns = col_name"],"metadata":{"id":"6Nb4xjSYinPo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg1.head()"],"metadata":{"id":"BOra_76Rjd9H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg1.info()"],"metadata":{"id":"zju64Fg7X09Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Datos duplicados"],"metadata":{"id":"wR9hIeYLUgnl"}},{"cell_type":"code","source":["df_alg1.duplicated().sum()"],"metadata":{"id":"Zac9eHbojQ4K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["duplicadosalg = df_alg1[df_alg1.duplicated(keep=False)]\n","print(duplicadosalg)"],"metadata":{"id":"777YInciYHZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg1 = df_alg1.drop_duplicates()"],"metadata":{"id":"KFT4fqR5YUii"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Valores nulos"],"metadata":{"id":"MrqVHJkoUq3k"}},{"cell_type":"code","source":["nulosalg1 = df_alg1.isnull().sum()\n","\n","for columna, cantidad in nulosalg1.items():\n","    print(f\"La columna '{columna}' tiene {cantidad} observaciones nulas.\")"],"metadata":{"collapsed":true,"id":"oPKUmMvlAaAc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Resumen estadístico"],"metadata":{"id":"6fi8gzxmY3Hv"}},{"cell_type":"code","source":["pd.set_option('display.float_format', lambda x: '{:.2f}'.format(x))\n","df_alg1.describe()"],"metadata":{"id":"BdoSk2ODghpO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Outliers"],"metadata":{"id":"P4uizgw8KQCa"}},{"cell_type":"markdown","source":["Revisamos el método gráfico para las variables con naturaleza numérica, las variables son: 'edad', 'faltas', 'ausencias', 'nota_01', 'nota_02', 'nota_03'."],"metadata":{"id":"EJtXicOGKQCi"}},{"cell_type":"code","source":["columnas_especificas = ['edad', 'faltas', 'ausencias', 'nota_01', 'nota_02', 'nota_03']\n","\n","# Configuramos el número de columnas y filas\n","n_cols = 3\n","n_rows = math.ceil(len(columnas_especificas) / n_cols)\n","\n","# Creamos la figura y los ejes\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n","\n","# Generamos los boxplots\n","for i, col in enumerate(columnas_especificas):\n","    row = i // n_cols\n","    col_idx = i % n_cols\n","    sns.boxplot(data=df_alg1, x=col, ax=axes[row, col_idx])\n","    axes[row, col_idx].set_xlabel(col.capitalize())\n","    axes[row, col_idx].set_title(f'Boxplot de {col.capitalize()}')\n","\n","# Eliminamos los ejes vacíos\n","for j in range(i + 1, n_rows * n_cols):\n","    row = j // n_cols\n","    col_idx = j % n_cols\n","    fig.delaxes(axes[row, col_idx])\n","\n","# Ajustamos el layout para que no se solapen las etiquetas\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"CgbLcogUKQCi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Revisamos el z-core para aquellas variables con puntos fuera de la caja"],"metadata":{"id":"_t8_qQcDKQCi"}},{"cell_type":"code","source":["#Obtenemos el conteo de los valores únicos de cada variable para analizar los outliers\n","columnas = ['edad', 'faltas', 'ausencias', 'nota_01', 'nota_02', 'nota_03']\n","\n","for col in columnas:\n","    conteo_valores = df_alg1[col].value_counts().sort_index()\n","    print(f\"Variable:\")\n","    print(conteo_valores)\n","    print(\"---\")"],"metadata":{"id":"TTy3BqsyKQCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col in columnas:\n","\n","    # Calcular z-score para cada columna y obtener su valor absoluto\n","    z_scores = zscore(df_alg1[col])\n","    abs_z_scores = np.abs(z_scores)\n","\n","    # Seleccionar los outliers usando un límite de 3\n","    outliers_zscore = df_alg1[abs_z_scores > 3]\n","\n","    # Contar el número de valores atípicos\n","    num_outliers = outliers_zscore[col].count()\n","\n","    # Imprimir el nombre de la columna, el valor mínimo del outlier y el número de outliers\n","    print(f\"Variable: {col}\")\n","    print(f\"Número de valores atípicos: {num_outliers}\")\n","    print(f\"Outlier mínimo (z-score): {outliers_zscore[col].min()}\")\n","    print(\"---\")"],"metadata":{"id":"BZPmScddKQCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# En el caso de la variable faltas, no se consideran valores atípicos al resultado del código z-score pues\n","# entre el conteo para 2 faltas y el conteo para 3 faltas hay un sólo registro de diferencia. En este caso,\n","# se prefiere hacer una transformación a booleana creando las nuevas categorías 'No falta a clase' con los\n","# registros de cero faltas y otra categoría 'Falta a clase' con los registros de 1, 2 y 3 faltas.\n","df_alg1['faltas'].value_counts()"],"metadata":{"id":"oWbu92BpKQCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aplicamos la recategorización para la variable faltas\n","def Transf(faltas):\n","    if faltas == 0:\n","        return 'No falta a clase'\n","    else:\n","        return 'Falta a clase'\n","\n","# Aplicar la función a la columna 'ausencias'\n","df_alg1['faltas'] = df_alg1['faltas'].apply(Transf)\n","df_alg1['faltas'].value_counts()"],"metadata":{"id":"LnX3zzgsKQCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aplicamos la recategorización para la variable ausencias\n","def Transf(ausencias):\n","    if ausencias == 0:\n","        return 'No se asuenta a la U'\n","    else:\n","        return 'Se ausenta a la U'\n","\n","# Aplicar la función a la columna 'ausencias'\n","df_alg1['ausencias'] = df_alg1['ausencias'].apply(Transf)\n","df_alg1['ausencias'].value_counts()"],"metadata":{"id":"4eQPWVKrJ0qu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En el caso de las notas, el z-score nos está mostrando la nota 0 como límite de outliers, pero hemos considerado que es un valor que representa información relevante."],"metadata":{"id":"lANdavAwMsli"}},{"cell_type":"markdown","source":["#### Análisis univariado"],"metadata":{"id":"BMN41SqH2b7Z"}},{"cell_type":"markdown","source":["##### Variable objetivo"],"metadata":{"id":"AiJ9uqv1iGw2"}},{"cell_type":"code","source":["## Distribución de la variable\n","sns.set_style('darkgrid')\n","\n","plt.figure(figsize=(5, 5))\n","sns.histplot(df_alg1['nota_03'], bins=len(df_alg1['nota_03'].unique()), kde=True, color='blue', edgecolor='black')\n","\n","plt.title('Distribución de la variable objetivo nota_03', fontsize=14)\n","plt.xlabel('Nota 03', fontsize=12)\n","plt.ylabel('Frecuencia', fontsize=12)\n","\n","plt.show()"],"metadata":{"id":"mwF3238giI8t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#retorno valores shapiro\n","statistic, p_value = stats.shapiro(df_alg1['nota_03'])\n","#Imprimir estadistica de prueba\n","print(\"estadistica de prueba:\", statistic)\n","#Imprimir p_value\n","print(\"p-value:\", p_value)"],"metadata":{"id":"nL1UWSJiiVJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusion"],"metadata":{"id":"aBUJwSLkKK96"}},{"cell_type":"markdown","source":["##### Análisis variable numéricas"],"metadata":{"id":"mL73ct9YW2H4"}},{"cell_type":"markdown","source":["###### Resumen estadístico"],"metadata":{"id":"nOIO4IRH7suS"}},{"cell_type":"code","source":["# Resumen estadístico para las variables numéricas\n","columnas_especificas = ['nota_01', 'nota_02']\n","df_alg1[columnas_especificas].describe()"],"metadata":{"id":"9J0_xAas7sud"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El análisis estadístico de las variables muestra que la edad promedio de los estudiantes es de 16 a 17 años, con poca variabilidad, y que la media de ausencias es 3-4, aunque con una alta dispersión, llegando hasta 32 ausencias en algunos casos. Las calificaciones promedio de nota_01 y nota_02 son 11-12, con una distribución relativamente simétrica en ambas, aunque nota_02 muestra una mayor dispersión."],"metadata":{"id":"UE_yPq2o7sud"}},{"cell_type":"markdown","source":["###### Distribuciones"],"metadata":{"id":"zpbdi8Ia7sud"}},{"cell_type":"code","source":["## Distribución de la variable\n","sns.set_style('darkgrid')\n","\n","n_col = 2  # Número de columnas\n","n_row = (len(columnas_especificas) + 1) // 2  # Número de filas\n","\n","fig, axes = plt.subplots(n_row, n_col, figsize=(12, 6))\n","\n","# Aplanar la matriz de ejes para iterar fácilmente\n","axes = axes.flatten()\n","\n","# Crear histogramas para cada variable\n","for i, columna in enumerate(columnas_especificas):\n","    sns.histplot(df_alg1[columna], bins=len(df_alg1[columna].unique()), kde=True, color='blue', edgecolor='black', ax=axes[i])\n","    axes[i].set_title(f'Histograma de {columna}', fontsize=14)\n","    axes[i].set_xlabel(columna, fontsize=12)\n","    axes[i].set_ylabel('Frecuencia', fontsize=12)\n","\n","# Eliminar ejes vacíos si hay menos subgráficos que columnas\n","for j in range(len(columnas_especificas), len(axes)):\n","    fig.delaxes(axes[j])\n","\n","# Ajustar el espacio entre las filas y columnas\n","plt.subplots_adjust(hspace=0.8, wspace=0.3)  # Aumenta el hspace para más espacio entre filas\n","\n","plt.show()"],"metadata":{"id":"Hw5ou52Y7sud"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Pruebas de normalidad"],"metadata":{"id":"eRUfh9aN7sue"}},{"cell_type":"code","source":["for columna in columnas_especificas:\n","    statistic, p_value = stats.shapiro(df_alg1[columna])\n","\n","    print(f\"\\nVariable: {columna}\")\n","    print(\"Estadística de prueba:\", statistic)\n","    print(\"p-value:\", p_value)\n","\n","    if p_value > 0.05:\n","        print(f\"La variable {columna} sigue una distribución normal (El p-value es mayor a 0.05).\")\n","    else:\n","        print(f\"La variable {columna} no sigue una distribución normal (El p-value es menor a 0.05).\")\n","\n","    print(\"\\n-----------------------------------------\")"],"metadata":{"id":"A99py6xx7sue"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["No sólo la variable respuesta NO sigue una distribución normal, sino que además de esto, ninguna de las variables númericas lo sigue."],"metadata":{"id":"O0aQYwjb7sue"}},{"cell_type":"markdown","source":["##### Análisis variables categóricas"],"metadata":{"id":"2GGl7mzI3YsP"}},{"cell_type":"markdown","source":["##### **Variables categóricas con más de dos categorías**"],"metadata":{"id":"Y-LNT1Da3Yse"}},{"cell_type":"markdown","source":["###### Exploración"],"metadata":{"id":"ZTK0sOQucjQu"}},{"cell_type":"code","source":["# Lista de columnas para graficar\n","columnas = [col for col in df_alg1.select_dtypes(include=['object']).columns\n","                                   if len(df_alg1[col].unique()) > 2]\n","\n","# Número de gráficos\n","n = len(columnas)\n","n_cols = 2\n","n_rows = (n + 1) // n_cols  # Calcula el número de filas necesarias\n","\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))\n","axes = axes.flatten()  # Aplanar la matriz de ejes\n","\n","for i, columna in enumerate(columnas):\n","    sizes = df_alg1[columna].value_counts()\n","    labels = sizes.index\n","    counts = sizes.values\n","\n","    # Crear el gráfico de dona sin etiquetas\n","    wedges, texts, autotexts = axes[i].pie(sizes, autopct='%1.1f%%', startangle=140, pctdistance=0.85)\n","    centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n","    axes[i].add_artist(centre_circle)\n","\n","    # Crear leyenda con conteos\n","    legend_labels = [f'{label} (Conteo: {count})' for label, count in zip(labels, counts)]\n","    axes[i].legend(wedges, legend_labels, title=\"Categorías\", loc=\"upper left\", bbox_to_anchor=(1, 1), fontsize='small')\n","\n","    axes[i].set_title(f'Diagrama de dona - {columna}')\n","\n","# Eliminar ejes vacíos si hay menos gráficos que subplots\n","for ax in axes[n:]:\n","    ax.remove()\n","\n","plt.tight_layout(rect=[0, 0, 0.75, 1])  # Ajustar el espacio para la leyenda\n","plt.show()"],"metadata":{"id":"8b_rk8R3cjQ5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### madre_edu y padre_edu"],"metadata":{"id":"76U9nji8eYbK"}},{"cell_type":"markdown","source":["En el caso de estas variables, aunque la categoría 0 (ninguna educación) hace ruido al representar menos del 1% de los datos, consideramos que no es correto agruparla con ningún otra categoría, pues el contexto de no tener ningún tipo de educación es muy diferente a tener un mínimo grado pues esto puede influir en la habilidad de leer, escribir y tener conocimiento básico de matemáticas."],"metadata":{"id":"NvIZVenleYbW"}},{"cell_type":"markdown","source":["###### t_examen"],"metadata":{"id":"MdekyjQMe4ya"}},{"cell_type":"markdown","source":["En el caso de esta categoría, al tener una connotación importante tener mas de una hora con las demás opciones, no se considera viable reagrupar."],"metadata":{"id":"OfJvAUQHe7br"}},{"cell_type":"markdown","source":["###### relacion_fam, tiempo_libre, salir_amigos, cons_alcohol_sem, cons_alcohol_finde, salud\n","\n","\n","\n","\n"],"metadata":{"id":"J2K7a4QDeqTa"}},{"cell_type":"markdown","source":["En el caso de estas variables, calificadas desde 1 - muy bajo hasta 5 - muy alto, no es correcto reagrupar las categorías para darle tratamiento, ya que cada una representa una percepción y comportamiento en diferentes aspectos, los valores en cada variable tienen un significado específico dentro de su contexto, lo cual impide que se puedan tratar de manera uniforme."],"metadata":{"id":"6StVJz2XeqTx"}},{"cell_type":"markdown","source":["###### padre_trab"],"metadata":{"id":"kXQ9ToStfUe-"}},{"cell_type":"markdown","source":["En el caso de esta variable, al no haber ningún tipo de relación entre los trabajos, tampoco es correcto pensar en reagrupaciones."],"metadata":{"id":"1p070TWsfUfi"}},{"cell_type":"markdown","source":["Para las variables categóricas consideramos que las categorías con menos del 5% se podrían considerar ruido, dependiendo del contexto."],"metadata":{"id":"DfT0Q43XcjQ5"}},{"cell_type":"markdown","source":["##### **Variables boolenas**"],"metadata":{"id":"T9vx6BqB3Ysf"}},{"cell_type":"markdown","source":["Para las variables booleanas hemos definido un umbral de 80-20. Donde las varibles que se distribuyan mayor de 80 y menor de 20, no serán significativas y se eliminarán del modelo."],"metadata":{"id":"FjeVjc4XiNwE"}},{"cell_type":"code","source":["# Lista de columnas para graficar\n","columnas = [col for col in df_alg1.select_dtypes(include=['object']).columns\n","            if len(df_alg1[col].unique()) <= 2]\n","\n","columnas_mayor_80 = []\n","\n","# Evaluar cada columna y crear gráficos\n","n = len(columnas)\n","n_cols = 2\n","n_rows = (n + 1) // n_cols  # Calcula el número de filas necesarias\n","\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))\n","axes = axes.flatten()  # Aplanar la matriz de ejes\n","\n","for i, columna in enumerate(columnas):\n","    sizes = df_alg1[columna].value_counts(normalize=True) * 100\n","    if any(sizes > 80):\n","        columnas_mayor_80.append(columna)\n","\n","    # Crear gráfico de dona para cada columna\n","    sizes = df_alg1[columna].value_counts()\n","    labels = sizes.index\n","    counts = sizes.values\n","\n","    # Crear el gráfico de dona\n","    wedges, texts, autotexts = axes[i].pie(sizes, autopct='%1.1f%%', startangle=140, pctdistance=0.85)\n","    centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n","    axes[i].add_artist(centre_circle)\n","\n","    # Crear leyenda con conteos\n","    legend_labels = [f'{label} (Conteo: {count})' for label, count in zip(labels, counts)]\n","    axes[i].legend(wedges, legend_labels, title=\"Categorías\", loc=\"upper left\", bbox_to_anchor=(1, 1), fontsize='small')\n","\n","    axes[i].set_title(f'Diagrama de dona - {columna}')\n","\n","# Eliminar ejes vacíos si hay menos gráficos que subplots\n","for ax in axes[n:]:\n","    ax.remove()\n","\n","plt.tight_layout(rect=[0, 0, 0.75, 1])  # Ajustar el espacio para la leyenda\n","plt.show()"],"metadata":{"id":"CWwky3U-hUKF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Columnas con categorías mayores al 80%:\")\n","print(columnas_mayor_80)"],"metadata":{"id":"yfW2pUhzLqC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg1 = df_alg1.drop(columns = columnas_mayor_80, axis=1)"],"metadata":{"id":"W3fyWNNgLqDD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Analisis Bivariado"],"metadata":{"id":"Z83c_H_HIoc0"}},{"cell_type":"markdown","source":["##### Variables numéricas"],"metadata":{"id":"E5mt2R8DIoc8"}},{"cell_type":"markdown","source":["###### Correlación y multicolinealidad"],"metadata":{"id":"eTsxpa3Tj5Bo"}},{"cell_type":"markdown","source":["Se revisa la correlación de las variables númericas con la variable a predecir"],"metadata":{"id":"gWFo8IFKj5Bz"}},{"cell_type":"code","source":["corr_matrix_num = df_alg1.select_dtypes(include=np.number).corr()\n","print(\"Correlaciones de las variables numericas con 'nota_03':\")\n","print(corr_matrix_num[\"nota_03\"].sort_values(ascending=False))"],"metadata":{"id":"WO5K6RgUj5B0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold = 0.8 #este valor se utiliza para correlacion del 80 en adelante\n","filter_= np.abs(corr_matrix_num[\"nota_03\"])> threshold\n","corr_features = corr_matrix_num.columns[filter_].tolist()\n","plt.figure(figsize=(12,8))\n","sns.heatmap(df_alg1[corr_features].corr(),annot=True, cmap = 'Blues', fmt=\".2f\")\n","plt.title(\"Correlacion entre variables numericas (Threshold: {})\".format(threshold))\n","plt.show()"],"metadata":{"id":"RX4WNqhIj5B1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Variables numéricas\n","sns.pairplot(df_alg1[corr_features], diag_kind=\"kde\")"],"metadata":{"id":"CC9x2lLYj5B2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Por la alta correlación de nota_01 y nota_02 con la variable respuesta, se realiza la prueba de correlación entre ellas para revisar multicolinealidad."],"metadata":{"id":"nbiIbZmVj5B2"}},{"cell_type":"code","source":["correlacion_nota_01_02 = df_alg1[\"nota_01\"].corr(df_alg1[\"nota_02\"])\n","print(\"La correlación entre nota_01 y nota_02 es:\", correlacion_nota_01_02)"],"metadata":{"id":"D3dNOG_dj5B2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Al encontrar multicolinealidad entre ellas, se decide eliminar nota_01, pues es la que menor correlación tiene con la variable respuesta.."],"metadata":{"id":"_KEswj9Vj5B3"}},{"cell_type":"code","source":["df_alg1 = df_alg1.drop(['nota_01'], axis=1)"],"metadata":{"id":"SyPQvcjPj5B3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Variables categóricas con más de dos categorías"],"metadata":{"id":"-OxZbVYMIoc9"}},{"cell_type":"markdown","source":["Cramér's V usa la tabla de contingencia para calcular el estadístico Chi-cuadrado, que luego se convierte en una medida de la fuerza de la asociación entre las variables categóricas. El estadístico de pruba V va de 0 (sin asociación) a 1 (asociación perfecta)."],"metadata":{"id":"_fkq8Gf0OYSu"}},{"cell_type":"code","source":["def cramers_v(x, y):\n","    # Crear una tabla de contingencia\n","    contingency_table = pd.crosstab(x, y)\n","    # Calcular el estadístico Chi-Cuadrado\n","    chi2_stat, _, _, _ = chi2_contingency(contingency_table)\n","    # Obtener el número de observaciones\n","    n = contingency_table.sum().sum()\n","    # Número de filas y columnas\n","    k = min(contingency_table.shape) - 1\n","    # Calcular Cramer's V\n","    return np.sqrt(chi2_stat / (n * k))"],"metadata":{"id":"duom2qQ3OQPJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####### Correlaciones con matriz de Cramér's V"],"metadata":{"id":"-UZMgcZYIoc9"}},{"cell_type":"markdown","source":["Se revisa la correlación de estas variables categóricas entre sí.\n"],"metadata":{"id":"P2hbbfUSIoc-"}},{"cell_type":"code","source":["# Lista de variables categóricas\n","categorical_vars = [col for col in df_alg1.select_dtypes(include=['object']).columns\n","                                   if len(df_alg1[col].unique()) > 2]\n","\n","# Crear un DataFrame para almacenar los resultados\n","cramers_v_results = pd.DataFrame(index=categorical_vars, columns=categorical_vars)\n","\n","# Calcular Cramér's V para cada par de variables categóricas\n","for var1 in categorical_vars:\n","    for var2 in categorical_vars:\n","        if var1 != var2:\n","            cramers_v_value = cramers_v(df_alg1[var1], df_alg1[var2])\n","            cramers_v_results.loc[var1, var2] = cramers_v_value\n","        else:\n","            # Para la diagonal (mismo par de variables), podemos poner NaN o 0\n","            cramers_v_results.loc[var1, var2] = np.nan\n","\n","print(\"Matriz de Cramér's V entre variables categóricas:\")\n","print(cramers_v_results)"],"metadata":{"id":"xV71RzmGIoc-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En el caso de las variables categóricas ordinales, no encontramos problemas de colinealidad o multicolinealidad, considerando correlacines entre ellas con un nivel de significancia por encima del 80%."],"metadata":{"id":"yUcNmVkuIoc-"}},{"cell_type":"markdown","source":["##### Variables categóricas booleanas"],"metadata":{"id":"LuUbrOonIoc-"}},{"cell_type":"markdown","source":["####### Correlaciones con matriz de Cramér's V"],"metadata":{"id":"NWMp9QKDIoc-"}},{"cell_type":"markdown","source":["Se revisa la correlación de estas variables categóricas entre sí.\n","Cramér's V usa la tabla de contingencia para calcular el estadístico Chi-cuadrado, que luego se convierte en una medida de la fuerza de la asociación entre las variables categóricas. El estadístico de pruba V va de 0 (sin asociación) a 1 (asociación perfecta)."],"metadata":{"id":"cUMESn7PIoc-"}},{"cell_type":"code","source":["# Lista de variables categóricas\n","categorical_vars = [col for col in df_alg1.select_dtypes(include=['object']).columns\n","                                   if len(df_alg1[col].unique()) <= 2]\n","\n","# Crear un DataFrame para almacenar los resultados\n","cramers_v_results = pd.DataFrame(index=categorical_vars, columns=categorical_vars)\n","\n","# Calcular Cramér's V para cada par de variables categóricas\n","for var1 in categorical_vars:\n","    for var2 in categorical_vars:\n","        if var1 != var2:\n","            cramers_v_value = cramers_v(df_alg1[var1], df_alg1[var2])\n","            cramers_v_results.loc[var1, var2] = cramers_v_value\n","        else:\n","            # Para la diagonal (mismo par de variables), podemos poner NaN o 0\n","            cramers_v_results.loc[var1, var2] = np.nan\n","\n","print(\"Matriz de Cramér's V entre variables categóricas:\")\n","print(cramers_v_results)"],"metadata":{"id":"SLSbUzreIoc-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En el caso de las variables categóricas booleanas, no encontramos problemas de colinealidad o multicolinealidad, considerando correlacines entre ellas con un nivel de significancia por encima del 80%."],"metadata":{"id":"vjJsw5z0Ioc_"}},{"cell_type":"markdown","source":["#### Transformaciones"],"metadata":{"id":"QyefZypnaIRE"}},{"cell_type":"code","source":["# Se revisa el tipo de cada variable y se convierten en el que se considera acorde\n","df_alg1.info()"],"metadata":{"id":"MSKY2_FKlPTk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convertir columnas enteras a int\n","columnas_enteras = ['nota_03']\n","df_alg1[columnas_enteras] = df_alg1[columnas_enteras].astype('int64')"],"metadata":{"id":"BRfewsVMf2aL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Luego se valida que variables categóricas no pueden ser dummies por tener mas de dos valores únicos\n","columnas_categoricas_no_dummies = [col for col in df_alg1.select_dtypes(include=['object']).columns\n","                                   if len(df_alg1[col].unique()) > 2]\n","print(\"Columnas con más de dos valores únicos:\")\n","print(columnas_categoricas_no_dummies)"],"metadata":{"id":"Q_W0PUEiaI-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Conversión de variables categóricas con dos valores únicos en variables dummies\n","df_alg1 = pd.get_dummies(df_alg1,\n","                         columns=[col for col in df_alg1.select_dtypes(include=['object']).columns\n","                                  if col not in columnas_categoricas_no_dummies],\n","                        drop_first=True)\n","df_alg1.info()"],"metadata":{"id":"DkpFZQ5ehcN1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Para las variables categóricas con más de dos valores únicos se trabaja el método One-Hot para\n","# transformarlas en un conjunto de columnas binarias que indiquen la presencia o ausencia de la categoría\n","\n","columnas_originales = df_alg1.columns.tolist()\n","df_alg1 = pd.get_dummies(df_alg1, columns=columnas_categoricas_no_dummies)\n","columnas_resultantes = df_alg1.columns.tolist()\n","\n","# Obtener las nuevas columnas generadas por pd.get_dummies() para mas adelante\n","nuevas_columnas = list(set(columnas_resultantes) - set(columnas_originales))\n","print(nuevas_columnas)"],"metadata":{"id":"11qAYyAV-xfE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Se normalizan las columnas númericas dividiendo cada valor por el máximo valor en la columna respectiva,\n","# de forma que los valores queden en un rango entre 0 y 1, exceptuando la variable a precedir 'nota_03'.\n","numericas_columns = []\n","\n","for col in df_alg1.select_dtypes(include=np.number).columns:\n","    numericas_columns.append(col)\n","    if col != 'nota_03':\n","        df_alg1[col] = df_alg1[col] / df_alg1[col].max()\n","\n","df_alg1[numericas_columns].head()"],"metadata":{"id":"klS6QBInp1K8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg1.info()"],"metadata":{"id":"YCgP1KK4cnME"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Análisis univariado VARIABLES NUEVAS"],"metadata":{"id":"iIaRbUEQMqFt"}},{"cell_type":"markdown","source":["###### **Variables boolenas**"],"metadata":{"id":"faHXLSElMqF0"}},{"cell_type":"markdown","source":["Para las variables booleanas hemos definido un umbral de 80-20. Donde las varibles que se distribuyan mayor de 80 y menor de 20, no serán significativas y se eliminarán del modelo."],"metadata":{"id":"1T9PVJORMqF0"}},{"cell_type":"code","source":["# Lista de columnas para graficar\n","nuevas_columnas.sort()\n","columnas = nuevas_columnas\n","\n","columnas_mayor_80 = []\n","\n","# Evaluar cada columna y crear gráficos\n","n = len(columnas)\n","n_cols = 2\n","n_rows = (n + 1) // n_cols  # Calcula el número de filas necesarias\n","\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))\n","axes = axes.flatten()  # Aplanar la matriz de ejes\n","\n","for i, columna in enumerate(columnas):\n","    sizes = df_alg1[columna].value_counts(normalize=True) * 100\n","    if any(sizes > 80):\n","        columnas_mayor_80.append(columna)\n","\n","    # Crear gráfico de dona para cada columna\n","    sizes = df_alg1[columna].value_counts()\n","    labels = sizes.index\n","    counts = sizes.values\n","\n","    # Crear el gráfico de dona\n","    wedges, texts, autotexts = axes[i].pie(sizes, autopct='%1.1f%%', startangle=140, pctdistance=0.85)\n","    centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n","    axes[i].add_artist(centre_circle)\n","\n","    # Crear leyenda con conteos\n","    legend_labels = [f'{label} (Conteo: {count})' for label, count in zip(labels, counts)]\n","    axes[i].legend(wedges, legend_labels, title=\"Categorías\", loc=\"upper left\", bbox_to_anchor=(1, 1), fontsize='small')\n","\n","    axes[i].set_title(f'Diagrama de dona - {columna}')\n","\n","# Eliminar ejes vacíos si hay menos gráficos que subplots\n","for ax in axes[n:]:\n","    ax.remove()\n","\n","plt.tight_layout(rect=[0, 0, 0.75, 1])  # Ajustar el espacio para la leyenda\n","plt.show()"],"metadata":{"id":"At2FVpNFMqF1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Columnas con categorías mayores al 80%:\")\n","columnas_mayor_80.sort()\n","print(columnas_mayor_80)"],"metadata":{"id":"7sWrwF66MqF1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg1 = df_alg1.drop(columns = ['razon_otro'], axis=1)"],"metadata":{"id":"ZsSKHneEMqF1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Ultimas transformaciónes**"],"metadata":{"id":"hQVMNxFpIMQG"}},{"cell_type":"markdown","source":["Al analizar las nuevas columnas resultantes de la dummización de las variables categóricas con más de dos valores únicos, identificamos que algunas categorías presentaban una baja representación, lo que podría influir negativamente en la capacidad predictiva del modelo. Para mitigar este efecto, decidimos agrupar valores con características similares dentro de estas variables categóricas. Este agrupamiento nos permitió mejorar la proporción de los datos en cada categoría, generando clases más equilibradas y con mayor representatividad. Además, renombramos estas nuevas variables agrupadas."],"metadata":{"id":"WMY_MAyUIMQH"}},{"cell_type":"code","source":["df_alg2 = df_alg1.copy()"],"metadata":{"id":"ItIYrRCO8pCJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Incluimos la opción en_casa en otros para la variable madre_trab\n","df_alg2['madre_trab_otro'] = df_alg2['madre_trab_otro'] | ~df_alg2['madre_trab_otro'] & df_alg2['madre_trab_en_casa']\n","df_alg2 = df_alg2.drop('madre_trab_en_casa', axis = 1)"],"metadata":{"id":"tFP-jZMhIMQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Incluimos la opción salud y profesor en servicios para la variable madre_trab, entendiendo servicios como trabajos\n","# del sector publico\n","df_alg2['madre_trab_servicios'] = (df_alg2['madre_trab_servicios'] | ~df_alg2['madre_trab_servicios'] & df_alg2['madre_trab_profesor']) | ~df_alg2['madre_trab_servicios'] & ~df_alg2['madre_trab_profesor'] & df_alg2['madre_trab_salud']\n","df_alg2 = df_alg2.drop(columns = ['madre_trab_profesor', 'madre_trab_salud'], axis = 1)"],"metadata":{"id":"e0XoqG0KIMQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Incluimos la opción en_casa en otros para la variable padre_trab\n","df_alg2['padre_trab_otro'] = df_alg2['padre_trab_otro'] | ~df_alg2['padre_trab_otro'] & df_alg2['padre_trab_en_casa']\n","df_alg2 = df_alg2.drop('padre_trab_en_casa', axis = 1)"],"metadata":{"id":"QtCipPrKIMQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Incluimos la opción salud y profesor en servicios para la variable padre_trab, entendiendo servicios como trabajos\n","# del sector publico\n","df_alg2['padre_trab_servicios'] = (df_alg2['padre_trab_servicios'] | ~df_alg2['padre_trab_servicios'] & df_alg2['padre_trab_profesor']) | ~df_alg2['padre_trab_servicios'] & ~df_alg2['padre_trab_profesor'] & df_alg2['padre_trab_salud']\n","df_alg2 = df_alg2.drop(columns = ['padre_trab_profesor', 'padre_trab_salud'], axis = 1)"],"metadata":{"id":"W16IpvieIMQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["columnas_resultantes = df_alg2.columns.tolist()\n","\n","# Obtener las nuevas columnas generadas por pd.get_dummies() para mas adelante\n","nuevas_columnas = list(set(columnas_resultantes) - set(columnas_originales))\n","\n","# Lista de columnas para graficar\n","nuevas_columnas.sort()\n","columnas = nuevas_columnas\n","\n","columnas_mayor_80 = []\n","\n","# Evaluar cada columna y crear gráficos\n","n = len(columnas)\n","n_cols = 2\n","n_rows = (n + 1) // n_cols  # Calcula el número de filas necesarias\n","\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4 * n_rows))\n","axes = axes.flatten()  # Aplanar la matriz de ejes\n","\n","for i, columna in enumerate(columnas):\n","    sizes = df_alg2[columna].value_counts(normalize=True) * 100\n","    if any(sizes > 80):\n","        columnas_mayor_80.append(columna)\n","\n","    # Crear gráfico de dona para cada columna\n","    sizes = df_alg2[columna].value_counts()\n","    labels = sizes.index\n","    counts = sizes.values\n","\n","    # Crear el gráfico de dona\n","    wedges, texts, autotexts = axes[i].pie(sizes, autopct='%1.1f%%', startangle=140, pctdistance=0.85)\n","    centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n","    axes[i].add_artist(centre_circle)\n","\n","    # Crear leyenda con conteos\n","    legend_labels = [f'{label} (Conteo: {count})' for label, count in zip(labels, counts)]\n","    axes[i].legend(wedges, legend_labels, title=\"Categorías\", loc=\"upper left\", bbox_to_anchor=(1, 1), fontsize='small')\n","\n","    axes[i].set_title(f'Diagrama de dona - {columna}')\n","\n","# Eliminar ejes vacíos si hay menos gráficos que subplots\n","for ax in axes[n:]:\n","    ax.remove()\n","\n","plt.tight_layout(rect=[0, 0, 0.75, 1])  # Ajustar el espacio para la leyenda\n","plt.show()"],"metadata":{"id":"AfxWUPNUIMQI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para las variables booleanas hemos definido un umbral de 80-20. Donde las varibles que se distribuyan mayor de 80 y menor de 20, no serán significativas y se eliminarán del modelo."],"metadata":{"id":"arrNE8RXIMQJ"}},{"cell_type":"code","source":["print(\"Columnas con categorías mayores al 80%:\")\n","columnas_mayor_80.sort()\n","print(columnas_mayor_80)"],"metadata":{"id":"w21q7645IMQJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg2 = df_alg2.drop(columns = columnas_mayor_80, axis=1)"],"metadata":{"id":"0RczJ_RpIMQJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Modelos de regresión"],"metadata":{"id":"Tts1HbgYUQ61"}},{"cell_type":"markdown","source":["##### Regresión logistica"],"metadata":{"id":"R-AAZT5pUcO1"}},{"cell_type":"code","source":["df_alg1[\"nota_03\"].unique()"],"metadata":{"id":"F8nMu1ECU0sM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Transf(nota):\n","    if nota < 12:\n","        return False\n","    else:\n","        return True"],"metadata":{"id":"bkTfziSHYb2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg1['nota_03'] = df_alg1['nota_03'].apply(Transf)"],"metadata":{"id":"KG0eZvV8Ymjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg1['nota_03'].head()"],"metadata":{"id":"OGj7X1acci3S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_alg = df_alg1.drop(\"nota_03\", axis=1)\n","y_alg = df_alg1[\"nota_03\"]\n","\n","# Separar los datos en conjuntos de entrenamiento y prueba\n","x_train, x_test, y_train, y_test = train_test_split(X_alg, y_alg, test_size=0.3, random_state=113)\n","\n","model = LogisticRegression() # definir el modelo\n","model.fit(x_train, y_train) # entrenar el modelo\n","\n","y_pred_train = model.predict(x_train) # guardar la predicción para train\n","y_pred_test = model.predict(x_test) # guardar la predicción para test"],"metadata":{"id":"yr3elC5va5-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Matriz de confusión:\n","cm = confusion_matrix(y_test, y_pred_test, labels=model.classes_) # guardar las clases para la matriz de confusión\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n","disp.plot();\n","print(cm)"],"metadata":{"id":"wUWMAO84blkm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 81\n","\n","print(f\"Accuracy test: {accuracy_score(y_test, y_pred_test)}\")\n","print(f'Precicion: {TP/(TP+FP)}')\n","print(f'Recall (Sensibilidad)): {TP/(TP+FN)}')\n","print(f'F1-score:', f1_score(y_test, y_pred_test, average='binary'))\n","print(f'Especificidad: {TN/(FP+TN)}')"],"metadata":{"id":"Uat0ANQwdpCY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Metricas de entrenamiento\n","cm = confusion_matrix(y_train, y_pred_train, labels=model.classes_) # guardar las clases para la matriz de confusión\n","\n","TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 91\n","\n","print(f\"Accuracy train: {accuracy_score(y_train, y_pred_train)}\")\n","print(f'Precicion: {TP/(TP+FP)}')\n","print(f'Recall (Sensibilidad)): {TP/(TP+FN)}')\n","print(f'F1-score:', f1_score(y_train, y_pred_train, average='binary'))\n","print(f'Especificidad: {TN/(FP+TN)}')"],"metadata":{"id":"Nlo93gJcehEv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc, roc_auc_score\n","y_pred = model.predict_proba(x_test)[::,1]\n","fpr, tpr,_ =roc_curve(y_test, y_pred)\n","auc = roc_auc_score(y_test, y_pred)\n","plt.plot(fpr, tpr,marker='.',label='Logistic (auc= %0.3f)'%auc)\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"0ZWc1-z_fO9m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **SEGUNDA ENTREGA**"],"metadata":{"id":"7EL20hu_6gMN"}},{"cell_type":"markdown","source":["## CÁLCULO DIFERENCIAL"],"metadata":{"id":"7QqZ4RyU9Swd"}},{"cell_type":"markdown","source":["### Selección de variables métodos de filtrado"],"metadata":{"id":"xBKPExaxUvRz"}},{"cell_type":"markdown","source":["#### Selección univariante"],"metadata":{"id":"LW0ABxtg_W0z"}},{"cell_type":"markdown","source":["\n",">Para realizar una mejor comparación de los métodos de seleccion de variables, aplicaremos los métodos *f-regression* y *mutual info regression* a la base de datos df_cal, la aplicaremos a 3 versiones de esta. La base original sin haberle hecho un tratamiento, la primera versión de nuestro tratmiento sólo con las trasformaciones donde hay 75 columnas o vairables y otra version despues de hacerle transformaciones y adicional agrupacciones que contiene 51, agrupaciones finales a criterio nuestro.\n","Esto con el objetivo de revisar si las predicciones mejoran al combinar trabajo manual con los métdos de selección o si por su parte estos trabajan mejor solos."],"metadata":{"id":"5Xs4R5CCvO7H"}},{"cell_type":"code","source":["print(len(df_cal.columns))"],"metadata":{"id":"-HrXAybtYJOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(df_cal1.columns))"],"metadata":{"id":"nmh0C1Df3S02"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression,  f_classif, mutual_info_classif, chi2"],"metadata":{"id":"PdJzntxsAuyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Función de filtro de caracteristicas - stadis. scores\n","def select_kbest(X,y,score_f,k): #se establece una funcion que permite sacar varias funciones de evaluacion\n","    sel_kb = SelectKBest(score_func=score_f, k=k)\n","    sel_kb.fit(X,y)\n","    scores = sel_kb.scores_\n","    pvalues = sel_kb.pvalues_\n","    new_cols = sel_kb.get_support()\n","    print(\"Scores:\\n\", scores, \"\\nP-values:\\n\", pvalues)\n","    return new_cols, scores, pvalues"],"metadata":{"id":"Dvqe2oxS_VzV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> Los siguientes metodos se usan en modelos de regresión"],"metadata":{"id":"LulCGta9jhI4"}},{"cell_type":"markdown","source":["##### Método f regression"],"metadata":{"id":"ivj7IAe4Cu77"}},{"cell_type":"markdown","source":["\n","\n","> Calcula el valor F y los p-valores asociados para evaluar la significancia de cada característica en relación con la variable objetivo, ayudando a seleccionar las características más relevantes.\n","\n"],"metadata":{"id":"AAPBBn1okT1k"}},{"cell_type":"markdown","source":["###### Df_cal (Data frame con 75 columnas)"],"metadata":{"id":"J-3Y1Lvu4e8U"}},{"cell_type":"code","source":["df_cal.columns"],"metadata":{"id":"cZ7DC_uLs8Zu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = df_cal\n","y = df_cal['nota_03']\n","x_new, scores, pvalues = select_kbest(df_cal, y, f_regression, 25) # Obtener columnas seleccionadas\n","df_cal2 = df_cal.iloc[:,x_new] # Nuevo conjunto de datos\n","df_cal2.head() #se escogen los valores p mas pequeños, y las variables tiene relacion con la variable objetivo"],"metadata":{"id":"P_Mn28s9Awq0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Estas son las 25 columnas que el modelo selecciona para trabajar en las predicciones."],"metadata":{"id":"LvFtM8Xj5PrJ"}},{"cell_type":"code","source":["df_cal2.columns"],"metadata":{"id":"4uNHmp0A_Vg2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_cal2.shape"],"metadata":{"id":"cTxInTbeAUzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_cal = df_cal2\n","y_cal = y\n","# Tranformación logarítmica\n","y_log = np.log(y_cal + 1)\n","# Separar los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X_cal, y_log, test_size=0.3, random_state=59)\n","# Inicializar y entrenar el modelo\n","lr_reg = LinearRegression()\n","lr_reg.fit(X_train, y_train) # .fit: entrenar el modelo\n","# Realizar predicciones\n","lr_pred = lr_reg.predict(X_test)"],"metadata":{"id":"F_rUG_3hgql3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 75var'\n","Modelo = 'Test_f_regression'\n","# Evaluación del modelo (por ejemplo, precisión)\n","accuracy = lr_reg.score(X_test, y_test)\n","print(\"Accuracy:\", accuracy)\n","# Metricas TEST\n","MSE = mean_squared_error(y_test, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, lr_pred)\n","print(\"R2: %.2f\" % r2_score(y_test, lr_pred))\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test )-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"Sc5T8ttzgqmG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(df_resultados)"],"metadata":{"id":"fsMYqUMzAyQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_pred = lr_reg.predict(X_train)"],"metadata":{"id":"XJU96CFwgqmG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 75var'\n","Modelo = 'Train_f_regression'\n","# Metricas\n","accuracy = lr_reg.score(X_train, y_train)\n","print(\"Accuracy: %.2f\" % accuracy)\n","MSE = mean_squared_error(y_train, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, lr_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"apOVdQ5JgqmG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(df_resultados)"],"metadata":{"id":"Sud9npi7Bxav"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Df_cal1 (Data frame con 51 columnas)"],"metadata":{"id":"cI8HWBGg4r1-"}},{"cell_type":"code","source":["df_cal1.columns"],"metadata":{"id":"QXs798Zo7U4u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = df_cal1\n","y = df_cal1['nota_03']\n","x_new, scores, pvalues = select_kbest(df_cal1, y, f_regression, 25) # Obtener columnas seleccionadas\n","df_cal3 = df_cal1.iloc[:,x_new] # Nuevo conjunto de datos\n","df_cal3.head() #se escogen los valores p mas pequeños, y las variables tiene relacion con la variable objetivo"],"metadata":{"id":"RsXBTlmz7U5C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Estas son las 25 columnas que el modelo selecciona para trabajar en las predicciones."],"metadata":{"id":"vP7gOR5G7U5C"}},{"cell_type":"code","source":["df_cal3.columns"],"metadata":{"id":"A2HNuAmi7U5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_cal3.shape"],"metadata":{"id":"-7s08NDl7U5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_cal = df_cal3\n","y_cal = y\n","# Tranformación logarítmica\n","y_log = np.log(y_cal + 1)\n","# Separar los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X_cal, y_log, test_size=0.3, random_state=59)\n","# Inicializar y entrenar el modelo\n","lr_reg = LinearRegression()\n","lr_reg.fit(X_train, y_train) # .fit: entrenar el modelo\n","# Realizar predicciones\n","lr_pred = lr_reg.predict(X_test)"],"metadata":{"id":"I964ADdP7U5D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'Test_f_regression'\n","# Evaluación del modelo (por ejemplo, precisión)\n","accuracy = lr_reg.score(X_test, y_test)\n","print(\"Accuracy:\", accuracy)\n","# Metricas TEST\n","MSE = mean_squared_error(y_test, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, lr_pred)\n","print(\"R2: %.2f\" % r2_score(y_test, lr_pred))\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test )-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"vLaz9FxK7U5D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(df_resultados)"],"metadata":{"id":"PRMMKi8y7U5D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_pred = lr_reg.predict(X_train)"],"metadata":{"id":"jVfrn1xj7U5E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'Train_f_regression'\n","# Metricas\n","accuracy = lr_reg.score(X_train, y_train)\n","print(\"Accuracy: %.2f\" % accuracy)\n","MSE = mean_squared_error(y_train, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, lr_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"6yPUwvAg7U5E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)"],"metadata":{"collapsed":true,"id":"FdlJ7_537U5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mostrar el DataFrame con los resultados\n","df_resultados"],"metadata":{"id":"KIrGnKSo74mT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">El modelo de selección *f-regression* muestra un desmejoramiento (aunque no significativo) de las métricas de evaluación del modelo en su etapa de entrenamiento. Adicionalmente se aprecia que este método arroja un r2 ajustado un poco mejores, para la base de datos con las transformaciones y agrupaciones propias, es decir que funciona un poco mejor al combinar el metodo y nuestra manipulación."],"metadata":{"id":"RQTmwBie79hC"}},{"cell_type":"markdown","source":["##### Método mutual info regression"],"metadata":{"id":"zrn9iFcWCqS0"}},{"cell_type":"markdown","source":["> Este método estima cuánta información sobre la variable objetivo se puede obtener de cada característica, siendo útil para la selección de características"],"metadata":{"id":"MknXLYFtkuHf"}},{"cell_type":"markdown","source":["###### Df_cal (Data frame con 75 columnas)"],"metadata":{"id":"38nM__7r8tT7"}},{"cell_type":"code","source":["y = df_cal['nota_03']\n","x_new, scores, pvalues = select_kbest(df_cal, y, mutual_info_regression, 31) # Obtener columnas seleciconadas\n","df_cal4 = df_cal.iloc[:,x_new] # Nuevo conjunto de datos\n","df_cal4.head() #se escogen los valores p mas pequeños, y las variables tiene relacion con la variable objetivo"],"metadata":{"id":"h_H7GFjNCpof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_cal = df_cal4.drop('nota_03', axis=1)\n","y_cal = df_cal[\"nota_03\"]\n","# Tranformación logarítmica\n","y_log = np.log(y_cal + 1)\n","# Separar los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X_cal, y_log, test_size=0.3, random_state=59)\n","# Inicializar y entrenar el modelo GBM\n","lr_reg = LinearRegression()\n","lr_reg.fit(X_train, y_train) # .fit: entrenar el modelo\n","# Realizar predicciones\n","lr_pred = lr_reg.predict(X_test)"],"metadata":{"id":"WJcV5gUBBX3x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 75var'\n","Modelo = 'Test_mutal_info'\n","# Evaluación del modelo (por ejemplo, precisión)\n","accuracy = lr_reg.score(X_test, y_test)\n","print(\"Accuracy:\", accuracy)\n","# Metricas TEST\n","MSE = mean_squared_error(y_test, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, lr_pred)\n","print(\"R2: %.2f\" % r2_score(y_test, lr_pred))\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test )-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"_oEGk2Z-CBlO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'Modelo': [Modelo],\n","    'DF': [DF],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(df_resultados)"],"metadata":{"id":"lK1hXsk6CBlv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_pred = lr_reg.predict(X_train)"],"metadata":{"id":"9Bcd0GGTBX3z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 75var'\n","Modelo = 'Train_mutal_info'\n","# Metricas\n","accuracy = lr_reg.score(X_train, y_train)\n","print(\"Accuracy: %.2f\" % accuracy)\n","MSE = mean_squared_error(y_train, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, lr_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"G2_d7rHKB79g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(df_resultados)"],"metadata":{"id":"ViIJh7Z3B79j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Df_cal1 (Data frame con 51 columnas)"],"metadata":{"id":"Fnz7rs2p9H5r"}},{"cell_type":"code","source":["y = df_cal['nota_03']\n","x_new, scores, pvalues = select_kbest(df_cal, y, mutual_info_regression, 31) # Obtener columnas seleciconadas\n","df_cal5 = df_cal.iloc[:,x_new] # Nuevo conjunto de datos\n","df_cal5.head() #se escogen los valores p mas pequeños, y las variables tiene relacion con la variable objetivo"],"metadata":{"id":"wM3Sr11Q9s02"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_cal = df_cal5.drop('nota_03', axis=1)\n","y_cal = df_cal[\"nota_03\"]\n","# Tranformación logarítmica\n","y_log = np.log(y_cal + 1)\n","# Separar los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X_cal, y_log, test_size=0.3, random_state=59)\n","# Inicializar y entrenar el modelo GBM\n","lr_reg = LinearRegression()\n","lr_reg.fit(X_train, y_train) # .fit: entrenar el modelo\n","# Realizar predicciones\n","lr_pred = lr_reg.predict(X_test)"],"metadata":{"id":"t_ETGxeN9s1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'Test_mutal_info'\n","# Evaluación del modelo (por ejemplo, precisión)\n","accuracy = lr_reg.score(X_test, y_test)\n","print(\"Accuracy:\", accuracy)\n","# Metricas TEST\n","MSE = mean_squared_error(y_test, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, lr_pred)\n","print(\"R2: %.2f\" % r2_score(y_test, lr_pred))\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test )-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"Rj4ZKmHl9s1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(df_resultados)"],"metadata":{"id":"BZb0l9RE9s1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_pred = lr_reg.predict(X_train)"],"metadata":{"id":"ZxMdYAJn9s1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'Train_mutal_info'\n","# Metricas\n","accuracy = lr_reg.score(X_train, y_train)\n","print(\"Accuracy: %.2f\" % accuracy)\n","MSE = mean_squared_error(y_train, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, lr_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"mnkHhf5X9s1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)"],"metadata":{"id":"X3HWr3559s1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mostrar el DataFrame con los resultados\n","df_resultados"],"metadata":{"id":"-xBhI7439xhE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Revisando las diferencias en los resultados del r2 ajustado para train y test, el modelo que presenta un rendimiento más equilibrado y mejor capacidad de generalización es el df_cal de 51 variables con el método f_regression. Aunque hay un ligero descenso en el R² ajustado en el conjunto de prueba, no es alarmante y es la menor diferencia."],"metadata":{"id":"M3hWn95YKcMe"}},{"cell_type":"code","source":["df_resultados[(df_resultados['Modelo'] == 'Test_original') | (df_resultados['Modelo'] == 'Test_f_regression')\n","                | (df_resultados['Modelo'] == 'Test_mutal_info')]"],"metadata":{"id":"alAcW4lC_vpt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Las mejores métricas para el modelo de Test las presenta el DataFrame al que se le aplican todas las transformaciones y queda con 51 variables junto con el método f regression.\n","\n",">Este modelo muestra el Accuracy más alto, los menores errores (aunque el MAPE es el segundo más bajo), y los valores más altos tanto para R² como para R² ajustado."],"metadata":{"id":"RC0vEKKpJYm2"}},{"cell_type":"code","source":["df_resultados[(df_resultados['Modelo'] == 'Train_original') | (df_resultados['Modelo'] == 'Train_f_regression')\n","                | (df_resultados['Modelo'] == 'Train_mutal_info')]"],"metadata":{"id":"Ty42cDAMAn1S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Las mejores métricas para el modelo de Train las presenta el DataFrame original, ya que, en comparación con el segundo mejor, que es el DataFrame con todas las transformaciones y el método mutual info regression, el modelo original obtiene mejores resultados.\n","\n",">Esto se debe a que, de las cuatro métricas de error, presenta los menores valores en dos de ellas, mientras que las otras dos son iguales. Además, observamos que el DataFrame original tiene un valor más alto en cuanto a Accuracy y R². El valor del R² ajustado es mejor en el DataFrame con todas las transformaciones y el método mutual info regression, debido a que incluye más variables. Sin embargo, este último no logra superar el rendimiento general del DataFrame original."],"metadata":{"id":"rDBx_JfKKJga"}},{"cell_type":"code","source":["df_resultados[(df_resultados['DF'] == 'df_cal, orig')\n","              | ((df_resultados['DF'] == 'df_cal, 51var') & (df_resultados['Modelo'] == 'Test_mutal_info'))\n","              | ((df_resultados['DF'] == 'df_cal, 51var') & (df_resultados['Modelo'] == 'Train_mutal_info'))]"],"metadata":{"id":"-aWMtAiRE_rw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">De lo anterior concluimos que el mejor modelo debe ser el que toma el DF con todas las transformaciones y ademas le aplica el método *f regression* ya que presenta mejor metrica en Test y menor GAP entre Train y Test."],"metadata":{"id":"cCgp8_2Klqf3"}},{"cell_type":"markdown","source":["#### Umbral de varianza"],"metadata":{"id":"3nrU6U745OHD"}},{"cell_type":"markdown","source":["###### Df_cal (Data frame con 75 columnas)"],"metadata":{"id":"uIhhKqOTZCOU"}},{"cell_type":"code","source":["#Se realiza una copia del data frame original pre-procesado\n","df_cal6= df_cal.copy()"],"metadata":{"id":"aYazOA2U5RQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(df_cal6.columns))"],"metadata":{"id":"qwTANUJsX8c3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y1 = df_cal6['nota_03']\n","df_cal6 = df_cal6.drop('nota_03', axis=1)"],"metadata":{"id":"PlpIpYo1eG26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separación de caracteristicas y objetivo, variables predictoras y objetivo\n","X_class = df_cal6\n","y_class = y1\n","\n","print(X_class.shape)\n","print(y_class.shape)"],"metadata":{"id":"MT1jzJah5YMs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Separacion test and train\n","from sklearn.model_selection import train_test_split\n","\n","#Separacion de conjuntos de train (80%) y test(20%)\n","X_train, X_test, y_train, y_test = train_test_split(X_class,y_class, test_size= 0.3, random_state=42)\n","\n","#Imprimir tamaño del dataset\n","print(\"Tamaño del conjunto de entrenamiento:\", X_train.shape)\n","print(\"Tamaño del conjunto de prueba:\", X_test.shape)"],"metadata":{"id":"zZtJ1iuT5cJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#seleccion de variables usando el metodo Variance Threshold, este metodo selecciona las caracteristicas cuya varianza supera un cierto umbral entonces,\n","#si la varianza de una variable es pequeña la probabilidad de que este contenga un valor igual o parecido es muy elevada por tanto, poco aportará al algoritmo\n","from sklearn.feature_selection import VarianceThreshold\n","\n","#Función de filtro de caracteristicas, que recibe dos ibservaciones\n","def variance_threshold(X,th):\n","    #definimos el filtrador\n","    var_thres=VarianceThreshold(threshold=th)\n","    #Convertimos todas la columnas a numericas sino es posible las eliminamos\n","    X = X.apply(pd.to_numeric, errors='coerce').dropna(axis=1)\n","    #ajustamos el filtrador\n","    var_thres.fit(X)\n","    #retorna un arreglo con las variables que vamos a conservar\n","    new_cols = var_thres.get_support()\n","    #retornar los nombres de las columnas en lugar de la matriz booleana\n","    return X.columns[new_cols]"],"metadata":{"id":"6Rbdc5RX5hT-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Obtener columnas seleccionadas, estableciendo un thershold de 20%\n","# el flitrador elimina las variables que tengan desde el 80% de observaciones iguales (como las constantes)\n","X_new_class = variance_threshold(X_class, 0.20)\n","# definir nuevo dataframe a partir, de X_class que es el df de las observaciones sin la var target, accediento a la columnas y filas predichas\n","df_classification_new = X_class[X_new_class]\n","df_classification_new.head()"],"metadata":{"id":"p2DauN1y5lMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_classification_new.shape"],"metadata":{"id":"oz8qy69X5sV-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_cal = df_classification_new\n","#X_cal = df_selected\n","y_cal = y1\n","\n","# Tranformación logarítmica\n","y_log = np.log(y_cal + 1)\n","# Separar los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X_cal, y_log, test_size=0.3, random_state=59)\n","# Inicializar y entrenar el modelo GBM\n","lr_reg = LinearRegression()\n","lr_reg.fit(X_train, y_train) # .fit: entrenar el modelo\n","# Realizar predicciones\n","lr_pred = lr_reg.predict(X_test)"],"metadata":{"id":"P6WhbneMcUmN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 75var'\n","Modelo = 'Test_variance_threshold'\n","# Evaluación del modelo (por ejemplo, precisión)\n","accuracy = lr_reg.score(X_test, y_test)\n","print(\"Accuracy:\", accuracy)\n","# Metricas TEST\n","MSE = mean_squared_error(y_test, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, lr_pred)\n","print(\"R2: %.2f\" % r2_score(y_test, lr_pred))\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test )-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"eRh_BFiqZkN5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(df_resultados)"],"metadata":{"id":"V7QKfNQeZkN5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_pred = lr_reg.predict(X_train)"],"metadata":{"id":"ZRVP5X90jvy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 75var'\n","Modelo = 'Train_variance_threshold'\n","# Metricas\n","accuracy = lr_reg.score(X_train, y_train)\n","print(\"Accuracy: %.2f\" % accuracy)\n","MSE = mean_squared_error(y_train, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, lr_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"8FbkD4-ObbtB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(df_resultados)"],"metadata":{"id":"8JQ28TIfbi6u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Df_cal (Data frame con 51 columnas)"],"metadata":{"id":"bCZWLdtlZING"}},{"cell_type":"code","source":["#Se realiza una copia del data frame original pre-procesado\n","df_cal6= df_cal1.copy()"],"metadata":{"id":"67mKFkHbZING"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(df_cal6.columns))"],"metadata":{"id":"9Vibvu2EZING"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y1 = df_cal6['nota_03']\n","df_cal6 = df_cal6.drop('nota_03', axis=1)"],"metadata":{"id":"eJLsS9YQZING"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separación de caracteristicas y objetivo, variables predictoras y objetivo\n","X_class = df_cal6\n","y_class = y1\n","\n","print(X_class.shape)\n","print(y_class.shape)"],"metadata":{"id":"odWClTcOZING"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Separacion test and train\n","from sklearn.model_selection import train_test_split\n","\n","#Separacion de conjuntos de train (80%) y test(20%)\n","X_train, X_test, y_train, y_test = train_test_split(X_class,y_class, test_size= 0.3, random_state=42)\n","\n","#Imprimir tamaño del dataset\n","print(\"Tamaño del conjunto de entrenamiento:\", X_train.shape)\n","print(\"Tamaño del conjunto de prueba:\", X_test.shape)"],"metadata":{"id":"UM-GwGsrZINH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Obtener columnas seleccionadas, estableciendo un thershold de 20%\n","# el flitrador elimina las variables que tengan desde el 80% de observaciones iguales (como las constantes)\n","X_new_class = variance_threshold(X_class, 0.20)\n","# definir nuevo dataframe a partir, de X_class que es el df de las observaciones sin la var target, accediento a la columnas y filas predichas\n","df_classification_new = X_class[X_new_class]\n","df_classification_new.head()"],"metadata":{"id":"dhmDl3jCZINH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_classification_new.shape"],"metadata":{"id":"eM6olTLXZINH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_cal = df_classification_new\n","#X_cal = df_selected\n","y_cal = y1\n","\n","# Tranformación logarítmica\n","y_log = np.log(y_cal + 1)\n","# Separar los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X_cal, y_log, test_size=0.3, random_state=59)\n","# Inicializar y entrenar el modelo GBM\n","lr_reg = LinearRegression()\n","lr_reg.fit(X_train, y_train) # .fit: entrenar el modelo\n","# Realizar predicciones\n","lr_pred = lr_reg.predict(X_test)"],"metadata":{"id":"aenBD-BHZINH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'Test_variance_threshold'\n","# Evaluación del modelo (por ejemplo, precisión)\n","accuracy = lr_reg.score(X_test, y_test)\n","print(\"Accuracy:\", accuracy)\n","# Metricas TEST\n","MSE = mean_squared_error(y_test, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, lr_pred)\n","print(\"R2: %.2f\" % r2_score(y_test, lr_pred))\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test )-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"GGYMjVc1bEmr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(df_resultados)"],"metadata":{"id":"oiNZcdVPbEms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_pred = lr_reg.predict(X_train)"],"metadata":{"id":"pAFKgqixj5Tr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'Train_variance_threshold'\n","# Metricas\n","accuracy = lr_reg.score(X_train, y_train)\n","print(\"Accuracy: %.2f\" % accuracy)\n","MSE = mean_squared_error(y_train, lr_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, lr_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, lr_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, lr_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, lr_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"FImIHO5gj5Ts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","df_resultados = pd.concat([df_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(df_resultados)"],"metadata":{"id":"b7oBbb0fj5Ts"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El aumento en los errrores y la desmejoría de las métricas en comparación con la selección univariante es notoria. El método f regression sigue siendo el más óptimo."],"metadata":{"id":"RYM7lqqSlKwF"}},{"cell_type":"markdown","source":["### Optimización de hiper-parámetros"],"metadata":{"id":"NKwT_Cark82i"}},{"cell_type":"markdown","source":["#### Regresión Ridge"],"metadata":{"id":"sR_hADw2VRjk"}},{"cell_type":"code","source":["#Se realiza una copia del data frame original pre-procesado\n","df_cal7= df_cal.copy()\n","X = df_cal7.drop('nota_03', axis=1)\n","Y = df_cal7['nota_03']"],"metadata":{"id":"4oO8NzJZUgNw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import Ridge, LogisticRegression, LinearRegression\n","from sklearn.model_selection import GridSearchCV, cross_val_score\n","\n","RidgeRegression = Ridge()\n","hyperParameters = {'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\n","ridgeRegressor = GridSearchCV(RidgeRegression, hyperParameters, scoring='neg_mean_squared_error')\n","ridgeRegressor.fit(X,y)"],"metadata":{"id":"uQ6S2SHZUxfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Best value for lambda : \",ridgeRegressor.best_params_)\n","print(\"Best score for cost function: \", ridgeRegressor.best_score_)"],"metadata":{"id":"FvPbLKbOVHpA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Regresión Lasso"],"metadata":{"id":"rgumfk5NVMUy"}},{"cell_type":"code","source":["from sklearn.linear_model import Lasso\n","\n","LassoRegression = Lasso()\n","hyperParameters = {'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\n","LassoRegressor = GridSearchCV(LassoRegression, hyperParameters, scoring='neg_mean_squared_error')\n","LassoRegressor.fit(X,y)"],"metadata":{"id":"GmbMNuiHH6IH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Best value for lambda : \",LassoRegressor.best_params_)\n","print(\"Best score for cost function: \", LassoRegressor.best_score_)"],"metadata":{"id":"8Pp4SBGxIGRP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Comparativa"],"metadata":{"id":"Akk6sSheAi_f"}},{"cell_type":"code","source":["# Definir función\n","from sklearn.model_selection import train_test_split\n","def test(models, X, y, iterations = 100):\n","    results = {}\n","    for i in models:\n","        mse_train = []\n","        mse_test = []\n","        for j in range(iterations):\n","            X_train, X_test, y_train, y_test = train_test_split(X,y,\n","                                                                test_size= 0.2)\n","            mse_test.append(metrics.mean_squared_error(y_test,\n","                                            models[i].fit(X_train,\n","                                                         y_train).predict(X_test)))\n","            mse_train.append(metrics.mean_squared_error(y_train,\n","                                             models[i].fit(X_train,\n","                                                          y_train).predict(X_train)))\n","        results[i] = [np.mean(mse_train), np.mean(mse_test)]\n","    return pd.DataFrame(results)"],"metadata":{"id":"FoUGLVkUzBzA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = {'OLS': LinearRegression(),\n","         'Lasso': Lasso(),\n","         'Ridge': Ridge(),}"],"metadata":{"id":"7Ms_5UHezDs3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.options.display.float_format = '{:.5f}'.format\n","test(models, X, y)"],"metadata":{"id":"S6v_zzKSzLdi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lasso_params = {'alpha':[0.01, 0.012, 0.014, 0.016, 0.018, 0.02, 0.022, 0.024, 0.026, 0.028, 0.03]}\n","ridge_params = {'alpha':[0.01, 0.012, 0.014, 0.016, 0.018, 0.02, 0.022, 0.024, 0.026, 0.028, 0.03]}\n","\n","models2 = {'OLS': LinearRegression(),\n","           'Lasso': GridSearchCV(Lasso(),\n","                               param_grid=lasso_params).fit(X, y).best_estimator_,\n","           'Ridge': GridSearchCV(Ridge(),\n","                               param_grid=ridge_params).fit(X, y).best_estimator_,}"],"metadata":{"id":"snLtYzmxzFpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test(models2, X, y)"],"metadata":{"id":"8PIGRLvXzSCl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Otros modelos"],"metadata":{"id":"tpKzq-1OlKnS"}},{"cell_type":"markdown","source":["#### Arboles de deción"],"metadata":{"id":"UDyRGqhLlNEq"}},{"cell_type":"code","source":["# Dividir en independientes y dependiente\n","X, y = df_cal3, df_cal1['nota_03']\n","\n","# Dividir en train y test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 123, test_size= 0.2)\n","\n","# Definir modelo\n","regressor = DecisionTreeRegressor(max_depth=4)   # Cambiado a DecisionTreeRegressor\n","\n","# Entrenar modelo\n","regressor.fit(X_train, y_train)"],"metadata":{"id":"XRqlKqLGl1N1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'Decision_tree_train'\n","y_pred = regressor.predict(X_train)\n","MSE = mean_squared_error(y_train, y_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, y_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, y_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, y_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, y_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"uQvGrq2iqueG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelos_resultados = pd.DataFrame(columns=['DF', 'Modelo', 'Accuracy', 'Precisión', 'Recall', 'F1-score', 'Especificidad'])"],"metadata":{"id":"n_-fSl228Dp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","modelos_resultados = pd.concat([modelos_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(modelos_resultados)"],"metadata":{"id":"XwDrD72XqueG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'Decision_tree_test'\n","y_pred = regressor.predict(X_test)\n","# Metricas TEST\n","MSE = mean_squared_error(y_test, y_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, y_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, y_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, y_pred)\n","print(\"R2: %.2f\" % r2_score(y_test, y_pred))\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test )-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"co-AiLC4queH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","modelos_resultados = pd.concat([modelos_resultados, nueva_fila], ignore_index=True)"],"metadata":{"collapsed":true,"id":"QseLrKXQqueH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mostrar el DataFrame con los resultados\n","modelos_resultados"],"metadata":{"id":"HTLQDLqXqueH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">El modelo de arbol de deción muestra unos peores resultado que el modelo de regresión lineal."],"metadata":{"id":"B9UGeg8Eo-QP"}},{"cell_type":"markdown","source":["#### Bosques Aleatorios"],"metadata":{"id":"TWBDV9BGlPdT"}},{"cell_type":"code","source":["model = RandomForestRegressor(n_estimators = 100, # número de arboles\n","                               criterion = 'squared_error',  #Se refiere al error cuadrático medio.\n","                               max_depth = 4, # estrategia de parada\n","                               max_leaf_nodes = 10, # estrategia de parada\n","                               max_features = None,\n","                               oob_score = False,\n","                               n_jobs = -1,\n","                               random_state = 123)\n","model.fit(X_train, y_train)"],"metadata":{"id":"dNotlbU2pIAE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'Random_forest_train'\n","y_pred = model.predict(X_train)\n","MSE = mean_squared_error(y_train, y_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, y_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, y_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, y_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, y_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"2dX5YjvqtVOg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","modelos_resultados = pd.concat([modelos_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(modelos_resultados)"],"metadata":{"id":"9TYrFjKVtVOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'Random_forest_test'\n","y_pred = model.predict(X_test)\n","MSE = mean_squared_error(y_test, y_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, y_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, y_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, y_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"fF3HRGR7tVOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","modelos_resultados = pd.concat([modelos_resultados, nueva_fila], ignore_index=True)"],"metadata":{"collapsed":true,"id":"3DgDBAYitVOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mostrar el DataFrame con los resultados\n","modelos_resultados"],"metadata":{"id":"h0YXBTwdtVOj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Gradient Boosting"],"metadata":{"id":"_gORYrHClXpq"}},{"cell_type":"code","source":["X, y = df_cal3, df_cal1['nota_03']\n","\n","model = GradientBoostingRegressor()\n","\n","cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n","\n","model.fit(X_train, y_train)"],"metadata":{"id":"x6AEAmPbnRIc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'Gradient_Regressor_test'\n","y_pred = model.predict(X_test)\n","Accuracy = np.NaN\n","print(\"Accuracy:\", Accuracy)\n","MSE = mean_squared_error(y_test, y_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, y_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, y_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, y_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"FbcCbqYOrejH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","modelos_resultados = pd.concat([modelos_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(modelos_resultados)"],"metadata":{"id":"SEXjAaCFs8Fi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'Gradient_Regressor_train'\n","y_pred = model.predict(X_train)\n","Accuracy = np.NaN\n","print(\"Accuracy:\", Accuracy)\n","MSE = mean_squared_error(y_train, y_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, y_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, y_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, y_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, y_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"HNI4rXCFtAOQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","modelos_resultados = pd.concat([modelos_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(modelos_resultados)"],"metadata":{"id":"4PgR6AkCtOZJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### KNN"],"metadata":{"id":"vwsx5REwlZan"}},{"cell_type":"code","source":["#eliminar clases con solo un valor unico\n","class_counts = df_cal1['nota_03'].value_counts()\n","classes_to_keep = class_counts[class_counts > 1].index\n","df_filtered = df_cal1[df_cal1['nota_03'].isin(classes_to_keep)]\n","X = df_cal3[df_cal1['nota_03'].isin(classes_to_keep)]\n","y = df_filtered['nota_03']"],"metadata":{"id":"ff0Nspwku8k6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=123, stratify=y)\n","\n","test_scores = []\n","train_scores = []\n","\n","for i in range(1,15):\n","\n","    knn = KNeighborsClassifier(i)\n","    knn.fit(X_train,y_train)\n","\n","    train_scores.append(knn.score(X_train,y_train))\n","    test_scores.append(knn.score(X_test,y_test))"],"metadata":{"id":"1mmGa2QyttXI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_test_score = max(test_scores)\n","test_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\n","print('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))"],"metadata":{"id":"2i1Cl-9gwVHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["knn = KNeighborsClassifier(8)\n","knn.fit(X_train,y_train)\n","knn.score(X_test,y_test)"],"metadata":{"id":"whTmQwTHwjRc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'KNN_test'\n","y_pred = knn.predict(X_test)\n","MSE = mean_squared_error(y_test, y_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, y_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, y_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, y_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"u5J8p9SXv7pu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","modelos_resultados = pd.concat([modelos_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(modelos_resultados)"],"metadata":{"id":"EkCG5si4v7pu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'KNN_train'\n","y_pred = knn.predict(X_train)\n","MSE = mean_squared_error(y_train, y_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, y_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, y_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, y_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, y_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"ZImrwzb-xIry"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","modelos_resultados = pd.concat([modelos_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(modelos_resultados)"],"metadata":{"id":"6uJuuDxsxKXj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### XGBoost"],"metadata":{"id":"hUi4q5BvldgM"}},{"cell_type":"code","source":["X, y = df_cal3, df_cal1['nota_03']\n","data = load_iris()\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n","\n","# Crear el modelo de regresión\n","bst = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror')\n","\n","# Entrenar el modelo\n","bst.fit(X_train, y_train)"],"metadata":{"id":"ZVIvCPPMyJbi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'XGBoost_test'\n","y_pred = bst.predict(X_test)\n","Accuracy = np.NaN\n","print(\"Accuracy:\", Accuracy)\n","MSE = mean_squared_error(y_test, y_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, y_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, y_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, y_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"Uss0_M5LzIJ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","modelos_resultados = pd.concat([modelos_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(modelos_resultados)"],"metadata":{"id":"VLQmSvMK0g72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'XGBoost_train'\n","y_pred = bst.predict(X_train)\n","Accuracy = np.NaN\n","print(\"Accuracy:\", Accuracy)\n","MSE = mean_squared_error(y_train, y_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, y_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, y_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, y_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, y_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"UeZlKjKy0nm5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","modelos_resultados = pd.concat([modelos_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(modelos_resultados)"],"metadata":{"id":"K4I-6A3Z0nm5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### SVR"],"metadata":{"id":"efV5nU4ClezT"}},{"cell_type":"code","source":["X, y = df_cal3, df_cal1['nota_03']\n","data = load_iris()\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n","\n","model = SVR(kernel='linear')\n","model.fit(X_train, y_train)"],"metadata":{"id":"1rK3fKriyJu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'SVR_lineal_test'\n","y_pred = model.predict(X_test)\n","Accuracy = np.NaN\n","print(\"Accuracy:\", Accuracy)\n","MSE = mean_squared_error(y_test, y_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_test, y_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_test, y_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_test, y_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_test, y_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"bvrHeZnx-kzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","modelos_resultados = pd.concat([modelos_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(modelos_resultados)"],"metadata":{"id":"CW8mkG6R_EKR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DF = 'df_cal, 51var'\n","Modelo = 'SVR_lineal_train'\n","y_pred = model.predict(X_train)\n","Accuracy = np.NaN\n","print(\"Accuracy:\", Accuracy)\n","MSE = mean_squared_error(y_train, y_pred)\n","print(\"MSE: %.2f\" % MSE)\n","RMSE = math.sqrt(mean_squared_error(y_train, y_pred))\n","print(\"RMSE %.2f\" % RMSE)\n","MAE = mean_absolute_error(y_train, y_pred)\n","print(\"MAE: %.2f\" % MAE)\n","MAPE = mean_absolute_percentage_error(y_train, y_pred)\n","print(\"MAPE: %.2f\" % MAPE)\n","r2 = r2_score(y_train, y_pred)\n","print(\"R2: %.2f\" % r2)\n","# R2 ajustado\n","adj_r2 = 1 - (1-r2)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n","print(\"R2-adjusted: %.2f\" % adj_r2)"],"metadata":{"id":"bFgoZPhW_HRY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'MSE': [MSE],\n","    'RMSE': [RMSE],\n","    'MAE': [MAE],\n","    'MAPE': [MAPE],\n","    'R2': [r2],\n","    'R2_adjusted': [adj_r2]\n","})\n","\n","# Concatenar con el DataFrame original\n","modelos_resultados = pd.concat([modelos_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(modelos_resultados)"],"metadata":{"id":"06cx13JV_O5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df_resultados)"],"metadata":{"id":"jpgc4CV1_2pI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A pesar de haber aplicado metodos de seleccion de variables inicialmente para el modelo de regresion lineal, estas opciones son las que tienen metricas peores. Por el contrario los nuevos modelos, desarrollados con la version del df_cal de 51 variables junto con f regression (df_cal3) tienen metricas en su mayoria que superan a estos anteriores.\n","\n","De entre estos, el arbol de decisión aunque no es precisamente el que menos error tiene, su r2 y r2 ajustado son bastante altos y el GAP entre ellos es casi nulo."],"metadata":{"id":"KvlU5v_qA5Gl"}},{"cell_type":"markdown","source":["## ÁLGEBRA VECTORIAL"],"metadata":{"id":"9TXrda9m1ne9"}},{"cell_type":"markdown","source":["### Selección de variables métodos de filtrado"],"metadata":{"id":"qf8qQzeu4xf9"}},{"cell_type":"markdown","source":["#### Selección univariante"],"metadata":{"id":"Q4M4b47YKmi9"}},{"cell_type":"markdown","source":["##### Método f_classif"],"metadata":{"id":"VsVoRL4wzR1i"}},{"cell_type":"markdown","source":["\n","\n","> El método f_classif se usa para evaluar la relación entre las características y la variable objetivo categórica, ayudando en la selección de características relevantes. Un alto valor F y un bajo p-valor indican que la característica es significativa.\n"],"metadata":{"id":"PLXYb8vzzR1j"}},{"cell_type":"markdown","source":["###### Df_alg1 (Data frame con 38 columnas)"],"metadata":{"id":"tQGpKseszR1j"}},{"cell_type":"code","source":["df_alg1.columns"],"metadata":{"id":"h01RHQznzR1j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = df_alg1.drop(columns = ['nota_03'], axis=1)\n","y = df_alg1['nota_03']\n","x_new, scores, pvalues = select_kbest(df_alg1, y, f_classif, 25) # Obtener columnas seleccionadas\n","df_alg3 = df_alg1.iloc[:,x_new] # Nuevo conjunto de datos\n","df_alg3.head() #se escogen los valores p mas pequeños, y las variables tiene relacion con la variable objetivo"],"metadata":{"id":"rE1cFPpqzR1j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Estas son las 25 columnas que el modelo selecciona para trabajar en las predicciones."],"metadata":{"id":"h0GTYowozR1k"}},{"cell_type":"code","source":["df_alg3['nota_03'].unique()"],"metadata":{"id":"XsvHISLoOSOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_alg = df_alg3.drop(columns = ['nota_03'], axis=1)\n","y_alg = df_alg3[\"nota_03\"]\n","\n","# Separar los datos en conjuntos de entrenamiento y prueba\n","x_train, x_test, y_train, y_test = train_test_split(X_alg, y_alg, test_size=0.3, random_state=113)\n","\n","model = LogisticRegression() # definir el modelo\n","model.fit(x_train, y_train) # entrenar el modelo\n","\n","y_pred_train = model.predict(x_train) # guardar la predicción para train\n","y_pred_test = model.predict(x_test) # guardar la predicción para test"],"metadata":{"id":"ZcdctKcZL6Dt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Matriz de confusión:\n","cm = confusion_matrix(y_test, y_pred_test, labels=model.classes_) # guardar las clases para la matriz de confusión\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n","disp.plot();\n","print(cm)"],"metadata":{"id":"YzXXHNiOFp5l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 81\n","\n","DF = 'df_alg, 38var'\n","Modelo = 'f_classif_train'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_train, y_pred_train)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score = f1_score(y_train, y_pred_train, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"Ts4LZL8GFp5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["alg_resultados = pd.DataFrame(columns=['DF', 'Modelo', 'Accuracy', 'Precisión', 'Recall', 'F1-score', 'Especificidad'])"],"metadata":{"id":"013toybOMPk8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"vQMtK_IiLom7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Metricas de entrenamiento\n","cm = confusion_matrix(y_test, y_pred_test, labels=model.classes_) # guardar las clases para la matriz de confusión\n","\n","TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 91\n","\n","TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 81\n","\n","DF = 'df_alg, 38var'\n","Modelo = 'f_classif_test'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_test, y_pred_test)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score = f1_score(y_test, y_pred_test, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"Fe7JGAZOFp5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"VLWE0VcqNEO4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Df_alg2 (Data frame con 31 columnas)"],"metadata":{"id":"YU6PmJM8NJ_O"}},{"cell_type":"code","source":["df_alg2.columns"],"metadata":{"id":"zk1LA5v6NJ_O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = df_alg2.drop(columns = ['nota_03'], axis=1)\n","y = df_alg2['nota_03']\n","x_new, scores, pvalues = select_kbest(df_alg2, y, f_classif, 25) # Obtener columnas seleccionadas\n","df_alg4 = df_alg2.iloc[:,x_new] # Nuevo conjunto de datos\n","df_alg4.head() #se escogen los valores p mas pequeños, y las variables tiene relacion con la variable objetivo"],"metadata":{"id":"jGSFxFr_NJ_P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">Estas son las 25 columnas que el modelo selecciona para trabajar en las predicciones."],"metadata":{"id":"c5biJWy7NJ_P"}},{"cell_type":"code","source":["df_alg4['nota_03'].unique()"],"metadata":{"id":"tPHkFnb4NJ_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg4['nota_03'] = df_alg4['nota_03'].apply(Transf)"],"metadata":{"id":"n3ve3gqzO3Bb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_alg = df_alg4.drop(columns = ['nota_03'], axis=1)\n","y_alg = df_alg4[\"nota_03\"]\n","\n","# Separar los datos en conjuntos de entrenamiento y prueba\n","x_train, x_test, y_train, y_test = train_test_split(X_alg, y_alg, test_size=0.3, random_state=113)\n","\n","model = LogisticRegression() # definir el modelo\n","model.fit(x_train, y_train) # entrenar el modelo\n","\n","y_pred_train = model.predict(x_train) # guardar la predicción para train\n","y_pred_test = model.predict(x_test) # guardar la predicción para test"],"metadata":{"id":"A94gDW82NJ_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Matriz de confusión:\n","cm = confusion_matrix(y_test, y_pred_test, labels=model.classes_) # guardar las clases para la matriz de confusión\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n","disp.plot();\n","print(cm)"],"metadata":{"id":"Ly0mMknbNJ_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 81\n","\n","DF = 'df_alg, 31var'\n","Modelo = 'f_classif_train'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_train, y_pred_train)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score = f1_score(y_train, y_pred_train, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"xGSJi89WNJ_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"zvDyQMoKNJ_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Metricas de entrenamiento\n","cm = confusion_matrix(y_test, y_pred_test, labels=model.classes_) # guardar las clases para la matriz de confusión\n","\n","TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 91\n","\n","TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 81\n","\n","DF = 'df_alg, 31var'\n","Modelo = 'f_classif_test'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_test, y_pred_test)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score = f1_score(y_test, y_pred_test, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"zFgE9DEpNJ_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"icLrA_1dNJ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Método mutual info regression"],"metadata":{"id":"TF4B2iwbzR1o"}},{"cell_type":"markdown","source":["> Este método estima cuánta información sobre la variable objetivo se puede obtener de cada característica, siendo útil para la selección de características"],"metadata":{"id":"U4nWi7m-zR1o"}},{"cell_type":"markdown","source":["###### Df_alg1 (Data frame con 38 columnas)"],"metadata":{"id":"x3rXbLH_zR1p"}},{"cell_type":"code","source":["y = df_alg1['nota_03']\n","x_new, scores, pvalues = select_kbest(df_alg1, y, mutual_info_regression, 31) # Obtener columnas seleciconadas\n","df_alg4 = df_alg1.iloc[:,x_new] # Nuevo conjunto de datos\n","df_alg4.head() #se escogen los valores p mas pequeños, y las variables tiene relacion con la variable objetivo"],"metadata":{"id":"aVu1KWFazR1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg4['nota_03'].unique()"],"metadata":{"id":"sHaeQ3y0RjQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_alg = df_alg4.drop(\"nota_03\", axis=1)\n","y_alg = df_alg4[\"nota_03\"]\n","\n","# Separar los datos en conjuntos de entrenamiento y prueba\n","x_train, x_test, y_train, y_test = train_test_split(X_alg, y_alg, test_size=0.3, random_state=113)\n","\n","model = LogisticRegression() # definir el modelo\n","model.fit(x_train, y_train) # entrenar el modelo\n","\n","y_pred_train = model.predict(x_train) # guardar la predicción para train\n","y_pred_test = model.predict(x_test) # guardar la predicción para test"],"metadata":{"id":"tBIlSR90K4mv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Matriz de confusión:\n","cm = confusion_matrix(y_test, y_pred_test, labels=model.classes_) # guardar las clases para la matriz de confusión\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n","disp.plot();\n","print(cm)"],"metadata":{"id":"vReeQK50LDkF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 81\n","\n","DF = 'df_alg, 38var'\n","Modelo = 'f_mutual_info_train'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_train, y_pred_train)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score = f1_score(y_train, y_pred_train, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"pCmipvhiR7bh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"npQRSmqTR7bh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Metricas de entrenamiento\n","cm = confusion_matrix(y_test, y_pred_test, labels=model.classes_) # guardar las clases para la matriz de confusión\n","\n","TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 91\n","\n","TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 81\n","\n","DF = 'df_alg, 38var'\n","Modelo = 'f_mutual_info_test'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_test, y_pred_test)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score = f1_score(y_test, y_pred_test, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"f7dZO0jTR7bh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"Xl7PUdQ2R7bi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Df_alg2 (Data frame con 31 columnas)"],"metadata":{"id":"CA5R94tSM4Io"}},{"cell_type":"code","source":["y = df_alg2['nota_03']\n","x_new, scores, pvalues = select_kbest(df_alg2, y, mutual_info_regression, 28) # Obtener columnas seleciconadas\n","df_alg5 = df_alg2.iloc[:,x_new] # Nuevo conjunto de datos\n","df_alg5.head() #se escogen los valores p mas pequeños, y las variables tiene relacion con la variable objetivo"],"metadata":{"id":"jydAVqbkM4Ip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg5['nota_03'].unique()"],"metadata":{"id":"SZefXEe1M4Iq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alg5['nota_03'] = df_alg5['nota_03'].apply(Transf)"],"metadata":{"id":"aXCrMmRFSTyy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_alg = df_alg5.drop(\"nota_03\", axis=1)\n","y_alg = df_alg5[\"nota_03\"]\n","\n","# Separar los datos en conjuntos de entrenamiento y prueba\n","x_train, x_test, y_train, y_test = train_test_split(X_alg, y_alg, test_size=0.3, random_state=113)\n","\n","model = LogisticRegression() # definir el modelo\n","model.fit(x_train, y_train) # entrenar el modelo\n","\n","y_pred_train = model.predict(x_train) # guardar la predicción para train\n","y_pred_test = model.predict(x_test) # guardar la predicción para test"],"metadata":{"id":"GePJEqb-N9de"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Matriz de confusión:\n","cm = confusion_matrix(y_test, y_pred_test, labels=model.classes_) # guardar las clases para la matriz de confusión\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n","disp.plot();\n","print(cm)"],"metadata":{"id":"XlVsxAWNM4Iq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 81\n","\n","DF = 'df_alg, 31var'\n","Modelo = 'f_mutual_info_train'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_train, y_pred_train)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score = f1_score(y_train, y_pred_train, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"G3kOVyPXSKkB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"sSNFV204SKkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Metricas de entrenamiento\n","cm = confusion_matrix(y_test, y_pred_test, labels=model.classes_) # guardar las clases para la matriz de confusión\n","\n","TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 91\n","\n","TP=cm[1,1] # El modelo dijo que eran 1 y en la realidad son 1, verdaderos positivos - 89\n","FP=cm[0,1] # El modelo dijo que eran 1, en la realidad son 0, falsos positivos - 19\n","FN=cm[1,0] # El modelo dijo que eran 0, en la realidad son 1, falsos negativos - 6\n","TN=cm[0,0] # El modelo dijo que eran 0 y en la realidad son 0, verdaderos negativos - 81\n","\n","DF = 'df_alg, 31var'\n","Modelo = 'f_mutual_info_test'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_test, y_pred_test)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score = f1_score(y_test, y_pred_test, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"EwyzXvw2SKkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"tjoyoNWDSKkC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Por su GAP, preferimos el f_classif de la base con 31 variables, pues generaliza mejor."],"metadata":{"id":"KoLsE8owSjgo"}},{"cell_type":"markdown","source":["### Optimización de hiper-parámetros"],"metadata":{"id":"YqNDOK3qTE6L"}},{"cell_type":"markdown","source":["#### Regresión Ridge"],"metadata":{"id":"vtQqi8wFTE6L"}},{"cell_type":"code","source":["#Se realiza una copia del data frame original pre-procesado\n","df_alg7= df_alg1.copy()\n","X = df_alg7.drop('nota_03', axis=1)\n","Y = df_alg7['nota_03']"],"metadata":{"id":"wI5cpdEcTE6L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import Ridge, LogisticRegression, LinearRegression\n","from sklearn.model_selection import GridSearchCV, cross_val_score\n","\n","RidgeRegression = Ridge()\n","hyperParameters = {'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\n","ridgeRegressor = GridSearchCV(RidgeRegression, hyperParameters, scoring='neg_mean_squared_error')\n","ridgeRegressor.fit(X,y)"],"metadata":{"id":"jHj9eX4bTE6L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Best value for lambda : \",ridgeRegressor.best_params_)\n","print(\"Best score for cost function: \", ridgeRegressor.best_score_)"],"metadata":{"id":"OLksvKDcTE6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Regresión Lasso"],"metadata":{"id":"mpEnKn0uTE6M"}},{"cell_type":"code","source":["from sklearn.linear_model import Lasso\n","\n","LassoRegression = Lasso()\n","hyperParameters = {'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\n","LassoRegressor = GridSearchCV(LassoRegression, hyperParameters, scoring='neg_mean_squared_error')\n","LassoRegressor.fit(X,y)"],"metadata":{"id":"NcLIP42-TE6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Best value for lambda : \",LassoRegressor.best_params_)\n","print(\"Best score for cost function: \", LassoRegressor.best_score_)"],"metadata":{"id":"OxB4v3-gTE6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Comparativa"],"metadata":{"id":"7BUnaJy_TE6M"}},{"cell_type":"code","source":["# Definir función\n","from sklearn.model_selection import train_test_split\n","def test(models, X, y, iterations = 100):\n","    results = {}\n","    for i in models:\n","        mse_train = []\n","        mse_test = []\n","        for j in range(iterations):\n","            X_train, X_test, y_train, y_test = train_test_split(X,y,\n","                                                                test_size= 0.2)\n","            mse_test.append(metrics.mean_squared_error(y_test,\n","                                            models[i].fit(X_train,\n","                                                         y_train).predict(X_test)))\n","            mse_train.append(metrics.mean_squared_error(y_train,\n","                                             models[i].fit(X_train,\n","                                                          y_train).predict(X_train)))\n","        results[i] = [np.mean(mse_train), np.mean(mse_test)]\n","    return pd.DataFrame(results)"],"metadata":{"id":"lSSQkz5XTE6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = {'OLS': LinearRegression(),\n","         'Lasso': Lasso(),\n","         'Ridge': Ridge(),}"],"metadata":{"id":"OXNJ8xeYTE6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.options.display.float_format = '{:.5f}'.format\n","test(models, X, y)"],"metadata":{"id":"jJSWCDa5TE6N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lasso_params = {'alpha':[0.01, 0.012, 0.014, 0.016, 0.018, 0.02, 0.022, 0.024, 0.026, 0.028, 0.03]}\n","ridge_params = {'alpha':[0.01, 0.012, 0.014, 0.016, 0.018, 0.02, 0.022, 0.024, 0.026, 0.028, 0.03]}\n","\n","models2 = {'OLS': LinearRegression(),\n","           'Lasso': GridSearchCV(Lasso(),\n","                               param_grid=lasso_params).fit(X, y).best_estimator_,\n","           'Ridge': GridSearchCV(Ridge(),\n","                               param_grid=ridge_params).fit(X, y).best_estimator_,}"],"metadata":{"id":"jBfrVNQ6TE6N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test(models2, X, y)"],"metadata":{"id":"wFVGqEsDTE6N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Otros modelos"],"metadata":{"id":"6Xh3cS1iTcPU"}},{"cell_type":"markdown","source":["#### Arboles de deción\n"],"metadata":{"id":"nsKk6rRyTmzc"}},{"cell_type":"code","source":["# Dividir en independientes y dependiente\n","X, y = df_alg3.drop('nota_03', axis=1), df_alg3['nota_03']\n","\n","# Definir modelo\n","clf = tree.DecisionTreeClassifier(max_depth=5)  # Ajusta el max_depth según sea necesario\n","\n","# Dividir en train y test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 123, test_size= 0.2)\n","\n","# Entrenar modelo\n","clf = clf.fit(X_train, y_train)"],"metadata":{"id":"S-PJgwwRTjiV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizar el árbol\n","dot_data = tree.export_graphviz(clf,\n","                                feature_names=df_alg3.drop('nota_03', axis=1).columns,\n","                                class_names=list(map(str, df_alg3['nota_03'].unique())),\n","                                filled=True, rounded=True,\n","                                special_characters=True)\n","graph = graphviz.Source(dot_data)\n","graph.format = 'png'\n","graph.render('decision_tree', view=True)\n","\n","# Mostrar la imagen\n","Image(\"decision_tree.png\")"],"metadata":{"id":"bSoT6u_BTnkf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hacer predicciones en el conjunto de entrenamiento\n","y_pred_train = clf.predict(X_train)\n","\n","# Calcular la matriz de confusión\n","cm = confusion_matrix(y_train, y_pred_train)\n","\n","# Extraer valores de la matriz de confusión\n","TP = cm[1, 1]  # Verdaderos Positivos\n","FP = cm[0, 1]  # Falsos Positivos\n","FN = cm[1, 0]  # Falsos Negativos\n","TN = cm[0, 0]  # Verdaderos Negativos\n","\n","DF = 'df_alg, 31var'\n","Modelo = 'arboles_decision_train'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_train, y_pred_train)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score_value = f1_score(y_train, y_pred_train, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score_value}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"KUVUlp_oUDbn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"ofzFEPKMUlCT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hacer predicciones en el conjunto de entrenamiento\n","y_pred_test = clf.predict(X_test)\n","\n","# Calcular la matriz de confusión\n","cm = confusion_matrix(y_test, y_pred_test)\n","\n","# Extraer valores de la matriz de confusión\n","TP = cm[1, 1]  # Verdaderos Positivos\n","FP = cm[0, 1]  # Falsos Positivos\n","FN = cm[1, 0]  # Falsos Negativos\n","TN = cm[0, 0]  # Verdaderos Negativos\n","\n","DF = 'df_alg, 31var'\n","Modelo = 'arboles_decision_test'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_test, y_pred_test)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score_value = f1_score(y_test, y_pred_test, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score_value}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"UOmY9SgLUnDg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"5MPSqRQiUsWF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Bosques Aleatorios"],"metadata":{"id":"3z8XH4XnU0M7"}},{"cell_type":"code","source":["model = RandomForestClassifier(n_estimators = 100, # número de arboles\n","                               criterion = 'gini',  #Se refiere al error cuadrático medio.\n","                               max_depth = 4, # estrategia de parada\n","                               max_leaf_nodes = 10, # estrategia de parada\n","                               max_features = None,\n","                               oob_score = False,\n","                               n_jobs = -1,\n","                               random_state = 123)\n","model.fit(X_train, y_train)"],"metadata":{"id":"keFKNwYtU0M8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hacer predicciones en el conjunto de entrenamiento\n","y_pred_train = model.predict(X_train)\n","\n","# Calcular la matriz de confusión\n","cm = confusion_matrix(y_train, y_pred_train)\n","\n","# Extraer valores de la matriz de confusión\n","TP = cm[1, 1]  # Verdaderos Positivos\n","FP = cm[0, 1]  # Falsos Positivos\n","FN = cm[1, 0]  # Falsos Negativos\n","TN = cm[0, 0]  # Verdaderos Negativos\n","\n","DF = 'df_alg, 31var'\n","Modelo = 'bosques_aleatorios_train'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_train, y_pred_train)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score_value = f1_score(y_train, y_pred_train, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score_value}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"Qc3qvyjgVCMG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"yhGrL7j8VCMG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hacer predicciones en el conjunto de entrenamiento\n","y_pred_test = model.predict(X_test)\n","\n","# Calcular la matriz de confusión\n","cm = confusion_matrix(y_test, y_pred_test)\n","\n","# Extraer valores de la matriz de confusión\n","TP = cm[1, 1]  # Verdaderos Positivos\n","FP = cm[0, 1]  # Falsos Positivos\n","FN = cm[1, 0]  # Falsos Negativos\n","TN = cm[0, 0]  # Verdaderos Negativos\n","\n","DF = 'df_alg, 31var'\n","Modelo = 'bosques_aleatorios_test'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_test, y_pred_test)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score_value = f1_score(y_test, y_pred_test, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score_value}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"gYdBSoyWVCMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"zeSnEg93VCMH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Gradient Boosting"],"metadata":{"id":"nwxvlEJLVb6d"}},{"cell_type":"code","source":["model = GradientBoostingClassifier()\n","\n","cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n","\n","model.fit(X_train, y_train)"],"metadata":{"id":"uSnasJXgVb6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hacer predicciones en el conjunto de entrenamiento\n","y_pred_train = model.predict(X_train)\n","\n","# Calcular la matriz de confusión\n","cm = confusion_matrix(y_train, y_pred_train)\n","\n","# Extraer valores de la matriz de confusión\n","TP = cm[1, 1]  # Verdaderos Positivos\n","FP = cm[0, 1]  # Falsos Positivos\n","FN = cm[1, 0]  # Falsos Negativos\n","TN = cm[0, 0]  # Verdaderos Negativos\n","\n","DF = 'df_alg, 31var'\n","Modelo = 'gardient_boosting_train'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_train, y_pred_train)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score_value = f1_score(y_train, y_pred_train, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score_value}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"Hn5Za86rVkYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"VeYwFWvjVkYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hacer predicciones en el conjunto de entrenamiento\n","y_pred_test = model.predict(X_test)\n","\n","# Calcular la matriz de confusión\n","cm = confusion_matrix(y_test, y_pred_test)\n","\n","# Extraer valores de la matriz de confusión\n","TP = cm[1, 1]  # Verdaderos Positivos\n","FP = cm[0, 1]  # Falsos Positivos\n","FN = cm[1, 0]  # Falsos Negativos\n","TN = cm[0, 0]  # Verdaderos Negativos\n","\n","DF = 'df_alg, 31var'\n","Modelo = 'gardient_boosting_test'\n","\n","# Métricas de clasificación para el conjunto de entrenamiento\n","Accuracy = accuracy_score(y_test, y_pred_test)\n","print(\"Accuracy:\", Accuracy)\n","\n","# Cálculo de las métricas\n","Precision = TP / (TP + FP) if (TP + FP) > 0 else np.NaN\n","Recall = TP / (TP + FN) if (TP + FN) > 0 else np.NaN\n","F1_score_value = f1_score(y_test, y_pred_test, average='binary')\n","Especificidad = TN / (FP + TN) if (FP + TN) > 0 else np.NaN\n","\n","# Imprimir métricas\n","print(f'Precisión: {Precision}')\n","print(f'Recall (Sensibilidad): {Recall}')\n","print(f'F1-score: {F1_score_value}')\n","print(f'Especificidad: {Especificidad}')"],"metadata":{"id":"0EsMn8awVkYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear una nueva fila como DataFrame\n","nueva_fila = pd.DataFrame({\n","    'DF': [DF],\n","    'Modelo': [Modelo],\n","    'Accuracy': [Accuracy],\n","    'Precisión': [Precision],\n","    'Recall': [Recall],\n","    'F1-score': [F1_score],\n","    'Especificidad': [Especificidad]\n","})\n","\n","# Concatenar con el DataFrame original\n","alg_resultados = pd.concat([alg_resultados, nueva_fila], ignore_index=True)\n","\n","# Mostrar el DataFrame con los resultados\n","print(alg_resultados)"],"metadata":{"id":"yasPVzzXVkYZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["De los modelos adicionales probados, se nota la gran diferencia en mejoría respecto a la regresión logística. De entre ellos arboles de decisión presenta menor gap por ende es el que mejor generaliza."],"metadata":{"id":"8Uafp0K9V3Fv"}}]}