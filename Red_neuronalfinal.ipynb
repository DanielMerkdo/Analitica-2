{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "3H9boUOfKk5-",
        "WtLQQsdavniG",
        "dfbD7f7tgFWL"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introducción**"
      ],
      "metadata": {
        "id": "cigHDkP2qzV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo más recomendable es usar el entorno de ejecución T4 GPU para ejecutar el trabajo"
      ],
      "metadata": {
        "id": "XZC2XzxVIYpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "## 1.1 Integrantes\n",
        "\n",
        "*   Daniel Perea Mercado\n",
        "*  David Diaz Rodriguez\n",
        "*  Nicolas Niño Valderrama\n",
        "*  Valentina Jimenez Torres\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqkdGPh9HoDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "## 1.2 Descripción del problema del negocio\n",
        "\n",
        "Hoy en día, los visitantes de parques naturales y jardines botánicos enfrentan dificultades para identificar especies de flores como margaritas, dientes de león, rosas, girasoles y tulipanes. Además, estos espacios, como el Parque Arví y el Jardín Botánico de Medellín, tienen problemas en la gestión de visitantes debido a la cantidad limitada de colaboradores, algunos de los cuales carecen de experiencia en botánica. Esto resulta en una incapacidad para atender la alta demanda de visitantes y convertir sus recorridos en experiencias botánicas satisfactorias."
      ],
      "metadata": {
        "id": "VNaMb8ZAHoXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "## 1.3 Diseño de solución propuesto\n",
        "\n",
        "Para abordar el problema expuesto, se propone desarrollar un sistema automatizado de clasificación de especies de flores basado en una Red Neuronal Artificial (ANN). Este modelo será entrenado para identificar, a partir de imágenes, especies como Margarita, Diente de León, Girasol, Rosa y Tulipán. La solución se integrará a una aplicación móvil que permitirá a los usuarios tomar o cargar imágenes, proporcionando una identificación rápida y precisa de las flores.\n",
        "\n",
        "Se empleará un conjunto de datos de imágenes etiquetadas, adecuadamente preprocesadas para optimizar el rendimiento del modelo. Posteriormente, se implementará la ANN aprovechando su capacidad para reconocer características visuales y patrones complejos. Durante el desarrollo, se ajustarán hiperparámetros clave, y se probarán técnicas de regularización para minimizar el sobreajuste y mejorar la precisión del modelo.\n",
        "\n",
        "Con esta propuesta, se espera mejorar la experiencia del visitante en parques naturales y jardines botánicos, ofreciendo información adicional sobre las flores. Además, el sistema reducirá los errores humanos en la identificación, optimizará costos asociados al proceso manual y enriquecerá significativamente el recorrido de los usuarios."
      ],
      "metadata": {
        "id": "MKpkB6qlJskk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "# **2. Pre procesamiento**"
      ],
      "metadata": {
        "id": "v_xqnJ2Aq7aG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Preparación del notebook"
      ],
      "metadata": {
        "id": "uhhTIlHZrEyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Instalación de librerias"
      ],
      "metadata": {
        "id": "_c_38iMEKeTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "viGkpqWfi73L",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN4AshBHGwqs"
      },
      "outputs": [],
      "source": [
        "# import system libraries\n",
        "import os\n",
        "import itertools\n",
        "from PIL import Image\n",
        "import pathlib\n",
        "import random\n",
        "import kagglehub\n",
        "\n",
        "# import data handling tools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# import Deep Learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Waring Library\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "### 2.1.2 Lectura de datos desde Kaggle"
      ],
      "metadata": {
        "id": "3H9boUOfKk5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este proyecto se hará uso de datos tipo imagen, importados desde el conjunto de datos llamado 'Flowers Dataset' de Kaggle, el cual consiste en imágenes de cinco especie diferentes de flores: Margaritas, Girasoles, Tulipánes, Rosas y Dientes de León."
      ],
      "metadata": {
        "id": "69fdRe7XKqfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar ultima versión\n",
        "ruta = kagglehub.dataset_download(\"rahmasleam/flowers-dataset\")\n",
        "print(\"Ruta al DataSet:\", ruta)"
      ],
      "metadata": {
        "id": "93VlvgyLTGrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En la estructura del dataset, las imágenes están todas dentro de una subcarpeta única (flower_photos),\n",
        "# y las categorías (como sunflowers, roses, etc.) son subcarpetas dentro de esa.\n",
        "ruta_categorias = os.path.join(ruta, 'flower_photos')"
      ],
      "metadata": {
        "id": "u51L-8Y9pRf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se descomprimen de las subcarpetas las imagenes y se crea un dataset con cada ruta individual y su\n",
        "# respectiva categoría\n",
        "\n",
        "Enlaces, Labels = [], []\n",
        "\n",
        "#Enlaces: Una lista donde se almacenarán las rutas de cada imagen.\n",
        "#Labels: Una lista donde se almacenarán las etiquetas asociadas a cada iamgen,\n",
        "# que serán los nombres de las carpetas.\n",
        "\n",
        "for categoria in os.listdir(ruta_categorias): #Itera sobre cada subcarpeta dentro de la ruta ruta_categorias\n",
        "    catpath = os.path.join(ruta_categorias, categoria)\n",
        "    if os.path.isdir(catpath):  # Verificar que es una carpeta\n",
        "        Enlaces += [os.path.join(catpath, img) for img in os.listdir(catpath)]\n",
        "        Labels += [categoria] * len(os.listdir(catpath))\n",
        "\n",
        "# Crear el dataframe\n",
        "df = pd.DataFrame({\"ruta\": Enlaces, \"categoría\": Labels})"
      ],
      "metadata": {
        "id": "OAx4W7NZcRkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "JoncJ3lQoRll",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta es la parte de la ruta que muestra en que subcarpeta esta cada imagen\n",
        "parte_ruta = df['ruta'].apply(lambda x: x.split('/')[-2])\n",
        "parte_ruta"
      ],
      "metadata": {
        "id": "Y_U6Gw8380vX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar si la parte de la ruta donde muestra la subcarpeta de la categoría coincide con la categoría asignada\n",
        "matchs = df.apply(lambda row: row['ruta'].split('/')[-2] == row['categoría'], axis=1)\n",
        "\n",
        "# Filtrar las filas donde no coincidan\n",
        "filas_no_coinciden = df[~matchs]  # '~' invierte la máscara booleana para obtener donde no coinciden\n",
        "filas_no_coinciden"
      ],
      "metadata": {
        "id": "OcVIpkUW65qY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las imágenes\n",
        "num_images = 4 # Número de imágenes por categoría\n",
        "\n",
        "flower_species = df['categoría'].unique() # Categorías únicas\n",
        "\n",
        "# Preparar el plot\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Recorrer cada categoría de flor\n",
        "for idx, flower in enumerate(flower_species):\n",
        "    # Filtra el DataFrame para obtener las rutas de esta especie de flor\n",
        "    flower_df = df[df['categoría'] == flower].sample(num_images)  # Obtener 16 imágenes aleatorias\n",
        "\n",
        "    # Recorre las 16 imágenes para mostrarlas\n",
        "    for i, file in enumerate(flower_df['ruta'].values):\n",
        "        plt.subplot(len(flower_species), num_images, idx * num_images + i + 1)\n",
        "        img = Image.open(file)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(flower)\n",
        "\n",
        "# Mostrar las imagenes\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vbLUA7u8YUfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "## 2.2 Análisis del balanceo entre clases\n"
      ],
      "metadata": {
        "id": "WtLQQsdavniG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "El accuracy mide la proporción de predicciones correctas entre el total de predicciones:\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAmYAAABqCAYAAAAWcZ3YAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAADJaSURBVHhe7d0J3G3V/MfxbR6SmaSiDJEQGSIkEiXNgyZThVIJzSUaJEmDlJImSfNcVCoqNKFMKUQyFBqEMg///3s5v2vf3T7nOec853nuuc/9fV6v87r3Ofvsvde01/qu3/qt377fYost9p8qSZIkSZIkmePcv/NvkiRJkiRJModJYZYkSZIkSTImpDBLkiRJkiQZE1KYJUmSJEmSjAkpzJIkSZIkScaEFGZJkiRJkiRjQgqzJEmSJEmSMSGFWZIkSZIkyZiQwixJkiRJkmRMSGGWJEmSJEkyJqQwS5IkSZIkGRNSmCVJkiRJkowJKcySJEmSJEnGhBRmSdIHT3va06rXve511ROf+MTON8moeeYzn1nKOemfF7zgBdXDH/7wzl9JkswE7rfYYov9p/P/JBlbDEAf/OAHqyc84Qmdb/7Hd77znWrnnXeu/vznP3e++R+f+9znqkUXXbTz1+x8+9vfrj7wgQ90/uoOsfCRj3ykCIcf/vCH1dve9rbOkWQyHHDAAdVLXvKS6iEPeUh1v/vdr/rHP/5R6uszn/lM5xdJnXgGnvzkJ1cPetCDyne/+93vqg9/+MPVt771rfI3CLU111yzWmihharzzz+/+v73v985Mr2MSzqSZG4jLWbJXMHf//736re//W3161//uvrNb35TPeABDygdv8+LX/zi6u1vf3vnl7PjHL+///3vP+v3//73v6vbbrutuuOOOzq/6o7fb7vtttUznvGMzjfJqFD+6vMvf/lL55ukF/EMaM//+te/Ot/el6222qraeuutq3XXXbfaa6+9quc85zmdI9PLuKQjSeY2UpglcwUsVVtuuWW14YYbVvvtt1/1hz/8ofrb3/5W/ec//6ke/OAHV29605uqV7ziFZ1f/48ddtihevOb31zttNNO1e233179+Mc/Ln+vv/761Uc/+tHOr7pD8LFU4Nprr6322GOP8v9k8ih/9fCVr3yl803Si3gGNt544+qXv/xl59v7wkJs4oJHPepR1SKLLFL+P92MSzqSmcEb3/jG6pxzzqne8Y53dL6ZuaQwS+ZaiCxiC5Y4PbAsXG184xvfqP70pz8VQWf5px/WXnvtap111inLbOeee271/ve/v/rZz37WOZqMil7Wn+S+WLJn9e2Gtn7vvfeWScsPfvCD6mtf+1rnyPQyLulIZgaEPYFvIj7TSWGWzLX89a9/rb74xS+WJR4sueSS1bve9a7y/1Gw2mqrlRn/5z//+bIU0+bDliTjxhe+8IVq+eWXr1760peWpcQ51W7HJR3JzOBJT3rSLN/KmU4Ks2Su5tOf/nRZYjQrf+ADH1jM3a95zWs6RyfHJz7xibIUethhh3W+SZIkSaabpzzlKdXznve8zl8znxRmyVyPnXyxPPmYxzym55LmINhFdvXVV3f+SpIkSeYEm2yySbXwwgt3/pr5ZLiMZK7DLkxO+Lfcckv1nve8p3z3zne+szjq8z/gs3TGGWdUH//4x8ux4OSTT67uvPPOWefAtd797ndX8803X1m2fPzjH1/9/ve/L+ExhOGwk4zDNcFnZ+fjHve44qcWx83khNwQ36x5XFgDfmqcoFnz+MPxVSMksfrqq5cda8IJ4NZbb62+9KUvlSWgbrz85S8vndSznvWs6mEPe1jJq116F1xwQXX44YeX35hZyiN/DKEoHvvYx1bXXXddddlllxVne+lyr5NOOqk6++yzyzkBQas8WB3lSZlYMv7JT35Sffazn62uvPLKzi8Hw3UJZtflD8hHKvL7/31QKYte4TLkSb6f//znV/PPP385Xz1dfvnl1Sc/+cm+lsm22GKL6lWvelX5f/iqnHDCCWWn48orr1wtvvji1UMf+tBSfwS5/Nb9EaX9LW95S/mN/GgTltKdLyyEa958882t5STEyqqrrloGlyhT/oqWyS+++OLOr2ZHntWXNuqeluxvvPHGkub3ve99JYxLM1zG3nvvXcqzra02Ub/aybLLLls9+tGPnhWyxDXPPPPMWe20TrP9sVTLy0033TRbvgdJR1xTOBrlGnX75S9/ubSFet2+9a1vrd7whjeUtNbr4Nhjjy19wDLLLFPqQZqUiTbeLUxHtPXXvva1pU1Kq3tdf/311dFHH13C6dTZaKONinuD59VzpaxswlAv2pJNRYMwTPnHOcpMPiPN3/3ud6sjjzxytrwO0l7vueeeofuMYZ7NfvOurj2blsMtY+rvrJCEb7ENYPrUer6VjQ0y+m79elxbXdk80K1/lQ+byzxv8qFs//jHP5a+T1uWrukK5fOA/6+s3Tv/T5K5Ap2ETkdnr6OBh/W5z31uEUoeKI6id999d/WjH/2oHAcRJDRDnAPnePhd0wCiU9LR6lR0RDrhVVZZpXrqU59ajscAGcc9sHaE1o/rBKSD2NAZXHrppWUQE3JD3C6dnmC1OnKD81e/+tXScT772c8uA4vjHKeb8J8LEegcwlPIiRe96EXluvIiXXztLOkuuOCCpTM2aEizMvN7fysfHemvfvWr6uc//3m5vuvY8Wp3q3TpsI8//vhSZjor15S2K664ovy+X1xPWl/96leXgVzaDboGAWVLuCp3HboBpjkgGgx32223Un7q+aCDDio7FJdaaqmSZp329773vTIY9EKnq7NWfjpsHb1y0PHLo87dNdSpTnrFFVcsfxMdUH7LLbdcKddHPvKR5XxpNrD/9Kc/LfWv3pWr9Bg81KsyNRD+85//rI477rjqiCOOKBOAF77whaVs1Eczz+raMro2EQJW3qXfhhQDh/tzrte+/AYE4NOf/vRy/WZbrSPPdsXacXzXXXeVAdegZRCVrhicrrrqqs4ZVbXddttV733ve0v+iReDJ1Epz9L1spe9bFZ76jcdcc1HPOIRZYA9+OCDSxvwPCpXPmrEg/A28Kyqb3UYdWCyRRgof+lVF+7tOZAuAqPZNohabVKdOl+dGHjlZemll65e+cpXlv4l+o/dd9+9tB9523///Ut79Iy4/hJLLFH6mtNOO638th+GKf84x3NOdBIJJqDKgBh5/etfX36n/DBIeyXM5HnQPmOYZ3OQvH/oQx8q5Utcmdz6V7r1t/oN7cTGEvmBc51j4kBQqWP1qk92bUIw+knfBZ5Dk33lceqpp5bJjgmQfkm56C/a+qapIoVZMtfRJsyg0ydQdEIeKJ2MDszv0CbMPNBmUGbyzjVA1Ac71qhTTjmlWFB0MjqMtuM6FufrNIgws0C+aQceeGB5mF1f56nT06FwZNWxxnEDnEHI+X4TA3vA8mbwce2zzjqrWB/cW/6kmZCQX5YCnTVBVc+TaxpMDUDKTkdDKLBwKAMDleClBjSiUAwqu17NrnWQOk0drU7NQPvNb36zk7Le6Ch32WWX0qkTOISlGbe0EGe+Uy7S0ybMdJh2wyoX6fL/X/ziF6UzlledrXzrsNvEbJ0LL7ywWEIM6qxjLFfuqZ5s7lCnLI9myQYNZWZwZ5klsJUnYUUkGLy1McKD0LzooouKNc4AJl0sDepn1113LfVq4DOgn3766eX7r3/962WQM8BoC8ojrHMsP4SN6wvMynFeebu/QUYbMPBJf1OYaRsnnnhiKfcFFljgPsehTMXmUw7Kf8cdd6wuueSSMpCZLLDqmdywRsSzwrqx3nrrlcFRGTn/hhtuKOLaQKv+HDNoq79+0hHXVF7Eq+dQe9PupEdbU/7EaYgrdXzMMceU+tbmlYH0siSybLuHspUu56tD17/mmms6d/2vpczAq47VJdHlvGhX2rk+RvloM557FhhC5ZBDDinXB3FCzBOk2lG/wmyY8o9zPAfagDQTrMpTW5MnbVKZaGvqZpD2quz51A7SZwzzbA6ad+2INVR9aAfuz8qsf/K9dIcoA6u0/kS7kDd1ZSLpX32y9uQe2mq9TWy++eal7Ag2bVF79dH3KQdCT11PlzBLH7NkxuDBMfuKgKVEBjHTDwSOB7Ebrh3XbcOgGgIQOj6DcKDDioFXJ2p2WT9O+Ji5g7AkZAKzuLCS+E0z7peOloiL2V1Qz5OOzu98p7Mz+BlgI4TBZpttVma9QooYjJpIK7Gos2PlMuD2AytDCMbzzjvvPuFGdNi9lkctH+nElY/015EmQkAnbPBUTv3AAhAhOgzI2kwd15Um1j1LXERxHcJKfkDEEZjKUXkqV+VrgFtjjTWKxUL6fO93gfy4h3ohXFgTYXBQvurSfQysTWx4ke9uuHakrw35kS9CQ5rqdUJsKBufsIpIE2uMuteG6+2WdUR5KittjVgIeqXDQK98XNNAXi8bONdkR3v0HFuSqyP/BmlIk2V85wTyJH8GYBbQOq5FNMBA69kOnOc7QssEipWYyDMZUY+e3Tp+r54GYdDyd09uAJ4DeSXWmxApyl4foa+oPwv9tlcM0mcM82wOmvdB0ZYIeO2RyK+LNs8by6Y2EfUfEF8EJPHZRL6VxXSSwiyZURhkzY48mDoFA2NzYJ1qPMQGrG7oJHttKjBjlPbAjDz80Jj+dZR1YnCEjtLyShMDXHSClih1qvvuu2/pVIksFhgdk47bbLuNuIdZt+WQiXBdM824bq8yacOMPt64QPTWO9mA4IXZMCvYqNCJKxtpl4duQtQgYKDyW+WpXJUvWB4MlAb5toHGQOq8uniw1KIOwUpXH7hGQbNOmnVt6cdkxsf/UU+TNzXUhYz0s+6x6sl7U2B1wwRCnfUahE1CPCueBxaftnaNZpqasDDWIRQs6xEHrKFNpMczTByzJPudZS8TJkuvlsrq7YEVzRJsPwxT/iussMKs58AErC2v6sH1YFLHSttGr/bapFefMcyzOUzeB8VEj4hlleMOEGmAtuZZBItgnZh062st0bPeRZs3ebNCYbI9XaQwS2YcnGBj5m5gJMx0sNMFURiz+TYmOt4kfLDAemDpovnhhwGDfHNWD8sbTUEXKBvlBINQvTOrY1AwQBkom1aINgykrA1w/5iV94t7RF5YlSw5NvMdliYDLevLqCCKwnKgbCyltBFWzjbCSmAgssTSTDsHeYM9QjwYwOQFZvyjpl4nBse2CQIBXRfR9TQRQW24TliE+0GbVi4Gym6WaO1VuwGLRgiBJp6nfpF/EwtoLyzFzXqxTMZCpp37DTcDIsJ9iAx1qS1aIrOhiPUlluomYpjy98xLD3pZbmKZWF/R7fns1V6b9Oozhnk2h8n7MGiHxCcXAUuUhx56aKknYs+yaxvSK00mxNJJgLPwq3s+dCbEo54k9SKFWTLj8ACdcsopszp1wsNurrkVnZpBAjots9bmx4yO3ww/pkE7NoMeQTcINgFMBLER6R6G+vkGnbZ8cyKWbyKn2yAyDERktB9lYzl2UEIoEuH8ZZpp/9jHPlbtueeeJf1839C07oyaYepkKtLUtFhMxLB10ISYiLbOGsdfqa1e+HCpF/6a4AvKz4/VJSA49C2shX7bD8OUv7KqW9D7gViaSoZ5NofJ+zDwMeOHyHmf9c1u3/DFI77asGzuGaxvUjBxICRt1JIH/nHTxdSXUpLMAfg4eBB1pDo1szez3bkRVoWwCrBo2aHX7cP/zFLDVBNLAlNJ/R4sdW35jY8ltEEsNhPB2hXCCsPkt36ODr8t3fEZpaicqYyizdUt1Z4pFsC2+ohPTHJM9mwuMNAb9PlbWsJzDf2L5Ua+XeNCXUBOBXPy2axDLNWXla2O2Mhj4wcRZpco/0gbE0yO6jsxm9hIwO/R7kzLlnzk4vcmBcIxTRcpzJIZy3HHHTfL/GzW2Y+VZxzRQRBkYN0aNeFTA0smTPltWAYIa0Msm/TCbyLdZsptS6y9CB8sqD+76KYLuyXDEVga6k7t/RJlpMy6LaE0sYMuRPigVpJ+qNd1v2Wq/UWaRmWJiYFaHmN5q4m0hWVt2DpowhJqWQosXuG72QuDv13OBn1CzeaLTTfdtFhS7KxmWWUNsmtwIoYpf0vaIRDC5aCNqBuiyVL8VDLMszlM3idCqBViGSZTG2ywQWlPlsdZwFjNuqE/4iurvyPGiDh5soRpJ7kQN+LcaTPaP3eGugicSlKYJTMWoozljBPrZKkPEtMN59PY+UVcdjOp84uwhDtoh8cfI3yHiJFu/lTEBfO+zpXFYCKEvogNAzpLYQgGgX9H3Vewm4+RZWq7xiKO0yiwGSIGQWUjLYNiA4EBQpl18/kxOBx11FHVAQccUP7myxQ76Oo760ZFva75t3UT4QLYCnfhuF2K8QxxLO/mr8m3c5999un81RvXJNoJ9m5ijy9Y+FYSuYPGz+sG3yaWM4KZr1sbdqJ6lsSS03bl2waEJupOqI5+Gab8hWyI56jXObEioK4G9ecclGGezWHyPgj8bGMCRMxanuyF69sksNZaa5Wdov6OANSBjRbi68UzOV2kMEvmOsyydegGvIkgzGzl7se0H9adNgR+nQprVT9wxhfCQvp0viuttFLnyP8wWNo9pwMftFOOWSIRoZNtmxX6jkXAzJHgEoRxIurpNri1CUrp7tUB6xQtSbBIWC5qWt38beettA8yQAYsJs1rSpOZtIGbNWSiDr4b4lopK2Vm4GrbHSxAJ+tcBDKtn0M0CKjaxHUMJMOgri3TENeso4IjN4UWQai9yzsLkTqMNHGobvPXtAQkHzHwTgTRE7vy+AARQk20F+2R0BD7Liw0k0V9mrTpPzjut7VLbc3zLlRCII1t6YT+JXwSezFM+RMHXBSISWLVrtImyl9blg670vvdHTsZBn02h8l7oA+J5dPwU9NvuE74hcl79POe3djkEVje7OXO0q2PCqRhqpZkm2SA2WSuwANr+YB5WaBYD51BQoegcxU00ODRBhO62a7ZlJl3BG1sYoAgPnTYOuQYOETwZyLXMegIfAxCHnQzep2i2FO2godVTWcVHagOyDXiuOvXBZDjPu5vJhnHdVD+lRZxz8zaDfCsL3ZqmbHqlAwWrGU6Hcss/rXbjIBzTem1bCNvOkpl2Swr8X8M9nbgubbyJfAsoXCm3WabbUoZCwrJEbbfDkqHrPNVXj46TMFC4bo77bRTuZ/OluCWdvmJfBMsOniBIQ08grKyHlrys53dkoN8WbauR0nvhXJnVXQ/1hrlqTzswnRv15QmeSdYWYLAT1FoCGEwnKNMla37OxZpriP9rG/aqmvzW/Oda2sT2rQyFdwzlquUrbblHGVnKdEH7m/pxr20E+3P7+SnrS0qV+1e+RFN/KLklc9M1LN7EdHK1P0EAVZXHONDMAoAaiBU5vKuLjwfyow1RHsTI0r7M8DJV690mED4NwJ+urb7y6f61t60X2KQ9URaYEeksA3yG8FIwVriWs5VRtEnOK7Nub7fcOBXBiwq6kX9qxeiJ4SAtPMXI3DER/MMCe9BqHk+hIaI9u91RcSza2qD/SzxD1P+LNTK3UeZylc8w9qREBHKV9BUy3La0iDtVRkM2mcM82wOk3ew5MuH+8k7i52xQLo9o+qO2PYbadHm9CWso/GsiV9HXIWLgralPlnytBUfkyF9d/h8yoeAz44JPkuMTgcjf1cmxzsdnzV5znRJMgq0KR1OxJZposMVSbwbBg/LEjqibr/z0BMdBAM8xIFZto7Dgx+YAbIciXzeli4dgpg4rC/ObTLRcbAU2HEZ6GiJxPCXk0YdqEHOe+lseBAUU0Rt+WmjV1npwNxDR0SImoESE0z5Oib3GHTWKB3uZ6DWKZox6xQNmISbtDetgM1819NFkMg3YUUMiLFUD3o6EfxGXEu5CXOgM1Z/6oO40fEbYDl5198LWD+vjWaaA9cmNAxwBryY2Uu/AcvutWaZGnBs9Y/lX/mVbwjySVQYbIOJ2qLr2yFX32RA4GhL8Xt1LG9EC9+pZiBT9egZEhzW/+VBukJs2xFscOz1rDbT0SybqFd1QKR5Pup14O0U2kIb7s2y1K2OHBefKogyJuL8Xn60eW2T07qdmMrV82TiY4JHBBNoUR+eDW1FG2wL/NqLQcsf8hbPv7T6OIflyu8JY2nGIO11VH1Gv8/mMHlXV97A4BmN8mfN1dYC/ShHf8+Ndhl9DfSNLInbb799EYfq22oKzWJiZNLLN4+Aj7YQ7YI7iWX6KNupZqTCzKxE4vmo6PCsFSfJ3IaH26xR52ImzXJkQPHuxOjk/D0nd9IZ/Mw2daRmof3GUeoXz3L43+g0CYjJdkrSKoCjGbZBV7oNlpyoWVkseUx0L+liLdGhsyIMk676gBWDU5QnWA+nom4j/ywL2o92VV+uaUNbJMIMel6PE/llqYmt/5Nti4O2pWHyMRFxTeIMRNBkr9kv2pIyYIly32iXgTog3ogFZW+5S3vFKNI5zLPsnBA1rHWjfv6HYZhnc9C81/vmXmXvd/oZ1jd16fnQViF90qkPivMJU6sP/ELr9xhV3zcoIxVmfA9E26VomSa9J65XROYkSZLppk2YJUmSjAsjdf43syPKYL170F1YSZIkSZIk8zIjE2bMmPUts9bemQuTJEmSJEmS/hiZMLNWzCmSHwSnOVin9X2SJMmcRqwwDsB1h2i+Jb6LOGJJkiRzmpEJM8uYdjXZvRDBCAm1XM5MkmQciBANnIaFf/Dx/wjfkCRJMg6MxPnftmNhBuwS4vxvu6v4JZgojEGSJEmSJEnyX0YizLzcU1A+gTu9mV+sG9GpxaKxRbUZP6cXYqIIICpGDz81MUgEMbT9VqDHtl2e/Zxjx6hAgGKf2JrLmue1IALaQXoFZ7RF1gYGcWLE5onjYvcIUGc7d5wvv+KuCHRn+7SAn+K22HILvxP4T5gFW3Qtn0gbAUuweo9Xr12rzreDTJwrwT+lTUwV23rFYyGAbeP1vjDXd6yOOC7i8diCvN5665Xt7fXfsGyK35IkSZIkyXgwksj/hJkovgLJiYocUc+JGEKJOCOCekGE7LnnnkX8EDnEhCC1oi/bWOAjejtBE69gGOQc6REclIARXZ1Ikq6IAi+GjujOAgiKGBzpjuOEmSCIAtPF+XzpiCZ5tgtVdGTihyC8/fbbS1RzEaRdS/RqAQv54IkPxfdO9GUxaOoRjgNLwPvtt195d5dYWmeeeWYpXwHwHIvI15dcckkJmigoqjg8Pnz7xGIhLpW79IksH5G44zjh5rpJkiRJkowHk7aYNZcxIxCbVyoMspy5++67l+jfLDreGCCCMeoRiV173333LVYgDHOONHL+JZba0kVkegVDt+MEjsjCxJn7evcXIUiEeQ0EMSd+m6CLJ598chFAEGGYJRG+Y6nyip1bbrmlWOW8liJwXGwlAsr1RAaPwHv1yNfNSNb1tLPasaTV8cJWlkV+NUTfRGI5IDilkaidLCyGonN7+W+SJEmSJLMzaed/1hvLeqJl16PjGvRj2Uz03V67M70uh7XKUp3lwGOPPbZzpCoWorvuuqv837/+xjDnQLRfFqhuEEK9jnsXmGjAIEYtKXrXltekeN2PN+kTZfAuOWXgtRAcjQOCSpRjEDusdXVY2YgzFjnXrkdDJuBESCdw6mIO3pMWTsysckRzHWmxxOtFyf2KssArMIaBdTMcrX28F1AZJ0mSJElyXyZtMTvooIPKKxX4lnmxaGAJbrfddiuijdBh0TrkkEM6R2fH77yclv+XF5LW330Fli8igz9WiL9hzkHdmtZmEZvoOIEZ74FrWqza8Hv3b746Iixf8b5E/mJwX+/oI8wIMJY172ms4xiLXQjAOqyXfNrCihhl7rpHHHFE+X/dspkkSZIkyfgwKWEWy5h8lYiYJvygvDcTRIRltjZi2bMpUnoxzDkYpTBrO94NGw+8Y80GBb5ffOJsMmim33sDLYs6zrI0yMYJuI8lU8KN71qIMFa4LbbYoviUdRPI405bG0uSJEmS6YKv+lQzKWFW92maiLvuuqs46teX5YLwxRpEZA1zDqZTmLmGXZWuaUOB5UD+XZZe/b3MMsvcJ/31+w8jzGBnKFHM4nbwwQeXpUsCWtq71UGSJEmSJHOeSfmYGfz5W1mWoyKbn5133rnsOgQLjjf0zysQVjYdbLDBBtX8889fnPHtHt1oo41KefFVmyoILyLQrlhlzrLpw+ctRVmSJEmSjC9DCzMDPYvVrbfeWl144YWdb2eHz5ldh7Dc6e0AbYQzuN8IO9EPw5wznWyyySbFQsUHzsYIy5N80rohtIfQG5zl77nnnvIdcTdM3ux6VD7urZ7sXJ1vvvnK7tFhIMCFDWEhnOzHBgX+iEmSJEmS3JehhVnsxuwVIBWOx+5MYRf4WTWxQ5CFh5DwInSCpAnhIhbY+973vvL3MOf0w0ILLTTrPXqTQbwwotFSpbRO5Gy/1lprlXAWfh87OMWDs7GiDZa3M844o8RXa2K3ps0Gdm4KTEuYEXxE0TDwD7SkOooPa2G+lzBJkiRJ2hlKmBFBQlUY+OthINogSoSVQLflzFNOOaUEQQVBw3etiV2egtgKd4FhzgExR/x0Q/pGIczcB95+wMm/jvKLGG9tiLlmCVgoEFa0NjHrO+E0ogyaXH755SWyP3Eovho/tWZ4jUFwvnRN9iO8SFg7kyRJkiSZnYEi/3utkVf7cGgX/JSAIYqWWGKJIhJi2RKi2gvkymoljpkNAkQKp3lWIMf5Xon9RSgZrJdccsmyY9E1iQlLgI5tuOGG5WOX4f7771+uP8w54Nvl3ixJ4otde+21xZpELLFY8Y0jOOWNRVBa5dVx+XHczkrH+XARWPW8BOHfxerlbQJelKx85H+XXXYpVj73US6WLlkT+esdeuihRWw5X7naJCAN0uh750unNxnYYSlwbRt+S5i6t3PtYq3Hc0uSJEmSZPwYaFemyPqESRNCqLkzkngjZHpZn84+++wS4T7gsyZavXsQLK5rGZSg4ze1zz773GdJcJhzWJtE7yeGxFhzDusUK5XYX5YJCaCAb5hr9cpPMy9wHQ7/hCPhSgiyYHm/pp2TxNXKK69c7k1kCmNx/vnnd86uyjGWQEIQzvdbQk55S2svttpqq5KGq6++eqDl3CSZKZhU6Q96+XcOi8na4osv3hpPMGlHf60Pm2ilJUnmZSYdYHYqIIo4zrOCCS8hSv5EHesw5/h9CDCizI7FeN8lIaVD14FMtlMnBA0QYNGrh7+QBku8V111VVc/NFZHDvjS6j2cvX5bhzDju3b44YeXpd9k3kLbMkGy8WMysLiy5DaDJI8jdoKb0LA48z9tmzQOi7eNmNCxpBMX0DdMFGR6XkZMRa+xi/pA2ySWz7IYjlYVTjrppM6308+4pCOZtxlLYZaMBlY5g0jzXZzJvEE9Jp5lcxZXE46A72MIDJMQxwPfh28kYbbXXnuV14ONO9wETKwWXHDBku9RCjMTrE033bRYwAXOJjRSmPVGgOvll1++TCoFzUZTmNk5rn3ZeKUdmkTOiQ1C45KOJJn0uzKTOY8gv0Jh+MTbFbwBgG/fpZdemqJsHsWyOfFg6Zsw4XPIHzI+/CsD/68fYx0iaFhmCTQ+lKOCJc+O4nip/yjxwv/111+/WDuIslHCoi4MDveIO++8s/Nt0guvgePre/rpp3etj0UWWaRMIGFCEK4b0824pCNJUpjNAFgIDJ4+xBhLwRprrFEG1csuu6zzq2Reg9WIOLviiiuqo446qvNtf2g7fB4tmxN3NtaMCtYT8fn6eWPIsNQtg6OGNWUqrz8T6VVeJo82TrHqcinx95xgXNKRJCnMZgB2ntrEcMcdd5SAv5/85CfLrk87MSeKM5fMbLyWi7/lsNx0001lUA1Lwigweei1KSiZtzAJYOm3GYp/F+vanGBc0pEkKcxmAEcffXR17rnnFguEkCZ8YFg7smOZtxESRjy9ehibQRHmxRKUXc6jYumll05hliRJ0oUUZjMAMz2+NSussEL1spe9rFp77bWLg20yb8NPhrWr7tQ/KMK4EHejeu2Zl/7bZZwkSZK0k7syk2SGwnrKaV+oizbqcQm9x5RoaiJ8zDrrrFPCxrDK1nn5y19enOEjADMRaAentzvYbGDCENjRaJeoN4b4LSyzR9Bjflv8Ies+kXzRLC05J0JU+J172Ohi13G3t0hEHEUMuytT/jiuCyLNGi0AtFA3fI/EBZS+brsy5VEavLmD5ZLFUXlcf/31xcLtjSgTYZegOrGbUd7jLSbbb799iW8otIM0KHchgljIvV2jzm677VYCVbs/cc13Sr2vvvrqJXA10d1WX+6tbvmvakNRt94owlWi/ts64iaqZ7tW3fP2228vQbDVm/bIUlrflem3ys/3ykw9ey/vxz/+8XK8iXSvu+66xTFfnfAHu/fee0sYoSOPPPI+rhvNeqi3oXq+B0lHXNPr8JS/a2obnhFt8sorr+z88r/Pj13xflevA/lXRibRiy66aDnGDUX99YpPGc+csFCCl8uLoOkXXHBBCYtURx1qvzbbqEP38BYe6VR+ApQP81wkU89Akf+TJJl74FtGcHVjlVVWKaEBYFAwEDUxiBho6m+1wHbbbVcCO9sUQLAdfPDB5X4GG4OgEAk333xzddttt5Xfi//FmmtwMJDZUOCtFvzNDEwLL7xwER31YK1eds8KbCnVAGLg8RtBXfkBLbfccuXvtjdaGIzE/oN09SOE6gi7IQ6gdPGzIyYsCXubh3IzOMuHAf60007rnPVfBFE1oBu47d60M9EgaJC2jEtoKle+ob2QfoLBJg4Dus09hANh4h433nhjEWTqQJkoj6c//emlvmIHpPAeviPqxBIjsFyXu4M6N0CzYBrkQ1CsttpqRdDxU7Vb96CDDiox7JZaaqkiBpU9ISTvgfR4w4pz3UdQa+1CPa+44opFJEi/v+WbwIOy8JFH8RzlT3nH8UB577nnniVgN6Fqh+zxxx9fBLr0+yhb7T3SJa377bdf2Y3MB1cdEK/ai2PKQf1ecsklfacjrmmiQZQTYtLBl1ObU1/K1oYbKF9x3LRzdahs1I22QGRKC6EvfyYAruE3bc+t0CMh8ohdbYxfsTSZYBHa0uv60rfHHnuU+5x66qnlPcUnnHBCqQP5FNtwmOcimR5SmCXJPEo/wqwNVhyvZmMlMEiZ4XubBfFmkDNAeFWagea6664rA+WFF15YHXvsscXKwQpD1Lif2b/vbVSpizIDlODIRIdBkig655xziiBwL8cXWGCB8sqxpiUPkxFmXrS/6qqrFmuMQZdIkbZrrrmmDHIGWUKIyGwKMwOsQdC9idLdd9+9DLxC1ggubRAlXg2uyiQEVBteq+b+7m3gVRYGVmVAFJ911lnljSTnnXdeET5EhrQpl9hR6DcnnnhiSa9B2vmsZEQvq4tziCUiRJw6AzoRTUQQP/4faVffhCnx4hzH4bosQCxzLGQEwTHHHFOsgwSE3xFzxCHqwkzejjvuuGL1cm27iOvHg1133bUIPBYi7Y3rht8RhBz1tREf37EI+V49EpfSJH3qzjGhYLRPefBRf/2kI65J6MoT4a7Na48R9Fv9av8hrlizxEPTdtWhciWCTUoOO+yw6sADDyxt0/1Zwxwn7FyPeA9Y1jwrzlWn8uPaLLjahfpXL+pIe998883Ld9LvGZUvH9dlDVT3rIspzMaT9DFLkqRvWIGEYiGwDNaWg+oYnAxCXqBvAGPhGAYWGtYg4si1CIfAYB8v7ydEWERGhUGbtY9wNDATjU0IoW5xzOTXwA6DXn1pjYXFd6xWrISsK/1g8DWogsggeupLuMrc4E8kSjeLiHwEjrOsgJgkAggLgsQgHtYtxOvonNN8Dy9rEwulexBaBBnEjSOA5ItQDMEWyLcl6l4i9O677y7ntyEv8uS+rLD1OmFxuuuuu8r//RvWU8vvhJRrEpz1NBGaLFzaVjPGY690bLbZZiWf2iNR3UT5EM2eDZMek4NAfdWF1kUXXTTb5izpiDol2gjpQDnLj+VI7U4Z11FP2kVYw0B8EZ1tb/0gJifjd5pMPSnMkiTpGx2/gYPlgnBqw+Bh5k4EWF4axtmfMNhyyy2LxYY/D7FXx6AKg9Eog98us8wyZSnL9YlB6RgEgoXlpttuWGVmUJRuwmFUGJxDvEq/fLRBHIUYIZaIja233roISAKX8AAREeK3Tpwb4oGFkJWIaCJ2iPVRE3UCFq96nUg3MRnvBfZ3PU3KuvkqMZYxPnqWBvnr9QORZSmX2OGndcMNN3SOzE4IYNZBS/pttKWpjvZD3AVcAMKyTXzWX+kH14r7EtWeN3UB5/Kx5D/nGFhTWdyIw2Q8SWGWJEnfsPQYnFgVovNvYuDwtgGYucdgPwwsHZZsLOVYfrOMxVoRy5SjhtiQPz5Jg0b3NyDGcp2BleiJDQ3xISAscxGt9cF3FLBmEZTSb7m0DcKszScP/KGIGlgOtdTWTL+3QiDEw7LLLluW30AwDVpm/RB1Iu3aQhP3ZfULwVZPE+uWZfomrHgsh/1CRMcEgOhuWtoClivpVL/Ksw11pH31i2eOkAcrdLNOfCwjwxKsOvQdAUmcapd8Qll6L7744rI0T+Apg2Q8SWGWJEnf8GcZBAMF35dhMMPn8H3++edXO+20UxlwWRuItTZrziiYjFgyIMovWAz5ze27776zfYhLfmf80LyWapQQRTHg81MaFP5LBAWImWbaffh2Sfvee+9dBLjyItKmkkHrZCrSZIIRddsvw9RBG/IT9cI61lYvLGDqxQYMv2Fhtvu3vkGDuLVxgk+e+uNPmIwnKcySJJlSuvns1LFUVB8oWCg4Rq+00kplkOUDxc+GAzSxxhIybtStIKwigvMKf9Dt02s5axjqwqqfMm9SP4fVpy3N8eFb2C1USfJfhqmDNlwnlu5Z69rqIz78z8JyyLrMH9RmDMuWLKrqFSZLlnOT8SSFWZIkfRODsSUSs+82LDOGZc0gwWF7IgguOw0DYR7ERwOfGGKslxBoCrthqS/FhcjpF0tj4YhOTIZf0HQRcbrQtnw3EeopBnX1189ysTqJZWvlFfcfJVHvLFaxRNkLeY80sWL2c85EhG8gLEV385t0r7CsDVMHbRBUBBlY7vqBGBPmRH1awhT+Zc011yz+mtopoWe5u75BIRkfUpglSdI3dhUaJAzC/JDa4GcVPjEGp4jpNAgsZpZeLAna4j8RTWE3LKxYLF+ElVASg8LXyfkGZ75BbQhjcMopp1Q77LBD55vJY+eeEAhQZuKPDQr/oxDR/Km6+QYKFCtkg92SljNDgNgBaJfsqIk2pz1IU/jB1SGUhAUR+JfwsEkArIjCV7Rhs4DlZDuNJ8LOThZQyGc3Hz7CTDqJODs0R4GJSfjWWR7tNgHhR6ZdEdREujJp7li2OYL1WRtJxpcUZkmS9I2OP3aksWgRGU0MHAZ2y40Cs4YVBjYMxBJP3bri92HlQH1ZkIWiTsTsmgr45oRTtPu05c/OS7sS24jzDc7CZrQNooLmsnw0g/b2A1HSJn7EfAsLnZ2RgqkOg0Gb0zhBI51NEeRvgWzVowC3qJ/DD7CJc8SuG9RHK9DmwqeQ+GxbgiNATAhYl2BZz85SbczuyLZ68J222I+/Ylie5FtbbbM0+U7sPtYoYUjETRsFNhrY8EKcaneW95uYyCh7uzNjUwOLdjcRB9fL5ejxJAPMJsk8AIuK5UGdOguBYJ0GyxA9rAAGFbvuHBeAsy36OB8VlgPHiSOzcoFUDYgG4G222abcw+DEgsEBvo6B0BZ+4Q8M5O4RbwpgGRDEFQZZokj6WOaIGIOI71jGWA5YtQz2YnwZsIWnEHCWSJE+57Hssd4J26EMDJi9MJgbfFlZbD5g9bKM5d6RP0tCrkl8SZ80uZ9yMYgKOCq0gvs7RmSGP5k64CsnYn3zFTq9EO2fmOMILq9ErDhr2GKLLcpxaWLx+tSnPjVr16CQIyLPS5/zlYcBW7BTZSLN9fharimffk/oaSOsNepYeVgS006EnAhLpnOUO6HuHL9jsdNW5F9QVvdSXj7Sod1oP+qE5YqVx9/SJ4/OI4DUu+sof2lS7/JvqZUAdcxrh3ykg/M7iC11Y7ciMaM+OcL7Xvq23XbbIrAJWKFGBBSeKB3uxxJll2i8TYEIkgbBYbUN5wpbwrlemuO5E9cs0g1t3/2hfrwFII4ro7r4066V5yKLLFIshnZ7ur+6lidpZC2TT6/bsuvW/aRF+3WvCLEh715z5pjgs0R1Mn7kuzKTZB5AJ8/vRMfcD6w+be+ADFzHQMR6YgA0+2adMKARaWIndXuRvtAXxASBYFCD5aodd9xxNuuaGGYGLSLO73yIMQOf939uvPHGs+J1GaS8osYgGO//bEIEtr0PtA0Drd8aLIlJS1Py51+7RInXelkahO2KiwHQYCn6OquZNIv7Fk7crDk2NtTzOhEnn3xysYqwiBB58u16BnGCTNkQtt5jKS1BnNdGM811CAXvGjWAu0fUrwCsXkNUD44a+D1xQ3gpJ3mWd0udyt41/R34zjHft1F/pybkgyhXv0STPLuHNsfpfZ999rlPma688srFwhahK6IeCVttNN5LSTz2m4562agDaZAey4PETr0Oej130i8NJiZtdRTH6++zdN8NNthg1o5P9aJMCWz3FSYDBBoRR6QRp9H+oi1qK23llYwHKcySJBka1hUWsLC8sS71s9vQQMU6weJlEGsTB4jrG2QMpiwU9evHUk0z2vyoYNmRTpYMrxhyH1YQwi12hvbKs3wanAk8v2OxGyZ+VAisEFMsYq7rO9dTfnVBNioi//LBZypePdQLdcK6Va8v5cWa5lwWRN8Pm95hynSYfEyEa4YfYfhCTpfQkX9WQM+H/Dfbv7dKsKZ5/6k2wppJkE53OpPhSGGWJEky5jSFWTchmyTJ3E86/ydJkiRJkowJKcySJEmSJEnGhFzKTJIkGUPECbMBwo5ZPnyc8G0e4CdkJ6oNBCLwJ0kys0iLWZIkyRhCfEUoEjtRBU31r79973iSJDOPtJglSZIkSZKMCWkxS5IkSZIkGRNSmCVJkiRJkowJKcySJEmSJEnGhBRmSZIkSZIkY0IKsyRJkiRJkjEhhVmSJEmSJMmYkMIsSZIkSZJkTEhhliRJkiRJMiakMEuSJEmSJBkTUpglSZIkSZKMCSnMkiRJkiRJxoQUZkmSJEmSJGNCCrMkSZIkSZIxIYVZkiRJkiTJmJDCLEmSJEmSZExIYZYkSZIkSTImpDBLkiRJkiQZE1KYJUmSJEmSjAkpzJIkSZIkScaEFGZJkiRJkiRjQgqzJEmSJEmSMSGFWZIkSZIkyZiQwixJkiRJkmRMSGGWJEmSJEkyFlTV/wFghBvo01U75AAAAABJRU5ErkJggg==)\n",
        "\n",
        "Es adecuado si:\n",
        "\n",
        "*   Las clases están bien balanceadas (tamaños similares entre todas las clases).\n",
        "*   No tienes una clase mayoritaria que pueda sesgar el resultado.\n",
        "\n",
        "Es inadecuado si:\n",
        "\n",
        "* Hay un gran desequilibrio entre las clases. Por ejemplo, si una clase representa el 90% de los datos, un modelo que predice siempre esa clase puede tener un accuracy alto (90%) sin ser realmente útil."
      ],
      "metadata": {
        "id": "F1AVAb7WwDfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "yQ_q7XHpngxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['categoría'].value_counts()"
      ],
      "metadata": {
        "id": "zN0_3wJnCwiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total de imágenes\n",
        "total = len(df)\n",
        "# Calcular el porcentaje directamente del total\n",
        "category_percentages = (df['categoría'].value_counts() / total) * 100\n",
        "\n",
        "# Mostrar resultados\n",
        "for category, percentage in category_percentages.items():\n",
        "    print(f\"{category}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "F3l6lfewbUBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se encuentra necesario balancear las clases para que la métrica tenga un mejor desempeño."
      ],
      "metadata": {
        "id": "a2bqrvn4DKM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar aleatoriamente 630 datos de cada categoría\n",
        "df2 = df.groupby('categoría').apply(lambda x: x.sample(n=630, random_state=123)).reset_index(drop=True)\n",
        "df2['categoría'].value_counts()"
      ],
      "metadata": {
        "id": "GnYAJNMVy1SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total de imágenes\n",
        "total = len(df2)\n",
        "# Calcular el porcentaje directamente del total\n",
        "category_percentages = (df2['categoría'].value_counts() / total) * 100\n",
        "\n",
        "# Mostrar resultados\n",
        "for category, percentage in category_percentages.items():\n",
        "    print(f\"{category}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "xzai-ITzb7Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Separación en entrenamiento, validación y test"
      ],
      "metadata": {
        "id": "dfbD7f7tgFWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separación del dataset\n",
        "\n",
        "# Crear la variable 'strat' basada en la columna 'categoría'\n",
        "strat = df2['categoría']\n",
        "\n",
        "# Configurar proporciones para entrenamiento (70%), validación (20%) y test (10%)\n",
        "train_size = 0.7\n",
        "valid_size = 0.2\n",
        "test_size = 0.1\n",
        "\n",
        "# Dividir el conjunto inicial en entrenamiento (70%) y \"validación+test\" (30%)\n",
        "train_df, temp_df = train_test_split(\n",
        "    df2,\n",
        "    test_size=(1 - train_size),\n",
        "    random_state=123,\n",
        "    stratify=strat,  # Usar la variable 'strat'\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Actualizar 'strat' para el conjunto temporal temp_df\n",
        "strat_temp = temp_df['categoría']\n",
        "\n",
        "# Dividir el conjunto \"validación+test\" en validación (20%) y test (10%)\n",
        "valid_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=(test_size / (valid_size + test_size)),  # Ajustar para mantener proporción\n",
        "    random_state=123,\n",
        "    stratify=strat_temp  # Usar 'strat_temp' para la segunda división\n",
        ")\n",
        "\n",
        "# Mostrar tamaños de los conjuntos\n",
        "print(f\"Tamaño del Dataset Original: {df2.shape[0]}\")\n",
        "print(f\"Tamaño del Dataset de Entrenamiento: {len(train_df)}\")\n",
        "print(f\"Tamaño del Dataset de Validación: {len(valid_df)}\")\n",
        "print(f\"Tamaño del Dataset de Test: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "zZdjqHLlYQrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['categoría'].value_counts()"
      ],
      "metadata": {
        "id": "7Y0QQXu19R4j",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las imágenes de entrenamiento\n",
        "num_images = 4 # Número de imágenes por categoría\n",
        "\n",
        "flower_species = train_df['categoría'].unique() # Categorías únicas\n",
        "\n",
        "# Preparar el plot\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Recorrer cada categoría de flor\n",
        "for idx, flower in enumerate(flower_species):\n",
        "    # Filtra el DataFrame para obtener las rutas de esta especie de flor\n",
        "    flower_train_df = train_df[train_df['categoría'] == flower].sample(num_images)  # Obtener 16 imágenes aleatorias\n",
        "\n",
        "    # Recorre las 16 imágenes para mostrarlas\n",
        "    for i, file in enumerate(flower_train_df['ruta'].values):\n",
        "        plt.subplot(len(flower_species), num_images, idx * num_images + i + 1)\n",
        "        img = Image.open(file)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(flower)\n",
        "\n",
        "# Mostrar las imagenes\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PbD_sYh704tQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df['categoría'].value_counts()"
      ],
      "metadata": {
        "id": "0W62P1u9r3Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las imágenes de validación\n",
        "num_images = 4 # Número de imágenes por categoría\n",
        "\n",
        "flower_species = valid_df['categoría'].unique() # Categorías únicas\n",
        "\n",
        "# Preparar el plot\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Recorrer cada categoría de flor\n",
        "for idx, flower in enumerate(flower_species):\n",
        "    # Filtra el DataFrame para obtener las rutas de esta especie de flor\n",
        "    flower_valid_df = valid_df[valid_df['categoría'] == flower].sample(num_images)  # Obtener 16 imágenes aleatorias\n",
        "\n",
        "    # Recorre las 16 imágenes para mostrarlas\n",
        "    for i, file in enumerate(flower_valid_df['ruta'].values):\n",
        "        plt.subplot(len(flower_species), num_images, idx * num_images + i + 1)\n",
        "        img = Image.open(file)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(flower)\n",
        "\n",
        "# Mostrar las imagenes\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-FNyXz3b0_Ik",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['categoría'].value_counts()"
      ],
      "metadata": {
        "id": "GFdY9qd3fxC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las imágenes de testeo\n",
        "num_images = 4 # Número de imágenes por categoría\n",
        "\n",
        "flower_species = test_df['categoría'].unique() # Categorías únicas\n",
        "\n",
        "# Preparar el plot\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Recorrer cada categoría de flor\n",
        "for idx, flower in enumerate(flower_species):\n",
        "    # Filtra el DataFrame para obtener las rutas de esta especie de flor\n",
        "    flower_test_df = test_df[test_df['categoría'] == flower].sample(num_images)  # Obtener 16 imágenes aleatorias\n",
        "\n",
        "    # Recorre las 16 imágenes para mostrarlas\n",
        "    for i, file in enumerate(flower_test_df['ruta'].values):\n",
        "        plt.subplot(len(flower_species), num_images, idx * num_images + i + 1)\n",
        "        img = Image.open(file)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(flower)\n",
        "\n",
        "# Mostrar las imagenes\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jjqyLgsFgYNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "## 2.4 Estandarización y generación de los datos"
      ],
      "metadata": {
        "id": "Fs8xGvIrlVnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros\n",
        "batch_size = 32 # Tamaño del lote que será cargado por el generador en cada iteración.\n",
        "img_size = (128, 128) # Dimensiones a las que se redimensionan las imágenes (128x128 píxeles).\n",
        "channels = 3 # RGB\n",
        "# Forma final de las imágenes después del preprocesamiento (128x128x3).\n",
        "\n",
        "# Generador con Data Augmentation en el conjunto de entrenamiento para aumentar la diversidad\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalización de píxeles a [0, 1]\n",
        "    rotation_range=30,  # Rotación aleatoria en grados\n",
        "    width_shift_range=0.2,  # Desplazamiento horizontal\n",
        "    height_shift_range=0.2,  # Desplazamiento vertical\n",
        "    shear_range=0.2,  # Transformación de corte\n",
        "    zoom_range=0.2,  # Zoom aleatorio\n",
        "    horizontal_flip=True,  # Volteo horizontal\n",
        "    fill_mode='nearest')  # Relleno de píxeles fuera del límite\n",
        "\n",
        "\n",
        "# Generador para validación y prueba (sin aumentación, solo normalización)\n",
        "valid_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Carga y estandarización de datos\n",
        "train_gen = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    x_col='ruta',\n",
        "    y_col='categoría',\n",
        "    target_size=img_size,\n",
        "    class_mode='categorical', # One-Hot Encoding para las categoría de las flores\n",
        "    color_mode='rgb',\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "valid_gen = valid_test_datagen.flow_from_dataframe(\n",
        "    valid_df,\n",
        "    x_col='ruta',\n",
        "    y_col='categoría',\n",
        "    target_size=img_size,\n",
        "    class_mode='categorical',  # Esto convierte automáticamente las etiquetas de texto en vectores One-Hot.\n",
        "    color_mode='rgb',\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "test_gen = valid_test_datagen.flow_from_dataframe(\n",
        "    test_df,\n",
        "    x_col='ruta',\n",
        "    y_col='categoría',\n",
        "    target_size=img_size,\n",
        "    class_mode='categorical',  # Esto convierte automáticamente las etiquetas de texto en vectores One-Hot.\n",
        "    color_mode='rgb',\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size)"
      ],
      "metadata": {
        "id": "u1LfZuNalbWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las imáganes de data aumentada\n",
        "\n",
        "# Número de imágenes por categoría\n",
        "num_images = 4\n",
        "\n",
        "# Obtener las clases (categorías) desde el generador\n",
        "class_indices = train_gen.class_indices\n",
        "classes = list(class_indices.keys())  # Lista de categorías únicas\n",
        "\n",
        "# Configurar el tamaño del plot\n",
        "plt.figure(figsize=(15, len(classes) * 3))\n",
        "\n",
        "# Visualizar imágenes por cada categoría\n",
        "for idx, flower in enumerate(classes):\n",
        "    count = 0  # Contador para limitar las imágenes a num_images\n",
        "    while count < num_images:\n",
        "        # Obtener un batch de imágenes y etiquetas\n",
        "        images, labels = next(train_gen)\n",
        "\n",
        "        # Filtrar las imágenes que pertenecen a la categoría actual\n",
        "        for i in range(len(labels)):\n",
        "            if np.argmax(labels[i]) == idx:  # Coincide con la categoría actual\n",
        "                plt.subplot(len(classes), num_images, idx * num_images + count + 1)\n",
        "                plt.imshow(images[i])\n",
        "                plt.axis('off')\n",
        "                plt.title(flower)\n",
        "                count += 1\n",
        "\n",
        "                # Si ya tenemos suficientes imágenes para esta categoría, salir del loop\n",
        "                if count == num_images:\n",
        "                    break\n",
        "\n",
        "# Mostrar las imágenes\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lukRpNioRnqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3PcmuDVlSi1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "# **3. Desarrollo red neuronal convolucional CNN**"
      ],
      "metadata": {
        "id": "DcbKEwQEPxxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Arquitectura base"
      ],
      "metadata": {
        "id": "QsojpdhbmbtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de la arquitectura CNN base\n",
        "model = models.Sequential()\n",
        "\n",
        "# Primera capa convolucional\n",
        "model.add(\n",
        "    layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=(3, 3),\n",
        "        strides=(1, 1),  # Strides más pequeños para preservar información\n",
        "        activation='relu',\n",
        "        input_shape=(128, 128, 3)))\n",
        "\n",
        "model.add(\n",
        "    layers.MaxPooling2D(\n",
        "        pool_size=(2, 2)))  # Primera reducción de dimensionalidad\n",
        "\n",
        "# Segunda capa convolucional\n",
        "model.add(\n",
        "    layers.Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        strides=(1, 1),  # Strides pequeños\n",
        "        activation='relu'))\n",
        "\n",
        "model.add(\n",
        "    layers.MaxPooling2D(\n",
        "        pool_size=(2, 2)))  # Segunda reducción\n",
        "\n",
        "# Tercera capa convolucional\n",
        "model.add(\n",
        "    layers.Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        strides=(1, 1),\n",
        "        activation='relu'))\n",
        "\n",
        "model.add(\n",
        "    layers.MaxPooling2D(\n",
        "        pool_size=(2, 2)))  # Tercera reducción\n",
        "\n",
        "# Aplanar para conectar con las capas densas\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Primera capa densa\n",
        "model.add(\n",
        "    layers.Dense(\n",
        "        units=128,\n",
        "        activation='relu'))\n",
        "\n",
        "# Segunda capa densa\n",
        "model.add(\n",
        "    layers.Dense(\n",
        "        units=32,\n",
        "        activation='relu'))\n",
        "\n",
        "# Capa de salida\n",
        "model.add(\n",
        "    layers.Dense(\n",
        "        units=5,\n",
        "        activation='softmax'))"
      ],
      "metadata": {
        "id": "geukvziKmjBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilador\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "bMga4MTlm0G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Representación esquemática de la arquitectura, para entender mejor la arquitectura de la red neorunal.\n",
        "keras.utils.plot_model(\n",
        "    model, #parametro a graficar\n",
        "    to_file = 'model.png', # nombre de la imagen\n",
        "    show_shapes = True, # mostrar figuras\n",
        "    show_layer_names = True) # mostrar nombres de las capas"
      ],
      "metadata": {
        "id": "eM3L2evRTND5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos la red neuronal convolucional\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=50,\n",
        "    validation_data=valid_gen,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "hUPpSSLlUnNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las curvas de error\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Perdida del modelo')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i2R9GXqwOQ4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las curvas de accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Accuracy del modelo')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yv410wO9Oeqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Optimización de hiperparámetros"
      ],
      "metadata": {
        "id": "pvMpq5JrwJy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de la arquitectura CNN base como un HyperModel\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Definición de la primera capa convolucional con optimización de hiperparámetros\n",
        "    model.add(\n",
        "        keras.layers.Conv2D(\n",
        "            filters = hp.Int('filters_1', min_value=16, max_value=256, step=16),\n",
        "            kernel_size = hp.Choice('kernel_size_1', values = [3, 5]),\n",
        "            strides=(1, 1),\n",
        "            activation = 'relu',\n",
        "            input_shape=(128, 128, 3)))\n",
        "\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "    # Definición de la segunda capa convolucional con optimización de hiperparámetros\n",
        "    model.add(\n",
        "        keras.layers.Conv2D(\n",
        "            filters = hp.Int('filters_2', min_value=16, max_value=256, step=16),\n",
        "            kernel_size = hp.Choice('kernel_size_2', values = [3, 5]),\n",
        "            strides=(1, 1),\n",
        "            activation = 'relu'))\n",
        "\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "    # Definición de la tercera capa convolucional con optimización de hiperparámetros\n",
        "    model.add(\n",
        "        keras.layers.Conv2D(\n",
        "            filters = hp.Int('filters_3', min_value=16, max_value=256, step=16),\n",
        "            kernel_size = hp.Choice('kernel_size_3', values = [3, 5]),\n",
        "            strides=(1, 1),\n",
        "            activation = 'relu'))\n",
        "\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "    # Aplanar el tensor 2D a 1D\n",
        "    model.add(keras.layers.Flatten())\n",
        "\n",
        "    # Adición de la primera capa oculta con optimización de hiperparámetros\n",
        "    model.add(\n",
        "        keras.layers.Dense(\n",
        "            units = hp.Int('units_1', min_value=32, max_value=256, step=16),\n",
        "            activation = hp.Choice('activation_d1', values=['relu', 'tanh'])))\n",
        "\n",
        "    # Adición de la segunda capa oculta con optimización de hiperparámetros\n",
        "    model.add(\n",
        "        keras.layers.Dense(\n",
        "            units = hp.Int('units_2', min_value=32, max_value=256, step=16),\n",
        "            activation = hp.Choice('activation_d2', values=['relu', 'tanh'])))\n",
        "\n",
        "    # Definición de la capa de salida\n",
        "    model.add(\n",
        "        keras.layers.Dense(\n",
        "            units=5,\n",
        "            activation='softmax'))\n",
        "\n",
        "    # Definición del compilador con optimización del hiperparámetro learning rate\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[0.01, 0.001, 0.0001, 0.00001])\n",
        "    model.compile(\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "        loss = 'categorical_crossentropy',  # Las etiquetas están one-hot encoded\n",
        "        metrics = ['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "vOx2eXT51Oea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del tuner para la búsqueda de hiperparámetros utilizando RandomSearch\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=6,\n",
        "    executions_per_trial=2,\n",
        "    directory='results_tuner',\n",
        "    project_name='cnn_tuning')\n",
        "\n",
        "# Realización de la búsqueda de los mejores hiperparámetros\n",
        "tuner.search(train_gen, epochs=7, validation_data=valid_gen)"
      ],
      "metadata": {
        "id": "JQu5ksXwb-Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el mejor modelo obtenido en la búsqueda\n",
        "for h_param in [f'filters_{i}' for i in [1,2,3]] + [f'kernel_size_{i}' for i in [1,2,3]] + [f\"units_{i}\" for i in [1,2]] + [f\"activation_d{i}\" for i in [1,2]] + ['learning_rate']:\n",
        "  print(f\"{h_param}: {tuner.get_best_hyperparameters()[0].get(h_param)}\")"
      ],
      "metadata": {
        "id": "HdAjygyi6qpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar los mejores hiperparámetros\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "kiZyC7E3cCg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtención del modelo con mejores hiperparámetros\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "pK5iv6BsTx46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del modelo con los mejores hiperparámetros\n",
        "history2 = model.fit(train_gen, epochs=50, validation_data=valid_gen)"
      ],
      "metadata": {
        "id": "InswIXtnyOM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las curvas de error\n",
        "plt.plot(history2.history['loss'])\n",
        "plt.plot(history2.history['val_loss'])\n",
        "plt.title('Perdida del modelo')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pAr4Le-6yQHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las curvas de accuracy\n",
        "plt.plot(history2.history['accuracy'])\n",
        "plt.plot(history2.history['val_accuracy'])\n",
        "plt.title('Accuracy del modelo')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-sbec_JbzjFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Regularización"
      ],
      "metadata": {
        "id": "eehQ28cWwQJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido al sobreajuste del modelo se hace necesario realizar la regularización"
      ],
      "metadata": {
        "id": "TuUMgbdEHroR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.1 CNN con Dropout"
      ],
      "metadata": {
        "id": "CwsMs9fsqOw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimización de hiperparámetro para Dropout utilizando los hiperparametros anteriormente encontrados\n",
        "\n",
        "def build_model_dropout(hp):\n",
        "\n",
        "    model_dropout = keras.Sequential()\n",
        "\n",
        "    # Primera capa convolucional\n",
        "    model_dropout.add(\n",
        "        layers.Conv2D(\n",
        "            filters=best_hps.get('filters_1'),\n",
        "            kernel_size=(best_hps.get('kernel_size_1'),best_hps.get('kernel_size_1')),\n",
        "            strides=(1, 1),\n",
        "            activation='relu',\n",
        "            input_shape=(128, 128, 3)))\n",
        "\n",
        "    model_dropout.add(\n",
        "        layers.MaxPooling2D(\n",
        "            pool_size=(2, 2)))  # Primera reducción de dimensionalidad\n",
        "\n",
        "    # Primera capa de dropout\n",
        "    model_dropout.add(keras.layers.Dropout(hp.Float('dropout_rate_1',\n",
        "                                                    min_value= 0.1,\n",
        "                                                    max_value= 0.5,\n",
        "                                                    step=0.1)))\n",
        "\n",
        "    # Segunda capa convolucional\n",
        "    model_dropout.add(\n",
        "        layers.Conv2D(\n",
        "            filters=best_hps.get('filters_2'),\n",
        "            kernel_size=(best_hps.get('kernel_size_2'),best_hps.get('kernel_size_2')),\n",
        "            strides=(1, 1),\n",
        "            activation='relu'))\n",
        "\n",
        "    model_dropout.add(\n",
        "        layers.MaxPooling2D(\n",
        "            pool_size=(2, 2)))  # Segunda reducción\n",
        "\n",
        "    # Segunda capa de dropout\n",
        "    model_dropout.add(keras.layers.Dropout(hp.Float('dropout_rate_2',\n",
        "                                                    min_value= 0.1,\n",
        "                                                    max_value= 0.5,\n",
        "                                                    step=0.1)))\n",
        "\n",
        "    # Tercera capa convolucional\n",
        "    model_dropout.add(\n",
        "        layers.Conv2D(\n",
        "            filters=best_hps.get('filters_3'),\n",
        "            kernel_size=(best_hps.get('kernel_size_3'),best_hps.get('kernel_size_3')),\n",
        "            strides=(1, 1),\n",
        "            activation='relu'))\n",
        "\n",
        "    model_dropout.add(\n",
        "        layers.MaxPooling2D(\n",
        "            pool_size=(2, 2)))  # Tercera reducción\n",
        "\n",
        "    # Tercera capa de dropout\n",
        "    model_dropout.add(keras.layers.Dropout(hp.Float('dropout_rate_3',\n",
        "                                                    min_value= 0.1,\n",
        "                                                    max_value= 0.5,\n",
        "                                                    step=0.1)))\n",
        "\n",
        "    # Capa de Flatten para convertir a 1D\n",
        "    model_dropout.add(layers.Flatten())\n",
        "\n",
        "    # Primera capa densa con Dropout\n",
        "    model_dropout.add(\n",
        "        layers.Dense(\n",
        "            units=best_hps.get('units_1'),\n",
        "            activation=best_hps.get('activation_d1')))\n",
        "\n",
        "    # Segunda capa densa\n",
        "    model_dropout.add(\n",
        "        layers.Dense(\n",
        "            units=best_hps.get('units_2'),\n",
        "            activation=best_hps.get('activation_d2')))\n",
        "\n",
        "    # Cuarta capa de dropout\n",
        "    model_dropout.add(keras.layers.Dropout(hp.Float('dropout_rate_4',\n",
        "                                                    min_value= 0.1,\n",
        "                                                    max_value= 0.5,\n",
        "                                                    step=0.1)))\n",
        "\n",
        "    # Capa de salida\n",
        "    model_dropout.add(\n",
        "        layers.Dense(\n",
        "            units=5,\n",
        "            activation='softmax'))\n",
        "\n",
        "    model_dropout.compile(\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "    return model_dropout"
      ],
      "metadata": {
        "id": "taNmcBFkMj1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del tuner para la búsqueda de hiperparámetros utilizando RandomSearch\n",
        "tuner = RandomSearch(\n",
        "    build_model_dropout,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=6,\n",
        "    executions_per_trial=2,\n",
        "    directory='results_tuner',\n",
        "    project_name='cnn_tuning_dropout')\n",
        "\n",
        "# Realización de la búsqueda de los mejores hiperparámetros\n",
        "tuner.search(train_gen, epochs=7, validation_data=valid_gen)"
      ],
      "metadata": {
        "id": "IuLDsetU-1Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el mejor modelo obtenido en la búsqueda\n",
        "for h_param in [f'dropout_rate_{i}' for i in range(1,5)]:\n",
        "  print(f\"{h_param}: {tuner.get_best_hyperparameters()[0].get(h_param)}\")"
      ],
      "metadata": {
        "id": "nX3EZ7QDIpIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar los mejores hiperparámetros\n",
        "best_hps_dropout = tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "jJHWdYaY-7Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtención del modelo con mejores hiperparámetros\n",
        "model_dropout = tuner.hypermodel.build(best_hps_dropout)\n",
        "model_dropout.summary()"
      ],
      "metadata": {
        "id": "wkIa_9dG_Kuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del modelo con los mejores hiperparámetros con dropout\n",
        "history3 = model_dropout.fit(train_gen, epochs=50, validation_data=valid_gen)"
      ],
      "metadata": {
        "id": "BFAmgPDZQNyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las curvas de error\n",
        "plt.plot(history3.history['loss'])\n",
        "plt.plot(history3.history['val_loss'])\n",
        "plt.title('Perdida del modelo')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pYdxjPFoZM1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las curvas de accuracy\n",
        "plt.plot(history3.history['accuracy'])\n",
        "plt.plot(history3.history['val_accuracy'])\n",
        "plt.title('Accuracy del modelo')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-NMUnuWjzQf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2 CNN con L2"
      ],
      "metadata": {
        "id": "Zg_v-tvpqirB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2 sólo enel modelo antes de Dropout"
      ],
      "metadata": {
        "id": "yxLCJOVaK9VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_l2_before = models.Sequential()\n",
        "\n",
        "# Primera capa convolucional con L2\n",
        "model_l2_before.add(\n",
        "    layers.Conv2D(\n",
        "        filters=best_hps.get('filters_1'),\n",
        "        kernel_size=(best_hps.get('kernel_size_1'), best_hps.get('kernel_size_1')),\n",
        "        strides=(1, 1),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.L2(0.01),  # Regularización L2\n",
        "        input_shape=(128, 128, 3)))\n",
        "\n",
        "model_l2_before.add(\n",
        "    layers.MaxPooling2D(\n",
        "        pool_size=(2, 2)))  # Primera reducción de dimensionalidad\n",
        "\n",
        "# Segunda capa convolucional con L2\n",
        "model_l2_before.add(\n",
        "    layers.Conv2D(\n",
        "        filters=best_hps.get('filters_2'),\n",
        "        kernel_size=(best_hps.get('kernel_size_2'), best_hps.get('kernel_size_2')),\n",
        "        strides=(1, 1),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.L2(0.01)))\n",
        "\n",
        "model_l2_before.add(\n",
        "    layers.MaxPooling2D(\n",
        "        pool_size=(2, 2)))  # Segunda reducción\n",
        "\n",
        "# Tercera capa convolucional con L2\n",
        "model_l2_before.add(\n",
        "    layers.Conv2D(\n",
        "        filters=best_hps.get('filters_3'),\n",
        "        kernel_size=(best_hps.get('kernel_size_3'), best_hps.get('kernel_size_3')),\n",
        "        strides=(1, 1),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.L2(0.01)))\n",
        "\n",
        "model_l2_before.add(\n",
        "    layers.MaxPooling2D(\n",
        "        pool_size=(2, 2)))  # Tercera reducción\n",
        "\n",
        "# Capa de Flatten para convertir a 1D\n",
        "model_l2_before.add(layers.Flatten())\n",
        "\n",
        "# Primera capa densa con L2\n",
        "model_l2_before.add(\n",
        "    layers.Dense(\n",
        "        units=best_hps.get('units_1'),\n",
        "        activation=best_hps.get('activation_d1'),\n",
        "        kernel_regularizer=regularizers.L2(0.01)))\n",
        "\n",
        "# Segunda capa densa con L2\n",
        "model_l2_before.add(\n",
        "    layers.Dense(\n",
        "        units=best_hps.get('units_2'),\n",
        "        activation=best_hps.get('activation_d2'),\n",
        "        kernel_regularizer=regularizers.L2(0.01)))\n",
        "\n",
        "# Capa de salida\n",
        "model_l2_before.add(\n",
        "    layers.Dense(\n",
        "        units=5,\n",
        "        activation='softmax'))"
      ],
      "metadata": {
        "id": "_mcjGTz9LHos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_l2_before.compile(\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model_l2_before.summary()"
      ],
      "metadata": {
        "id": "Nu-tgpADNBO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos la red neuronal convolucional\n",
        "history4 = model_l2_before.fit(\n",
        "    train_gen,\n",
        "    epochs=50,\n",
        "    validation_data=valid_gen,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "rKhT3xrUZ0lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las curvas de error\n",
        "plt.plot(history4.history['loss'])\n",
        "plt.plot(history4.history['val_loss'])\n",
        "plt.title('Perdida del modelo')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Km1eBFLqdcw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las curvas de accuracy\n",
        "plt.plot(history4.history['accuracy'])\n",
        "plt.plot(history4.history['val_accuracy'])\n",
        "plt.title('Accuracy del modelo')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sr-iDZZtLuWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2 CNN con Dropout y L2"
      ],
      "metadata": {
        "id": "mkxudlcxN469"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_l2_dp = models.Sequential()\n",
        "\n",
        "# Primera capa convolucional con L2\n",
        "model_l2_dp.add(\n",
        "    layers.Conv2D(\n",
        "        filters=best_hps.get('filters_1'),\n",
        "        kernel_size=(best_hps.get('kernel_size_1'), best_hps.get('kernel_size_1')),\n",
        "        strides=(1, 1),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.L2(0.001),  # Regularización L2\n",
        "        input_shape=(128, 128, 3)))\n",
        "\n",
        "model_l2_dp.add(\n",
        "    layers.MaxPooling2D(\n",
        "        pool_size=(2, 2)))  # Primera reducción de dimensionalidad\n",
        "\n",
        "# Primera capa de dropout\n",
        "model_l2_dp.add(keras.layers.Dropout(best_hps_dropout.get('dropout_rate_1')))\n",
        "\n",
        "# Segunda capa convolucional con L2\n",
        "model_l2_dp.add(\n",
        "    layers.Conv2D(\n",
        "        filters=best_hps.get('filters_2'),\n",
        "        kernel_size=(best_hps.get('kernel_size_2'), best_hps.get('kernel_size_2')),\n",
        "        strides=(1, 1),\n",
        "        activation='relu'))\n",
        "\n",
        "model_l2_dp.add(\n",
        "    layers.MaxPooling2D(\n",
        "        pool_size=(2, 2)))  # Segunda reducción\n",
        "\n",
        "# Segunda capa de dropout\n",
        "model_l2_dp.add(keras.layers.Dropout(best_hps_dropout.get('dropout_rate_2')))\n",
        "\n",
        "# Tercera capa convolucional con L2\n",
        "model_l2_dp.add(\n",
        "    layers.Conv2D(\n",
        "        filters=best_hps.get('filters_3'),\n",
        "        kernel_size=(best_hps.get('kernel_size_3'), best_hps.get('kernel_size_3')),\n",
        "        strides=(1, 1),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.L2(0.01)))\n",
        "\n",
        "model_l2_dp.add(\n",
        "    layers.MaxPooling2D(\n",
        "        pool_size=(2, 2)))  # Tercera reducción\n",
        "\n",
        "# Tercera capa de dropout\n",
        "model_l2_dp.add(keras.layers.Dropout(best_hps_dropout.get('dropout_rate_3')))\n",
        "\n",
        "# Capa de Flatten para convertir a 1D\n",
        "model_l2_dp.add(layers.Flatten())\n",
        "\n",
        "# Primera capa densa con L2\n",
        "model_l2_dp.add(\n",
        "    layers.Dense(\n",
        "        units=best_hps.get('units_1'),\n",
        "        activation=best_hps.get('activation_d1')))\n",
        "\n",
        "# Segunda capa densa con L2\n",
        "model_l2_dp.add(\n",
        "    layers.Dense(\n",
        "        units=best_hps.get('units_2'),\n",
        "        activation=best_hps.get('activation_d2'),\n",
        "        kernel_regularizer=regularizers.L2(0.01)))\n",
        "\n",
        "# Cuarta capa de dropout\n",
        "model_l2_dp.add(keras.layers.Dropout(best_hps_dropout.get('dropout_rate_4')))\n",
        "\n",
        "# Capa de salida\n",
        "model_l2_dp.add(\n",
        "    layers.Dense(\n",
        "        units=5,\n",
        "        activation='softmax'))"
      ],
      "metadata": {
        "id": "Qe0sPzV-N4H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_l2_dp.compile(\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=best_hps.get('learning_rate')),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model_l2_dp.summary()"
      ],
      "metadata": {
        "id": "Ueex65iSPzxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos la red neuronal convolucional\n",
        "history5 = model_l2_dp.fit(\n",
        "    train_gen,\n",
        "    epochs=50,\n",
        "    validation_data=valid_gen,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "FNyIFj0ZPzxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las curvas de error\n",
        "plt.plot(history5.history['loss'])\n",
        "plt.plot(history5.history['val_loss'])\n",
        "plt.title('Perdida del modelo')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-4WrG0g3Pzxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de las curvas de accuracy\n",
        "plt.plot(history5.history['accuracy'])\n",
        "plt.plot(history5.history['val_accuracy'])\n",
        "plt.title('Accuracy del modelo')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hMKy4dVBPzxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Elección del modelo**\n",
        "\n"
      ],
      "metadata": {
        "id": "TFK6H4MnRcZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se evaluaron cinco configuraciones diferentes de una red neuronal convolucional (CNN) para determinar la estrategia de optimización y regularización más efectiva. Las configuraciones incluyeron una CNN base, una CNN con optimización de hiperparámetros, una CNN con regularización por dropout, una CNN con regularización por L2, y una CNN con una combinación de dropout y L2.\n",
        "\n",
        "La CNN base mostró una alta precisión en el conjunto de entrenamiento, pero un rendimiento significativamente peor en el conjunto de validación, acompañado de una pérdida de validación creciente al final del entrenamiento. Este comportamiento sugiere un problema de sobreajuste, donde el modelo memoriza los datos de entrenamiento, resultando en una baja capacidad de generalización a nuevos datos. Con la optimización de hiperparámetros, se observó una mejora en la precisión tanto en el entrenamiento como en la validación. Además, la pérdida de validación mostró una tendencia más estable en comparación con la CNN base. Ajustar los hiperparámetros clave ayudó a encontrar un equilibrio adecuado entre el sesgo y la varianza, mejorando la robustez del modelo y su capacidad de generalización.\n",
        "\n",
        "La introducción de dropout resultó en una precisión de entrenamiento menor que en los modelos anteriores, pero la precisión de validación fue comparable y la pérdida de validación disminuyó significativamente. Dropout ayudó a reducir el sobreajuste, mejorando la capacidad del modelo para generalizar a datos nuevos, aunque la precisión de entrenamiento fue menor debido a la regularización. Por otro lado, la regularización L2 mejoró la precisión de validación y mostró una tendencia más estable en la pérdida de validación en comparación con los modelos anteriores. La regularización L2 ayudó a limitar la complejidad del modelo, reduciendo el sobreajuste y mejorando la capacidad de generalización a datos nuevos.\n",
        "\n",
        "Finalmente, la combinación de dropout y L2 resultó en la mejor precisión de validación y una pérdida de validación que disminuyó de manera constante. Combinar ambas técnicas de regularización aprovechó las fortalezas de cada una, resultando en un modelo robusto y simple con la mejor capacidad de generalización entre todas las configuraciones evaluadas.\n",
        "\n",
        "Se concluye que, la CNN con la combinación de regularización por dropout y L2 mostró el mejor rendimiento en términos de capacidad de generalización y estabilidad. Esta configuración balancea eficientemente el aprendizaje del modelo con la prevención del sobreajuste, siendo la elección más adecuada para el problema planteado."
      ],
      "metadata": {
        "id": "iBoUJMRH5UBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar la red neuronal en el conjunto de prueba\n",
        "test_loss, test_acc = model_l2_dp.evaluate(test_gen)\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "gJl6iMY2NG8c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}